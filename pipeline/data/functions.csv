function_id,function_name,source_file,loc,has_for_loop,has_while_loop,has_nested_loops,num_loops,has_list_comp,has_dict_comp,has_generator,has_numpy,has_pandas,has_torch,has_tensorflow,has_sklearn,has_string_ops,num_function_calls,num_arithmetic_ops,cyclomatic_complexity,category,source_code
huggingface__transformers__.circleci__create_circleci_config.py__create_circleci_config,create_circleci_config,huggingface/transformers:.circleci/create_circleci_config.py,40,False,False,False,0,True,True,False,False,False,False,False,False,True,15,4,12,other,"def create_circleci_config(folder=None):
    if folder is None:
        folder = os.getcwd()
    os.environ[""test_preparation_dir""] = folder
    jobs = [k for k in ALL_TESTS if os.path.isfile(os.path.join(""test_preparation"" , f""{k.job_name}_test_list.txt"") )]
    print(""The following jobs will be run "", jobs)

    if len(jobs) == 0:
        jobs = [EmptyJob()]
    else:
        print(""Full list of job name inputs"", {j.job_name + ""_test_list"":{""type"":""string"", ""default"":''} for j in jobs})
        # Add a job waiting all the test jobs and aggregate their test summary files at the end
        collection_job = EmptyJob()
        collection_job.job_name = ""collection_job""
        jobs = [collection_job] + jobs

    config = {
        ""version"": ""2.1"",
        ""parameters"": {
            # Only used to accept the parameters from the trigger
            ""nightly"": {""type"": ""boolean"", ""default"": False},
            # Only used to accept the parameters from GitHub Actions trigger
            ""GHA_Actor"": {""type"": ""string"", ""default"": """"},
            ""GHA_Action"": {""type"": ""string"", ""default"": """"},
            ""GHA_Event"": {""type"": ""string"", ""default"": """"},
            ""GHA_Meta"": {""type"": ""string"", ""default"": """"},
            ""tests_to_run"": {""type"": ""string"", ""default"": """"},
            **{j.job_name + ""_test_list"":{""type"":""string"", ""default"":''} for j in jobs},
            **{j.job_name + ""_parallelism"":{""type"":""integer"", ""default"":1} for j in jobs},
        },
        ""jobs"": {j.job_name: j.to_dict() for j in jobs}
    }
    if ""CIRCLE_TOKEN"" in os.environ:
        # For private forked repo. (e.g. new model addition)
        config[""workflows""] = {""version"": 2, ""run_tests"": {""jobs"": [{j.job_name: {""context"": [""TRANSFORMERS_CONTEXT""]}} for j in jobs]}}
    else:
        # For public repo. (e.g. `transformers`)
        config[""workflows""] = {""version"": 2, ""run_tests"": {""jobs"": [j.job_name for j in jobs]}}
    with open(os.path.join(folder, ""generated_config.yml""), ""w"", encoding=""utf-8"") as f:
        f.write(yaml.dump(config, sort_keys=False, default_flow_style=False).replace(""' << pipeline"", "" << pipeline"").replace("">> '"", "" >>""))"
huggingface__transformers__.circleci__parse_test_outputs.py__parse_pytest_output,parse_pytest_output,huggingface/transformers:.circleci/parse_test_outputs.py,13,True,False,False,2,False,False,False,False,False,False,False,False,True,10,1,5,other,"def parse_pytest_output(file_path):
    skipped_tests = {}
    skipped_count = 0
    with open(file_path, 'r', encoding='utf-8') as file:
        for line in file:
            match = re.match(r'^SKIPPED \[(\d+)\] (tests/.*): (.*)$', line)
            if match:
                skipped_count += 1
                test_file, test_line, reason = match.groups()
                skipped_tests[reason] = skipped_tests.get(reason, []) + [(test_file, test_line)]
    for k,v in sorted(skipped_tests.items(), key=lambda x:len(x[1])):
        print(f""{len(v):4} skipped because: {k}"")
    print(""Number of skipped tests:"", skipped_count)"
huggingface__transformers__.circleci__parse_test_outputs.py__parse_pytest_failure_output,parse_pytest_failure_output,huggingface/transformers:.circleci/parse_test_outputs.py,15,True,False,False,2,False,False,False,False,False,False,False,False,True,11,1,6,other,"def parse_pytest_failure_output(file_path):
    failed_tests = {}
    failed_count = 0
    with open(file_path, 'r', encoding='utf-8') as file:
        for line in file:
            match = re.match(r'^FAILED (tests/.*) - (.*): (.*)$', line)
            if match:
                failed_count += 1
                _, error, reason = match.groups()
                failed_tests[reason] = failed_tests.get(reason, []) + [error]
    for k,v in sorted(failed_tests.items(), key=lambda x:len(x[1])):
        print(f""{len(v):4} failed because `{v[0]}` -> {k}"")
    print(""Number of failed tests:"", failed_count)
    if failed_count>0:
        exit(1)"
huggingface__transformers__.circleci__parse_test_outputs.py__parse_pytest_errors_output,parse_pytest_errors_output,huggingface/transformers:.circleci/parse_test_outputs.py,16,True,False,False,2,False,False,False,False,False,False,False,False,True,12,1,6,other,"def parse_pytest_errors_output(file_path):
    print(file_path)
    error_tests = {}
    error_count = 0
    with open(file_path, 'r', encoding='utf-8') as file:
        for line in file:
            match = re.match(r'^ERROR (tests/.*) - (.*): (.*)$', line)
            if match:
                error_count += 1
                _, test_error, reason = match.groups()
                error_tests[reason] = error_tests.get(reason, []) + [test_error]
    for k,v in sorted(error_tests.items(), key=lambda x:len(x[1])):
        print(f""{len(v):4} errored out because of `{v[0]}` -> {k}"")
    print(""Number of errors:"", error_count)
    if error_count>0:
        exit(1)"
huggingface__transformers__.circleci__parse_test_outputs.py__main,main,huggingface/transformers:.circleci/parse_test_outputs.py,16,False,False,False,0,False,False,False,False,False,False,False,False,False,9,0,4,other,"def main():
    parser = argparse.ArgumentParser()
    parser.add_argument(""--file"", help=""file to parse"")
    parser.add_argument(""--skip"", action=""store_true"", help=""show skipped reasons"")
    parser.add_argument(""--fail"", action=""store_true"", help=""show failed tests"")
    parser.add_argument(""--errors"", action=""store_true"", help=""show failed tests"")
    args = parser.parse_args()

    if args.skip:
        parse_pytest_output(args.file)

    if args.fail:
        parse_pytest_failure_output(args.file)

    if args.errors:
        parse_pytest_errors_output(args.file)"
huggingface__transformers__.github__scripts__assign_reviewers.py__pattern_to_regex,pattern_to_regex,huggingface/transformers:.github/scripts/assign_reviewers.py,12,False,False,False,0,False,False,False,False,False,False,False,False,True,4,1,3,other,"def pattern_to_regex(pattern):
    if pattern.startswith(""/""):
        start_anchor = True
        pattern = re.escape(pattern[1:])
    else:
        start_anchor = False
        pattern = re.escape(pattern)
    # Replace `*` with ""any number of non-slash characters""
    pattern = pattern.replace(r""\*"", ""[^/]*"")
    if start_anchor:
        pattern = r""^\/?"" + pattern  # Allow an optional leading slash after the start of the string
    return pattern"
huggingface__transformers__.github__scripts__assign_reviewers.py__get_file_owners,get_file_owners,huggingface/transformers:.github/scripts/assign_reviewers.py,19,True,False,False,1,True,False,False,False,False,False,False,False,True,7,0,5,other,"def get_file_owners(file_path, codeowners_lines):
    # Process lines in reverse (last matching pattern takes precedence)
    for line in reversed(codeowners_lines):
        # Skip comments and empty lines, strip inline comments
        line = line.split('#')[0].strip()
        if not line:
            continue

        # Split into pattern and owners
        parts = line.split()
        pattern = parts[0]
        # Can be empty, e.g. for dummy files with explicitly no owner!
        owners = [owner.removeprefix(""@"") for owner in parts[1:]]

        # Check if file matches pattern
        file_regex = pattern_to_regex(pattern)
        if re.search(file_regex, file_path) is not None:
            return owners  # Remember, can still be empty!
    return []  # Should never happen, but just in case"
huggingface__transformers__.github__scripts__assign_reviewers.py__pr_author_is_in_hf,pr_author_is_in_hf,huggingface/transformers:.github/scripts/assign_reviewers.py,14,True,False,False,1,True,False,False,False,False,False,False,False,True,4,0,5,other,"def pr_author_is_in_hf(pr_author, codeowners_lines):
    # Check if the PR author is in the codeowners file
    for line in codeowners_lines:
        line = line.split('#')[0].strip()
        if not line:
            continue

        # Split into pattern and owners
        parts = line.split()
        owners = [owner.removeprefix(""@"") for owner in parts[1:]]

        if pr_author in owners:
            return True
    return False"
huggingface__transformers__.github__scripts__assign_reviewers.py__main,main,huggingface/transformers:.github/scripts/assign_reviewers.py,44,True,False,True,2,True,False,False,False,False,False,False,False,True,25,1,11,other,"def main():
    script_dir = Path(__file__).parent.absolute()
    with open(script_dir / ""codeowners_for_review_action"") as f:
        codeowners_lines = f.readlines()

    g = Github(os.environ['GITHUB_TOKEN'])
    repo = g.get_repo(""huggingface/transformers"")
    with open(os.environ['GITHUB_EVENT_PATH']) as f:
        event = json.load(f)

    # The PR number is available in the event payload
    pr_number = event['pull_request']['number']
    pr = repo.get_pull(pr_number)
    pr_author = pr.user.login
    if pr_author_is_in_hf(pr_author, codeowners_lines):
        print(f""PR author {pr_author} is in codeowners, skipping review request."")
        return

    existing_reviews = list(pr.get_reviews())
    if existing_reviews:
        print(f""Already has reviews: {[r.user.login for r in existing_reviews]}"")
        return

    users_requested, teams_requested = pr.get_review_requests()
    users_requested = list(users_requested)
    if users_requested:
        print(f""Reviewers already requested: {users_requested}"")
        return

    locs_per_owner = Counter()
    for file in pr.get_files():
        owners = get_file_owners(file.filename, codeowners_lines)
        for owner in owners:
            locs_per_owner[owner] += file.changes

    # Assign the top 2 based on locs changed as reviewers, but skip the owner if present
    locs_per_owner.pop(pr_author, None)
    top_owners = locs_per_owner.most_common(2)
    print(""Top owners"", top_owners)
    top_owners = [owner[0] for owner in top_owners]
    try:
        pr.create_review_request(top_owners)
    except github.GithubException as e:
        print(f""Failed to request review for {top_owners}: {e}"")"
huggingface__transformers__benchmark__benches__llama.py__collect_metrics,collect_metrics,huggingface/transformers:benchmark/benches/llama.py,13,False,True,False,1,False,False,False,False,False,True,False,False,False,9,2,3,ml,"def collect_metrics(benchmark_id, continue_metric_collection, metrics_recorder):
    p = psutil.Process(os.getpid())
    while not continue_metric_collection.is_set():
        with p.oneshot():
            cpu_util = p.cpu_percent()
            mem_megabytes = p.memory_info().rss / (1024 * 1024)
        gpu_stats = gpustat.GPUStatCollection.new_query()
        gpu_util = gpu_stats[0][""utilization.gpu""]
        gpu_mem_megabytes = gpu_stats[0][""memory.used""]
        metrics_recorder.collect_device_measurements(
            benchmark_id, cpu_util, mem_megabytes, gpu_util, gpu_mem_megabytes
        )
        sleep(0.01)"
huggingface__transformers__benchmark__benchmark.py__checkout_commit,checkout_commit,huggingface/transformers:benchmark/benchmark.py,15,False,False,False,0,False,False,False,False,False,False,False,False,False,2,0,2,other,"def checkout_commit(repo: Repo, commit_id: str):
    """"""
    Context manager that checks out a given commit when entered, but gets back to the reference it was at on exit.
    Args:
        repo (`git.Repo`): A git repository (for instance the Transformers repo).
        commit_id (`str`): The commit reference to checkout inside the context manager.
    """"""
    current_head = repo.head.commit if repo.head.is_detached else repo.head.ref

    try:
        repo.git.checkout(commit_id)
        yield

    finally:
        repo.git.checkout(current_head)"
huggingface__transformers__benchmark__benchmark.py__summarize,summarize,huggingface/transformers:benchmark/benchmark.py,87,True,False,True,4,True,False,False,False,False,False,False,False,True,30,1,16,other,"def summarize(run_dir, metrics, expand_metrics=False):
    """"""Produce a summary for each optimum-benchmark launched job's output directory found in `run_dir`.

    Each summary's format is as follows (for `expand_metrics=False`):
    ```
    {
        ""model"": ""google/gemma-2b"",
        ""commit"": ""3cd6ed22e4d49219f300f5055e71e3929aba20d7"",
        ""config"": ""benchmark.input_shapes.batch_size=1,benchmark.input_shapes.sequence_length=5"",
        ""metrics"": {
            ""decode.latency.mean"": 1.624666809082031,
            ""per_token.latency.mean"": 0.012843788806628804,
            ""per_token.throughput.value"": 77.85864553330948
        }
    }
    ```
    """"""
    reports = glob.glob(os.path.join(run_dir, ""**/benchmark_report.json""), recursive=True)
    report_dirs = [str(Path(report).parent) for report in reports]

    summaries = []
    for report_dir in report_dirs:
        commit = re.search(r""/commit=([^/]+)"", report_dir).groups()[0]

        if not os.path.isfile(os.path.join(report_dir, ""benchmark.json"")):
            continue
        benchmark = Benchmark.from_json(os.path.join(report_dir, ""benchmark.json""))
        report = benchmark.report

        model = benchmark.config.backend[""model""]

        # This looks like `benchmark.input_shapes.batch_size=1,benchmark.input_shapes.sequence_length=5`.
        # (we rely on the usage of hydra's `${hydra.job.override_dirname}`.)
        benchmark_name = re.sub(f""backend.model={model},*"", """", report_dir)
        benchmark_name = str(Path(benchmark_name).parts[-1])
        if benchmark_name.startswith(""commit=""):
            benchmark_name = benchmark.config.name

        metrics_values = {}
        # post-processing of report: show a few selected/important metric
        for metric in metrics:
            keys = metric.split(""."")
            value = report.to_dict()
            current = metrics_values
            for key in keys:
                # Avoid KeyError when a user's specified metric has typo.
                # TODO: Give warnings.
                if key not in value:
                    continue
                value = value[key]

                if expand_metrics:
                    if isinstance(value, dict):
                        if key not in current:
                            current[key] = {}
                            current = current[key]
                    else:
                        current[key] = value

            if not expand_metrics:
                metrics_values[metric] = value

        # show some config information
        print(f""model: {model}"")
        print(f""commit: {commit}"")
        print(f""config: {benchmark_name}"")
        if len(metrics_values) > 0:
            print(""metrics:"")
            if expand_metrics:
                print(metrics_values)
            else:
                for metric, value in metrics_values.items():
                    print(f""  - {metric}: {value}"")
        print(""-"" * 80)

        summary = {
            ""model"": model,
            ""commit"": commit,
            ""config"": benchmark_name,
            ""metrics"": metrics_values,
        }
        summaries.append(summary)

        with open(os.path.join(report_dir, ""summary.json""), ""w"") as fp:
            json.dump(summary, fp, indent=4)

    return summaries"
huggingface__transformers__benchmark__benchmark.py__combine_summaries,combine_summaries,huggingface/transformers:benchmark/benchmark.py,47,True,False,False,1,False,False,False,False,False,False,False,False,True,5,0,6,other,"def combine_summaries(summaries):
    """"""Combine a list of summary obtained from the function `summarize`.

    The combined summary's format is as follows:
    ```
    ""google/gemma-2b"": {
        ""benchmark.input_shapes.batch_size=1,benchmark.input_shapes.sequence_length=5"": {
            ""3cd6ed22e4d49219f300f5055e71e3929aba20d7"": {
                ""metrics"": {""decode.latency.mean"": 1.624666809082031}
            },
            ""c97ee28b117c0abe8e08891f402065e4df6d72aa"": {
                ""metrics"": {""decode.latency.mean"": 1.6278163452148438}
            }
        },
        ""benchmark.input_shapes.batch_size=2,benchmark.input_shapes.sequence_length=5"": {
            ""3cd6ed22e4d49219f300f5055e71e3929aba20d7"": {
                ""metrics"": {""decode.latency.mean"": 1.6947791748046876}
            },
            ""c97ee28b117c0abe8e08891f402065e4df6d72aa"": {
                ""metrics"": {
                    ""decode.latency.mean"": 1.6980519409179688}
            }
        }
    }
    ```
    """"""
    combined = {}
    for summary in summaries:
        model = summary[""model""]
        config = summary[""config""]
        commit = summary[""commit""]

        if model not in combined:
            combined[model] = {}

        if config not in combined[model]:
            combined[model][config] = {}

        if commit not in combined[model][config]:
            combined[model][config][commit] = {""metrics"": summary[""metrics""]}

    with open(os.path.join(exp_run_dir, ""summary.json""), ""w"") as fp:
        json.dump(combined, fp, indent=4)

    print(json.dumps(combined, indent=4))

    return combined"
huggingface__transformers__benchmark__benchmarks_entrypoint.py__create_database_connection,create_database_connection,huggingface/transformers:benchmark/benchmarks_entrypoint.py,17,False,False,False,0,False,False,False,False,True,False,False,False,True,4,0,3,data_processing,"def create_database_connection():
    """"""
    Try to create a database connection. Returns None if connection fails.
    """"""
    if not PSYCOPG2_AVAILABLE:
        logger.warning(""psycopg2 not available - running in CSV-only mode"")
        return None

    try:
        import psycopg2

        conn = psycopg2.connect(""dbname=metrics"")
        logger.info(""Successfully connected to database"")
        return conn
    except Exception as e:
        logger.warning(f""Failed to connect to database: {e}. Running in CSV-only mode"")
        return None"
huggingface__transformers__benchmark__benchmarks_entrypoint.py__create_global_metrics_recorder,create_global_metrics_recorder,huggingface/transformers:benchmark/benchmarks_entrypoint.py,23,False,False,False,0,False,False,False,False,True,False,False,False,True,8,0,4,data_processing,"def create_global_metrics_recorder(
    repository: str, branch: str, commit_id: str, commit_msg: str, generate_csv: bool = False
) -> MetricsRecorder:
    """"""
    Create a global metrics recorder that will be used across all benchmarks.
    """"""
    connection = create_database_connection()
    recorder = MetricsRecorder(connection, logger, repository, branch, commit_id, commit_msg, generate_csv)

    # Log the storage mode
    storage_modes = []
    if connection is not None:
        storage_modes.append(""database"")
    if generate_csv:
        storage_modes.append(""CSV"")

    if not storage_modes:
        logger.warning(""Running benchmarks with NO data storage (no database connection, CSV disabled)"")
        logger.warning(""Use --csv flag to enable CSV output when database is unavailable"")
    else:
        logger.info(f""Running benchmarks with: {' + '.join(storage_modes)} storage"")

    return recorder"
huggingface__transformers__benchmark_v2__benchmark_scripts__continuous_batching_overall.py__run_and_parse_cb_example,run_and_parse_cb_example,huggingface/transformers:benchmark_v2/benchmark_scripts/continuous_batching_overall.py,20,False,False,False,0,False,False,False,False,False,False,False,False,True,11,2,3,other,"def run_and_parse_cb_example(args: str) -> dict:
    print(f""Benchmarking with args: {args}"")
    output = subprocess.run(
        [""python"", SCRIPT_LOCATION] + args.split() + COMMON_ARGS,
        stdout=subprocess.PIPE,
    )
    output = output.stdout.decode(""utf-8"")
    if ""generate_batch despite unexpected termination"" in output:
        return {""args"": args, **ERROR_OUTPUT}
    pattern = r""CB generation took: ([\d.]+) seconds for (\d+) tokens\. ([\d.]+)tok/s""
    match = re.search(pattern, output)
    if match is not None:
        return {
            ""args"": args,
            ""time_seconds"": float(match.group(1)),
            ""num_tokens"": int(match.group(2)),
            ""throughput_tok_per_sec"": float(match.group(3)),
        }
    else:
        return {""args"": args, **ERROR_OUTPUT}"
huggingface__transformers__benchmark_v2__framework__benchmark_config.py__is_fa2_or_kernel_available,is_fa2_or_kernel_available,huggingface/transformers:benchmark_v2/framework/benchmark_config.py,25,False,False,False,0,False,False,False,False,False,True,False,False,False,5,0,4,ml,"def is_fa2_or_kernel_available() -> bool:
    """"""Returns True if the flash_attn_2 or a fallback kernel is available""""""
    # Early return if flash_attn_2 is available
    if is_flash_attn_2_available():
        return True
    # Early return if kernels is not available
    if not is_kernels_available():
        logger.warning(
            ""flash_attention_2 is not available. kernels is not installed. Benchmarking flash_attention_2 will not ""
            ""be possible.""
        )
        return False
    # If kernels is available, try to get the flash_attn_2 kernel
    try:
        from kernels import get_kernel

        # TODO: Pass the 'version' kwarg to specify the binary version once kernels >= 0.12.0 is supported.
        get_kernel(""kernels-community/flash-attn2"")
    except Exception as _:
        logger.warning(
            ""flash_attention_2 is not available. kernels is installed, but the flash_attn kernel is not available.""
            ""Benchmarking flash_attention_2 will not be possible.""
        )
        return False
    return True"
huggingface__transformers__benchmark_v2__framework__benchmark_config.py__adapt_configs,adapt_configs,huggingface/transformers:benchmark_v2/framework/benchmark_config.py,36,True,False,True,2,False,False,True,False,False,True,False,False,False,6,0,5,ml,"def adapt_configs(
    configs: list[BenchmarkConfig],
    warmup_iterations: int | list[int] = 5,
    measurement_iterations: int | list[int] = 20,
    batch_size: int | list[int] = 1,
    sequence_length: int | list[int] = 128,
    num_tokens_to_generate: int | list[int] = 128,
    gpu_monitoring: bool | list[bool] = True,
) -> list[BenchmarkConfig]:
    parameters = (
        x if isinstance(x, list) else [x]
        for x in [
            warmup_iterations,
            measurement_iterations,
            batch_size,
            sequence_length,
            num_tokens_to_generate,
            gpu_monitoring,
        ]
    )
    iterator = itertools.product(*parameters)

    adapted_configs = []
    for warmup_iters, measurement_iters, bs, seqlen, ntok, monitor in iterator:
        for config in configs:
            config = config.to_dict()
            config[""warmup_iterations""] = warmup_iters
            config[""measurement_iterations""] = measurement_iters
            config[""batch_size""] = bs
            config[""sequence_length""] = seqlen
            config[""num_tokens_to_generate""] = ntok
            config[""gpu_monitoring""] = monitor
            # Remove the old name so it gets re-inferred with the updated values
            config.pop(""name"", None)
            adapted_configs.append(BenchmarkConfig.from_dict(config))
    return adapted_configs"
huggingface__transformers__benchmark_v2__framework__benchmark_config.py__get_config_by_level,get_config_by_level,huggingface/transformers:benchmark_v2/framework/benchmark_config.py,33,True,False,True,4,False,False,False,False,False,True,False,False,False,18,0,11,ml,"def get_config_by_level(level: int) -> list[BenchmarkConfig]:
    configs = []
    # Early return if level is greater than 3: we generate all combinations of configs, maybe even w/ all compile modes
    if level >= 3:
        for attn_implementation in BenchmarkConfig.all_attn_implementations:
            # Usually there is not much to gain by compiling with other modes, but we allow it for level 4
            compile_modes = BenchmarkConfig.all_compiled_modes if level >= 4 else [None, ""default""]
            for cm in compile_modes:
                compile_kwargs = {""mode"": cm} if cm is not None else None
                for kernelize_on in {False, KERNELIZATION_AVAILABLE}:
                    for cb_on in [False, True]:
                        configs.append(
                            BenchmarkConfig(
                                attn_implementation=attn_implementation,
                                compile_kwargs=compile_kwargs,
                                kernelize=kernelize_on,
                                continuous_batching=cb_on,
                            )
                        )
        return configs
    # Otherwise, we add the configs for the given level
    if level >= 0:
        configs.append(BenchmarkConfig(attn_implementation=""flex_attention"", compile_kwargs={}))
    if level >= 1:
        configs.append(BenchmarkConfig(attn_implementation=""flash_attention_2""))
        configs.append(BenchmarkConfig(attn_implementation=""eager"", compile_kwargs={}))
        configs.append(BenchmarkConfig(attn_implementation=""flash_attention_2"", continuous_batching=True))
    if level >= 2:
        configs.append(BenchmarkConfig(attn_implementation=""sdpa"", compile_kwargs={}))
        configs.append(BenchmarkConfig(attn_implementation=""flex_attention"", compile_kwargs={}, kernelize=True))
        configs.append(BenchmarkConfig(attn_implementation=""flash_attention_2"", kernelize=True))
        configs.append(BenchmarkConfig(attn_implementation=""sdpa"", continuous_batching=True))
    return configs"
huggingface__transformers__benchmark_v2__framework__benchmark_runner.py__flush_memory,flush_memory,huggingface/transformers:benchmark_v2/framework/benchmark_runner.py,28,False,False,False,0,False,False,False,False,False,True,False,False,False,18,0,9,ml,"def flush_memory(flush_compile: bool = True) -> None:
    """"""Flush GPU memory and run garbage collection. If the flush_compile flag is set, we also clear the everything
    related to compile cache.""""""
    gc.collect()
    # If needed, flush everything related to torch.compile
    if flush_compile:
        # Dynamo resets
        torch._dynamo.reset()
        torch._dynamo.reset_code_caches()
        if hasattr(torch._inductor, ""codecache""):
            # Clear FX graph cache
            if hasattr(torch._inductor.codecache, ""FxGraphCache""):
                torch._inductor.codecache.FxGraphCache.clear()
            # Clear PyCodeCache
            if hasattr(torch._inductor.codecache, ""PyCodeCache""):
                torch._inductor.codecache.PyCodeCache.cache_clear()
            # Clear TritonFuture cache (for async compilation)
            if hasattr(torch._inductor.codecache, ""TritonFuture""):
                if hasattr(torch._inductor.codecache.TritonFuture, ""_compile_cache""):
                    torch._inductor.codecache.TritonFuture._compile_cache.clear()
    # Clear device cache
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    elif is_torch_xpu_available():
        torch.xpu.empty_cache()
        torch.xpu.synchronize()
    gc.collect()"
huggingface__transformers__benchmark_v2__framework__data_classes.py__add_unit_to_duration,add_unit_to_duration,huggingface/transformers:benchmark_v2/framework/data_classes.py,16,True,False,False,1,False,False,False,True,False,False,False,False,True,2,5,7,data_processing,"def add_unit_to_duration(stats: dict[str, float]) -> dict[str, str]:
    for key in list(stats.keys()):
        value = stats[key]
        if value > 3600:
            stats[key] = f""{(value / 3600):.2f}hr""
        elif value > 60:
            stats[key] = f""{(value / 60):.2f}min""
        elif value > 1:
            stats[key] = f""{value:.2f}s""
        elif value > 1e-3:
            stats[key] = f""{(value * 1e3):.2f}ms""
        elif value > 1e-6:
            stats[key] = f""{(value * 1e6):.2f}us""
        else:
            stats[key] = f""{(value * 1e9):.2f}ns""
    return stats"
huggingface__transformers__benchmark_v2__framework__hardware_metrics.py__get_intel_xpu_stats,get_intel_xpu_stats,huggingface/transformers:benchmark_v2/framework/hardware_metrics.py,28,True,False,False,1,False,False,False,False,False,True,False,False,True,14,1,5,ml,"def get_intel_xpu_stats() -> tuple[int, float]:
    """"""Returns the utilization and memory used of an Intel XPU""""""
    # xpu-smi outputs CSV format: Timestamp, DeviceId, GPU Memory Utilization (%), GPU Memory Used (MiB)
    xpu_smi_output = subprocess.check_output([""xpu-smi"", ""dump"", ""-m"", ""5,18"", ""-n"", ""1""])
    lines = xpu_smi_output.decode(""utf-8"").strip().split(""\n"")

    # Parse all data lines (skip header) and collect stats from all cards
    xpu_stats = []
    for line in lines[1:]:
        data_line = line.split("","")
        if len(data_line) < 4:
            continue
        device_id = data_line[1].strip()
        utilization_str = data_line[2].strip()
        memory_used_str = data_line[3].strip()
        if utilization_str != ""N/A"" and memory_used_str != ""N/A"":
            utilization = int(float(utilization_str))
            memory_used_mib = float(memory_used_str)
            xpu_stats.append((device_id, utilization, memory_used_mib))

    if not xpu_stats:
        return 0, 0.0

    # Sort by utilization (descending) and pick the highest
    xpu_stats.sort(key=lambda x: x[1], reverse=True)
    device_id, utilization, memory_used_mib = xpu_stats[0]
    memory_used_gb = memory_used_mib / 1024
    return utilization, memory_used_gb"
huggingface__transformers__scripts__check_tokenizers.py__check_diff,check_diff,huggingface/transformers:scripts/check_tokenizers.py,21,False,False,False,0,False,False,False,False,False,False,False,False,True,10,0,4,ml,"def check_diff(
    spm_diff: list[int], tok_diff: list[int], slow: PreTrainedTokenizerBase, fast: PreTrainedTokenizerBase
) -> bool:
    if spm_diff == list(reversed(tok_diff)):
        # AAA -> AA+A vs A+AA case.
        return True
    elif len(spm_diff) == len(tok_diff) and fast.decode(spm_diff) == fast.decode(tok_diff):
        # Second order OK
        # Barrich -> Barr + ich vs Bar + rich
        return True
    spm_reencoded = slow.encode(slow.decode(spm_diff))
    tok_reencoded = fast.encode(fast.decode(spm_diff))
    if spm_reencoded != spm_diff and spm_reencoded == tok_reencoded:
        # Type 3 error.
        # Snehagatha ->
        #       Sne, h, aga, th, a
        #       Sne, ha, gat, ha
        # Encoding the wrong with sp does not even recover what spm gave us
        # It fits tokenizer however...
        return True
    return False"
huggingface__transformers__scripts__check_tokenizers.py__check_details,check_details,huggingface/transformers:scripts/check_tokenizers.py,61,True,False,True,4,True,False,True,False,False,False,False,False,True,31,18,18,ml,"def check_details(
    line: str, spm_ids: list[int], tok_ids: list[int], slow: PreTrainedTokenizerBase, fast: PreTrainedTokenizerBase
) -> bool:
    # Encoding can be the same with same result AAA -> A + AA vs AA + A
    # We can check that we use at least exactly the same number of tokens.
    for i, (spm_id, tok_id) in enumerate(zip(spm_ids, tok_ids)):
        if spm_id != tok_id:
            break
    first = i
    for i, (spm_id, tok_id) in enumerate(zip(reversed(spm_ids), reversed(tok_ids))):
        if spm_id != tok_id:
            break
    last = len(spm_ids) - i

    spm_diff = spm_ids[first:last]
    tok_diff = tok_ids[first:last]

    if check_diff(spm_diff, tok_diff, slow, fast):
        return True

    if check_LTR_mark(line, first, fast):
        return True

    if last - first > 5:
        # We might have twice a single problem, attempt to subdivide the disjointed tokens into smaller problems
        spms = Counter(spm_ids[first:last])
        toks = Counter(tok_ids[first:last])

        removable_tokens = {spm_ for (spm_, si) in spms.items() if toks.get(spm_, 0) == si}
        min_width = 3
        for i in range(last - first - min_width):
            if all(spm_ids[first + i + j] in removable_tokens for j in range(min_width)):
                possible_matches = [
                    k
                    for k in range(last - first - min_width)
                    if tok_ids[first + k : first + k + min_width] == spm_ids[first + i : first + i + min_width]
                ]
                for j in possible_matches:
                    if check_diff(
                        spm_ids[first : first + i], tok_ids[first : first + j], slow, fast
                    ) and check_details(
                        line,
                        spm_ids[first + i : last],
                        tok_ids[first + j : last],
                        slow,
                        fast,
                    ):
                        return True

    print(f""Spm: {[fast.decode([spm_ids[i]]) for i in range(first, last)]}"")
    try:
        print(f""Tok: {[fast.decode([tok_ids[i]]) for i in range(first, last)]}"")
    except Exception as e:
        print(f""Could not decode tok_ids: {e}"")

    fast.decode(spm_ids[:first])
    fast.decode(spm_ids[last:])
    wrong = fast.decode(spm_ids[first:last])
    print()
    print(wrong)
    return False"
huggingface__transformers__scripts__stale.py__main,main,huggingface/transformers:scripts/stale.py,37,True,False,False,1,False,False,True,False,False,False,False,False,True,28,4,9,other,"def main():
    g = Github(os.environ[""GITHUB_TOKEN""])
    repo = g.get_repo(""huggingface/transformers"")
    open_issues = repo.get_issues(state=""open"")

    for i, issue in enumerate(open_issues):
        print(i, issue)
        comments = sorted(issue.get_comments(), key=lambda i: i.created_at, reverse=True)
        last_comment = comments[0] if len(comments) > 0 else None
        if (
            last_comment is not None
            and last_comment.user.login == ""github-actions[bot]""
            and (dt.utcnow() - issue.updated_at.replace(tzinfo=None)).days > 7
            and (dt.utcnow() - issue.created_at.replace(tzinfo=None)).days >= 30
            and not any(label.name.lower() in LABELS_TO_EXEMPT for label in issue.get_labels())
        ):
            # print(f""Would close issue {issue.number} since it has been 7 days of inactivity since bot mention."")
            try:
                issue.edit(state=""closed"")
            except github.GithubException as e:
                print(""Couldn't close the issue:"", repr(e))
        elif (
            (dt.utcnow() - issue.updated_at.replace(tzinfo=None)).days > 23
            and (dt.utcnow() - issue.created_at.replace(tzinfo=None)).days >= 30
            and not any(label.name.lower() in LABELS_TO_EXEMPT for label in issue.get_labels())
        ):
            # print(f""Would add stale comment to {issue.number}"")
            try:
                issue.create_comment(
                    ""This issue has been automatically marked as stale because it has not had ""
                    ""recent activity. If you think this still needs to be addressed ""
                    ""please comment on this thread.\n\nPlease note that issues that do not follow the ""
                    ""[contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) ""
                    ""are likely to be ignored.""
                )
            except github.GithubException as e:
                print(""Couldn't create comment:"", repr(e))"
huggingface__transformers__src__transformers__backbone_utils.py__consolidate_backbone_kwargs_to_config,consolidate_backbone_kwargs_to_config,huggingface/transformers:src/transformers/backbone_utils.py,48,False,False,False,0,False,False,False,False,False,False,False,False,True,16,0,7,ml,"def consolidate_backbone_kwargs_to_config(
    backbone_config,
    default_backbone: str | None = None,
    default_config_type: str | None = None,
    default_config_kwargs: dict | None = None,
    timm_default_kwargs: dict | None = None,
    **kwargs,
):
    # Lazy import to avoid circular import issues. Can be imported properly
    # after deleting ref to `BackboneMixin` in `utils/backbone_utils.py`
    from .configuration_utils import PreTrainedConfig
    from .models.auto import CONFIG_MAPPING

    use_timm_backbone = kwargs.pop(""use_timm_backbone"", True)
    backbone_kwargs = kwargs.pop(""backbone_kwargs"", {})
    backbone = kwargs.pop(""backbone"") if kwargs.get(""backbone"") is not None else default_backbone
    kwargs.pop(""use_pretrained_backbone"", None)

    # Init timm backbone with hardcoded values for BC. If everything is set to `None` and there is
    # a default timm config, we use it to init the backbone.
    if (
        timm_default_kwargs is not None
        and use_timm_backbone
        and backbone is not None
        and backbone_config is None
        and not backbone_kwargs
    ):
        backbone_config = CONFIG_MAPPING[""timm_backbone""](backbone=backbone, **timm_default_kwargs)
    elif backbone is not None and backbone_config is None:
        if repo_exists(backbone):
            config_dict, _ = PreTrainedConfig.get_config_dict(backbone)
            config_class = CONFIG_MAPPING[config_dict[""model_type""]]
            config_dict.update(backbone_kwargs)
            backbone_config = config_class(**config_dict)
        else:
            backbone_config = CONFIG_MAPPING[""timm_backbone""](backbone=backbone, **backbone_kwargs)
    elif backbone_config is None and default_config_type is not None:
        logger.info(
            f""`backbone_config` is `None`. Initializing the config with the default `{default_config_type}` vision config.""
        )
        default_config_kwargs = default_config_kwargs or {}
        backbone_config = CONFIG_MAPPING[default_config_type](**default_config_kwargs)
    elif isinstance(backbone_config, dict):
        backbone_model_type = backbone_config.get(""model_type"")
        config_class = CONFIG_MAPPING[backbone_model_type]
        backbone_config = config_class.from_dict(backbone_config)

    return backbone_config, kwargs"
huggingface__transformers__src__transformers__backbone_utils.py__load_backbone,load_backbone,huggingface/transformers:src/transformers/backbone_utils.py,19,False,False,False,0,False,False,False,False,False,False,False,False,False,3,0,2,ml,"def load_backbone(config):
    """"""
    Loads the backbone model from a config object.

    If the config is from the backbone model itself, then we return a backbone model with randomly initialized
    weights.

    If the config is from the parent model of the backbone model itself, then we load the pretrained backbone weights
    if specified.
    """"""
    from transformers import AutoBackbone

    backbone_config = getattr(config, ""backbone_config"", None)

    if backbone_config is None:
        backbone = AutoBackbone.from_config(config=config)
    else:
        backbone = AutoBackbone.from_config(config=backbone_config)
    return backbone"
huggingface__transformers__src__transformers__cli__add_fast_image_processor.py__add_fast_image_processor,add_fast_image_processor,huggingface/transformers:src/transformers/cli/add_fast_image_processor.py,60,True,False,False,1,False,False,False,False,False,False,False,False,True,20,3,7,other,"def add_fast_image_processor(
    model_name: Annotated[str, typer.Argument(help=""The name of the folder containing the model's implementation."")],
):
    """"""
    Add a fast image processor to a model.

    Adds the necessary references to the fast image processor in the transformers package, and create the fast image processor file in the model's folder.
    """"""
    model_module = TRANSFORMERS_PATH / ""models"" / model_name
    image_processing_module_file = list(model_module.glob(""image_processing*.py""))
    if not image_processing_module_file:
        raise ValueError(f""No image processing module found in {model_module}"")
    elif len(image_processing_module_file) > 1:
        for file_name in image_processing_module_file:
            if not str(file_name).endswith(""_fast.py""):
                image_processing_module_file = str(file_name)
                break
    else:
        image_processing_module_file = str(image_processing_module_file[0])

    with open(image_processing_module_file, ""r"", encoding=""utf-8"") as f:
        content_base_file = f.read()

    # regex to find object starting with ""class "" and ending with ""ImageProcessor"", including ""ImageProcessor"" in the match
    image_processor_name = re.findall(r""class (\w*ImageProcessor)"", content_base_file)
    if not image_processor_name:
        raise ValueError(f""No ImageProcessor class found in {image_processing_module_file}"")

    image_processor_name = image_processor_name[0]
    fast_image_processor_name = image_processor_name + ""Fast""
    fast_image_processing_module_file = image_processing_module_file.replace("".py"", ""_fast.py"")

    print(f""Adding {fast_image_processor_name} to {fast_image_processing_module_file}"")

    add_fast_image_processor_to_model_init(
        fast_image_processing_module_file=fast_image_processing_module_file,
        fast_image_processor_name=fast_image_processor_name,
        model_name=model_name,
    )

    add_fast_image_processor_to_auto(
        image_processor_name=image_processor_name,
        fast_image_processor_name=fast_image_processor_name,
    )

    add_fast_image_processor_to_doc(
        fast_image_processor_name=fast_image_processor_name,
        model_name=model_name,
    )

    add_fast_image_processor_to_tests(
        fast_image_processor_name=fast_image_processor_name,
        model_name=model_name,
    )

    add_fast_image_processor_file(
        fast_image_processing_module_file=fast_image_processing_module_file,
        fast_image_processor_name=fast_image_processor_name,
        content_base_file=content_base_file,
    )"
huggingface__transformers__src__transformers__cli__add_fast_image_processor.py__add_fast_image_processor_to_model_init,add_fast_image_processor_to_model_init,huggingface/transformers:src/transformers/cli/add_fast_image_processor.py,114,False,False,False,0,True,False,True,False,False,False,False,False,True,32,16,14,other,"def add_fast_image_processor_to_model_init(
    fast_image_processing_module_file: str, fast_image_processor_name, model_name: str
):
    """"""
    Add the fast image processor to the __init__.py file of the model.
    """"""
    with open(TRANSFORMERS_PATH / ""models"" / model_name / ""__init__.py"", ""r"", encoding=""utf-8"") as f:
        content = f.read()

    fast_image_processing_module_file = fast_image_processing_module_file.split(os.sep)[-1].replace("".py"", """")

    if ""import *"" in content:
        # we have an init file in the updated format
        # get the indented block after if TYPE_CHECKING: and before else:, append the new import, sort the imports and write the updated content
        # Step 1: Find the block
        block_regex = re.compile(
            r""if TYPE_CHECKING:\n(?P<if_block>.*?)(?=\s*else:)"",
            re.DOTALL,
        )
        match = block_regex.search(content)

        if not match:
            raise ValueError(""Couldn't find the 'if TYPE_CHECKING' block."")

        block_content = match.group(""if_block"")  # The captured import block

        # Step 2: Parse existing entries
        entries = block_content.split(""\n"")
        indent = "" "" * (len(entries[0]) - len(entries[0].lstrip()))
        new_entry = f""{indent}from .{fast_image_processing_module_file} import *""
        if new_entry not in entries:
            entries.append(new_entry)
        entries.sort()
        updated_block = ""\n"".join(entry for entry in entries)

        # Replace the original block in the content
        updated_content = content[: match.start(""if_block"")] + updated_block + content[match.end(""if_block"") :]
    else:
        # we have an init file in the old format

        # add ""is_torchvision_available"" import to from ...utils import (
        # Regex to match import statements from transformers.utils
        pattern = r""""""
            from\s+\.\.\.utils\s+import\s+
            (?:                                   # Non-capturing group for either:
                ([\w, ]+)                         # 1. Single-line imports (e.g., 'a, b')
                |                                 # OR
                \((.*?)\)                         # 2. Multi-line imports (e.g., '(a, ... b)')
            )
        """"""
        regex = re.compile(pattern, re.VERBOSE | re.DOTALL)

        def replacement_function(match):
            # Extract existing imports
            imports = (match.group(1) or match.group(2)).split("","")
            imports = imports[:-1] if imports[-1] == ""\n"" else imports
            imports = [imp.strip() for imp in imports]

            # Add the new import if not already present
            if ""is_torchvision_available"" not in imports:
                imports.append(""is_torchvision_available"")
                imports.sort()

            # Convert to multi-line import in all cases
            updated_imports = ""(\n    "" + "",\n    "".join(imports) + "",\n)""

            return f""from ...utils import {updated_imports}""

        # Replace all matches in the file content
        updated_content = regex.sub(replacement_function, content)

        vision_import_structure_block = f'    _import_structure[""{fast_image_processing_module_file[:-5]}""] = [""{fast_image_processor_name[:-4]}""]\n'

        added_import_structure_block = (
            ""try:\n    if not is_torchvision_available():\n""
            ""        raise OptionalDependencyNotAvailable()\n""
            ""except OptionalDependencyNotAvailable:\n""
            ""    pass\n""
            ""else:\n""
            f'    _import_structure[""{fast_image_processing_module_file}""] = [""{fast_image_processor_name}""]\n'
        )

        if vision_import_structure_block not in updated_content:
            raise ValueError(""Couldn't find the 'vision _import_structure block' block."")

        if added_import_structure_block not in updated_content:
            updated_content = updated_content.replace(
                vision_import_structure_block, vision_import_structure_block + ""\n"" + added_import_structure_block
            )

        vision_import_statement_block = (
            f""        from .{fast_image_processing_module_file[:-5]} import {fast_image_processor_name[:-4]}\n""
        )

        added_import_statement_block = (
            ""    try:\n        if not is_torchvision_available():\n""
            ""            raise OptionalDependencyNotAvailable()\n""
            ""    except OptionalDependencyNotAvailable:\n""
            ""        pass\n""
            ""    else:\n""
            f""        from .{fast_image_processing_module_file} import {fast_image_processor_name}\n""
        )

        if vision_import_statement_block not in updated_content:
            raise ValueError(""Couldn't find the 'vision _import_structure block' block."")

        if added_import_statement_block not in updated_content:
            updated_content = updated_content.replace(
                vision_import_statement_block, vision_import_statement_block + ""\n"" + added_import_statement_block
            )

    # write the updated content
    with open(TRANSFORMERS_PATH / ""models"" / model_name / ""__init__.py"", ""w"", encoding=""utf-8"") as f:
        f.write(updated_content)"
huggingface__transformers__src__transformers__cli__add_fast_image_processor.py__add_fast_image_processor_to_auto,add_fast_image_processor_to_auto,huggingface/transformers:src/transformers/cli/add_fast_image_processor.py,15,False,False,False,0,False,False,False,False,False,False,False,False,True,5,6,3,other,"def add_fast_image_processor_to_auto(image_processor_name: str, fast_image_processor_name: str):
    """"""
    Add the fast image processor to the auto module.
    """"""
    with open(TRANSFORMERS_PATH / ""models"" / ""auto"" / ""image_processing_auto.py"", ""r"", encoding=""utf-8"") as f:
        content = f.read()

    # get all lines containing the image processor name
    updated_content = content.replace(
        f'(""{image_processor_name}"",)', f'(""{image_processor_name}"", ""{fast_image_processor_name}"")'
    )

    # write the updated content
    with open(TRANSFORMERS_PATH / ""models"" / ""auto"" / ""image_processing_auto.py"", ""w"", encoding=""utf-8"") as f:
        f.write(updated_content)"
huggingface__transformers__src__transformers__cli__add_fast_image_processor.py__add_fast_image_processor_to_doc,add_fast_image_processor_to_doc,huggingface/transformers:src/transformers/cli/add_fast_image_processor.py,32,True,False,False,1,False,False,False,False,False,False,False,False,True,11,4,7,other,"def add_fast_image_processor_to_doc(fast_image_processor_name: str, model_name: str):
    """"""
    Add the fast image processor to the model's doc file.
    """"""
    doc_source = REPO_PATH / ""docs"" / ""source""
    # find the doc files
    doc_files = list(doc_source.glob(f""*/model_doc/{model_name}.md""))
    if not doc_files:
        # try again with ""-""
        doc_files = list(doc_source.glob(f""*/model_doc/{model_name.replace('_', '-')}.md""))
    if not doc_files:
        raise ValueError(f""No doc files found for {model_name}"")

    base_doc_string = (
        f""## {fast_image_processor_name[:-4]}\n\n[[autodoc]] {fast_image_processor_name[:-4]}\n    - preprocess""
    )
    fast_doc_string = f""## {fast_image_processor_name}\n\n[[autodoc]] {fast_image_processor_name}\n    - preprocess""

    for doc_file in doc_files:
        with open(doc_file, ""r"", encoding=""utf-8"") as f:
            content = f.read()

        if fast_doc_string not in content:
            # add the fast image processor to the doc
            updated_content = content.replace(
                base_doc_string,
                base_doc_string + ""\n\n"" + fast_doc_string,
            )

            # write the updated content
            with open(doc_file, ""w"", encoding=""utf-8"") as f:
                f.write(updated_content)"
huggingface__transformers__src__transformers__cli__add_fast_image_processor.py__add_fast_image_processor_to_tests,add_fast_image_processor_to_tests,huggingface/transformers:src/transformers/cli/add_fast_image_processor.py,73,False,False,False,0,True,False,False,False,False,False,False,False,True,23,10,11,other,"def add_fast_image_processor_to_tests(fast_image_processor_name: str, model_name: str):
    """"""
    Add the fast image processor to the image processing tests.
    """"""
    tests_path = REPO_PATH / ""tests"" / ""models"" / model_name
    test_file = tests_path / f""test_image_processing_{model_name}.py""
    if not os.path.exists(test_file):
        logger.warning(f""No test file found for {model_name}. Skipping."")
        return

    with open(test_file, ""r"", encoding=""utf-8"") as f:
        content = f.read()

    # add is_torchvision_available import to the imports
    # Regex to match import statements from transformers.utils
    pattern = r""""""
        from\s+transformers\.utils\s+import\s+
        (?:                                   # Non-capturing group for either:
            ([\w, ]+)                         # 1. Single-line imports (e.g., 'a, b')
            |                                 # OR
            \((.*?)\)                         # 2. Multi-line imports (e.g., '(a, ... b)')
        )
    """"""
    regex = re.compile(pattern, re.VERBOSE | re.DOTALL)

    def replacement_function(match):
        # Extract existing imports
        existing_imports = (match.group(1) or match.group(2)).split("","")
        existing_imports = existing_imports[:-1] if existing_imports[-1] == ""\n"" else existing_imports
        existing_imports = [imp.strip() for imp in existing_imports]

        # Add the new import if not already present
        if ""is_torchvision_available"" not in existing_imports:
            existing_imports.append(""is_torchvision_available"")
            existing_imports.sort()

        # Rebuild the import statement
        if match.group(1):  # Single-line import
            updated_imports = "", "".join(existing_imports)
        else:  # Multi-line import
            updated_imports = ""(\n    "" + "",\n    "".join(existing_imports) + "",\n)""

        return f""from transformers.utils import {updated_imports}""

    # Replace all matches in the file content
    updated_content = regex.sub(replacement_function, content)

    # add the fast image processor to the imports
    base_import_string = f""    from transformers import {fast_image_processor_name[:-4]}""
    fast_import_string = (
        f""    if is_torchvision_available():\n        from transformers import {fast_image_processor_name}""
    )
    if fast_import_string not in updated_content:
        updated_content = updated_content.replace(base_import_string, base_import_string + ""\n\n"" + fast_import_string)

    # get line starting with ""    image_processing_class = "" and add a line after it starting with ""    fast_image_processing_class = ""
    image_processing_class_line = re.search(r""    image_processing_class = .*"", updated_content)
    if not image_processing_class_line:
        logger.warning(f""Couldn't find the 'image_processing_class' line in {test_file}. Skipping."")
        return

    fast_image_processing_class_line = (
        f""    fast_image_processing_class = {fast_image_processor_name} if is_torchvision_available() else None""
    )
    if ""    fast_image_processing_class = "" not in updated_content:
        updated_content = updated_content.replace(
            image_processing_class_line.group(0),
            image_processing_class_line.group(0) + ""\n"" + fast_image_processing_class_line,
        )

    # write the updated content
    with open(test_file, ""w"", encoding=""utf-8"") as f:
        f.write(updated_content)"
huggingface__transformers__src__transformers__cli__add_fast_image_processor.py__get_fast_image_processing_content_header,get_fast_image_processing_content_header,huggingface/transformers:src/transformers/cli/add_fast_image_processor.py,34,False,False,False,0,False,False,False,False,False,False,False,False,True,7,0,3,other,"def get_fast_image_processing_content_header(content: str) -> str:
    """"""
    Get the header of the slow image processor file.
    """"""
    # get all the commented lines at the beginning of the file
    content_header = re.search(r""^# coding=utf-8\n(#[^\n]*\n)*"", content, re.MULTILINE)
    if not content_header:
        logger.warning(""Couldn't find the content header in the slow image processor file. Using a default header."")
        return (
            f""# coding=utf-8\n""
            f""# Copyright {CURRENT_YEAR} The HuggingFace Team. All rights reserved.\n""
            f""#\n""
            f'# Licensed under the Apache License, Version 2.0 (the ""License"");\n'
            f""# you may not use this file except in compliance with the License.\n""
            f""# You may obtain a copy of the License at\n""
            f""#\n""
            f""#     http://www.apache.org/licenses/LICENSE-2.0\n""
            f""#\n""
            f""# Unless required by applicable law or agreed to in writing, software\n""
            f'# distributed under the License is distributed on an ""AS IS"" BASIS,\n'
            f""# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n""
            f""# See the License for the specific language governing permissions and\n""
            f""# limitations under the License.\n""
            f""\n""
        )
    content_header = content_header.group(0)
    # replace the year in the copyright
    content_header = re.sub(r""# Copyright (\d+)\s"", f""# Copyright {CURRENT_YEAR} "", content_header)
    # get the line starting with """"""Image processor in content if it exists
    match = re.search(r'^""""""Image processor.*$', content, re.MULTILINE)
    if match:
        content_header += match.group(0).replace(""Image processor"", ""Fast Image processor"")

    return content_header"
huggingface__transformers__src__transformers__cli__add_fast_image_processor.py__write_default_fast_image_processor_file,write_default_fast_image_processor_file,huggingface/transformers:src/transformers/cli/add_fast_image_processor.py,29,False,False,False,0,False,False,False,False,False,False,False,False,True,3,2,2,other,"def write_default_fast_image_processor_file(
    fast_image_processing_module_file: str, fast_image_processor_name: str, content_base_file: str
):
    """"""
    Write a default fast image processor file. Used when encountering a problem while parsing the slow image processor file.
    """"""
    imports = ""\n\nfrom ...image_processing_utils_fast import BaseImageProcessorFast\n\n\n""
    content_header = get_fast_image_processing_content_header(content_base_file)
    content_base_file = (
        f""class {fast_image_processor_name}(BaseImageProcessorFast):\n""
        ""    # To be implemented\n""
        ""    resample = None\n""
        ""    image_mean = None\n""
        ""    image_std = None\n""
        ""    size = None\n""
        ""    default_to_square = None\n""
        ""    crop_size = None\n""
        ""    do_resize = None\n""
        ""    do_center_crop = None\n""
        ""    do_rescale = None\n""
        ""    do_normalize = None\n""
        ""    do_convert_rgb = None\n\n\n""
        f'__all__ = [""{fast_image_processor_name}""]\n'
    )

    content = content_header + imports + content_base_file

    with open(fast_image_processing_module_file, ""w"", encoding=""utf-8"") as f:
        f.write(content)"
huggingface__transformers__src__transformers__cli__add_fast_image_processor.py__add_fast_image_processor_file,add_fast_image_processor_file,huggingface/transformers:src/transformers/cli/add_fast_image_processor.py,105,False,False,False,0,True,False,True,False,False,False,False,False,True,55,3,17,other,"def add_fast_image_processor_file(
    fast_image_processing_module_file: str, fast_image_processor_name: str, content_base_file: str
):
    """"""
    Add the fast image processor file to the model's folder.
    """"""
    # if the file already exists, do nothing
    if os.path.exists(fast_image_processing_module_file):
        print(f""{fast_image_processing_module_file} already exists. Skipping."")
        return

    regex = rf""class {fast_image_processor_name[:-4]}.*?(\n\S|$)""
    match = re.search(regex, content_base_file, re.DOTALL)
    if not match:
        print(f""Couldn't find the {fast_image_processor_name[:-4]} class in {fast_image_processing_module_file}"")
        print(""Creating a new file with the default content."")
        return write_default_fast_image_processor_file(
            fast_image_processing_module_file, fast_image_processor_name, content_base_file
        )
    # Exclude the last unindented line
    slow_class_content = match.group(0).rstrip()
    # get default args:
    # find the __init__ block which start with def __init__ and ends with def
    match = re.search(r""def __init__.*?def "", slow_class_content, re.DOTALL)
    if not match:
        print(
            f""Couldn't find the __init__ block for {fast_image_processor_name[:-4]} in {fast_image_processing_module_file}""
        )
        print(""Creating a new file with the default content."")
        return write_default_fast_image_processor_file(
            fast_image_processing_module_file, fast_image_processor_name, content_base_file
        )
    init = match.group(0)
    init_signature_block = init.split("")"")[0]
    arg_names = init_signature_block.split("":"")
    arg_names = [arg_name.split(""\n"")[-1].strip() for arg_name in arg_names]
    # get the default values
    default_args = re.findall(r""= (.*?)(?:,|\))"", init_signature_block)

    # build default args dict
    default_args_dict = dict(zip(arg_names, default_args))
    pattern_default_size = r""size = size if size is not None else\s+(.*)""
    match_default_size = re.findall(pattern_default_size, init)
    default_args_dict[""size""] = match_default_size[0] if match_default_size else None
    pattern_default_crop_size = r""crop_size = crop_size if crop_size is not None else\s+(.*)""
    match_default_crop_size = re.findall(pattern_default_crop_size, init)
    default_args_dict[""crop_size""] = match_default_crop_size[0] if match_default_crop_size else None
    pattern_default_image_mean = r""self.image_mean = image_mean if image_mean is not None else\s+(.*)""
    match_default_image_mean = re.findall(pattern_default_image_mean, init)
    default_args_dict[""image_mean""] = match_default_image_mean[0] if match_default_image_mean else None
    pattern_default_image_std = r""self.image_std = image_std if image_std is not None else\s+(.*)""
    match_default_image_std = re.findall(pattern_default_image_std, init)
    default_args_dict[""image_std""] = match_default_image_std[0] if match_default_image_std else None
    default_args_dict[""default_to_square""] = False if ""(size, default_to_square=False"" in init else None

    content_header = get_fast_image_processing_content_header(content_base_file)
    content_base_file = (
        f""@auto_docstring\n""
        f""class {fast_image_processor_name}(BaseImageProcessorFast):\n""
        ""    # This generated class can be used as a starting point for the fast image processor.\n""
        ""    # if the image processor is only used for simple augmentations, such as resizing, center cropping, rescaling, or normalizing,\n""
        ""    # only the default values should be set in the class.\n""
        ""    # If the image processor requires more complex augmentations, methods from BaseImageProcessorFast can be overridden.\n""
        ""    # In most cases, only the `_preprocess` method should be overridden.\n\n""
        ""    # For an example of a fast image processor requiring more complex augmentations, see `LlavaNextImageProcessorFast`.\n\n""
        ""    # Default values should be checked against the slow image processor\n""
        ""    # None values left after checking can be removed\n""
        f""    resample = {default_args_dict.get('resample')}\n""
        f""    image_mean = {default_args_dict.get('image_mean')}\n""
        f""    image_std = {default_args_dict.get('image_std')}\n""
        f""    size = {default_args_dict.get('size')}\n""
        f""    default_to_square = {default_args_dict.get('default_to_square')}\n""
        f""    crop_size = {default_args_dict.get('crop_size')}\n""
        f""    do_resize = {default_args_dict.get('do_resize')}\n""
        f""    do_center_crop = {default_args_dict.get('do_center_crop')}\n""
        f""    do_rescale = {default_args_dict.get('do_rescale')}\n""
        f""    do_normalize = {default_args_dict.get('do_normalize')}\n""
        f""    do_convert_rgb = {default_args_dict.get('do_convert_rgb')}\n\n\n""
        f'__all__ = [""{fast_image_processor_name}""]\n'
    )

    imports = ""\n\nfrom ...image_processing_utils_fast import BaseImageProcessorFast\n""
    image_utils_imports = []
    if default_args_dict.get(""resample"") is not None and ""PILImageResampling"" in default_args_dict.get(""resample""):
        image_utils_imports.append(""PILImageResampling"")
    if default_args_dict.get(""image_mean"") is not None and not any(
        char.isdigit() for char in default_args_dict.get(""image_mean"")
    ):
        image_utils_imports.append(default_args_dict.get(""image_mean""))
    if default_args_dict.get(""image_std"") is not None and not any(
        char.isdigit() for char in default_args_dict.get(""image_std"")
    ):
        image_utils_imports.append(default_args_dict.get(""image_std""))

    if image_utils_imports:
        # sort imports
        image_utils_imports.sort()
        imports += f""from ...image_utils import {', '.join(image_utils_imports)}\n""

    imports += ""from ...utils import auto_docstring\n""

    content = content_header + imports + ""\n\n"" + content_base_file

    with open(fast_image_processing_module_file, ""w"", encoding=""utf-8"") as f:
        f.write(content)"
huggingface__transformers__src__transformers__cli__add_new_model_like.py__add_new_model_like,add_new_model_like,huggingface/transformers:src/transformers/cli/add_new_model_like.py,24,False,False,False,0,False,False,False,False,False,False,False,False,False,4,0,2,ml,"def add_new_model_like(
    repo_path: Annotated[
        str | None, typer.Argument(help=""When not using an editable install, the path to the Transformers repo."")
    ] = None,
):
    """"""
    Add a new model to the library, based on an existing one.
    """"""
    (
        old_model_infos,
        new_lowercase_name,
        new_model_paper_name,
        filenames_to_add,
        create_fast_image_processor,
    ) = get_user_input()

    _add_new_model_like_internal(
        repo_path=Path(repo_path) if repo_path is not None else REPO_PATH,
        old_model_infos=old_model_infos,
        new_lowercase_name=new_lowercase_name,
        new_model_paper_name=new_model_paper_name,
        filenames_to_add=filenames_to_add,
        create_fast_image_processor=create_fast_image_processor,
    )"
huggingface__transformers__src__transformers__cli__add_new_model_like.py__add_content_to_file,add_content_to_file,huggingface/transformers:src/transformers/cli/add_new_model_like.py,20,False,False,False,0,False,False,False,False,False,False,False,False,True,5,3,3,ml,"def add_content_to_file(file_name: str | os.PathLike, new_content: str, add_after: str):
    """"""
    A utility to add some content inside a given file.

    Args:
        file_name (`str` or `os.PathLike`):
            The name of the file in which we want to insert some content.
        new_content (`str`):
            The content to add.
       add_after (`str`):
           The new content is added just after the first instance matching it.
    """"""
    with open(file_name, ""r"", encoding=""utf-8"") as f:
        old_content = f.read()

    before, after = old_content.split(add_after, 1)
    new_content = before + add_after + new_content + after

    with open(file_name, ""w"", encoding=""utf-8"") as f:
        f.write(new_content)"
huggingface__transformers__src__transformers__cli__add_new_model_like.py__add_model_to_auto_mappings,add_model_to_auto_mappings,huggingface/transformers:src/transformers/cli/add_new_model_like.py,65,True,False,True,3,True,False,True,False,False,False,False,False,True,15,20,8,ml,"def add_model_to_auto_mappings(
    repo_path: Path,
    old_model_infos: ModelInfos,
    new_lowercase_name: str,
    new_model_paper_name: str,
    filenames_to_add: list[tuple[str, bool]],
):
    """"""
    Add a model to all the relevant mappings in the auto module.

    Args:
        old_model_infos (`ModelInfos`):
            The structure containing the class information of the old model.
        new_lowercase_name (`str`):
            The new lowercase model name.
        new_model_paper_name (`str`):
            The fully cased name (as in the official paper name) of the new model.
        filenames_to_add (`list[tuple[str, bool]]`):
            A list of tuples of all potential filenames to add for a new model, along a boolean flag describing if we
            should add this file or not. For example, [(`modeling_xxx.px`, True), (`configuration_xxx.py`, True), (`tokenization_xxx.py`, False),...]
    """"""
    new_cased_name = """".join(x.title() for x in new_lowercase_name.replace(""-"", ""_"").split(""_""))
    old_lowercase_name = old_model_infos.lowercase_name
    old_cased_name = old_model_infos.camelcase_name
    filenames_to_add = [
        (filename.replace(old_lowercase_name, ""auto""), to_add) for filename, to_add in filenames_to_add[1:]
    ]
    # fast tokenizer/image processor have the same auto mappings as normal ones
    corrected_filenames_to_add = []
    for file, to_add in filenames_to_add:
        if re.search(r""(?:tokenization)|(?:image_processing)_auto_fast.py"", file):
            previous_file, previous_to_add = corrected_filenames_to_add[-1]
            corrected_filenames_to_add[-1] = (previous_file, previous_to_add or to_add)
        else:
            corrected_filenames_to_add.append((file, to_add))

    # Add the config mappings directly as the handling for config is a bit different
    add_content_to_file(
        repo_path / ""src"" / ""transformers"" / ""models"" / ""auto"" / ""configuration_auto.py"",
        new_content=f'        (""{new_lowercase_name}"", ""{new_cased_name}Config""),\n',
        add_after=""CONFIG_MAPPING_NAMES = OrderedDict[str, str](\n    [\n        # Add configs here\n"",
    )
    add_content_to_file(
        repo_path / ""src"" / ""transformers"" / ""models"" / ""auto"" / ""configuration_auto.py"",
        new_content=f'        (""{new_lowercase_name}"", ""{new_model_paper_name}""),\n',
        add_after=""MODEL_NAMES_MAPPING = OrderedDict[str, str](\n    [\n        # Add full (and cased) model names here\n"",
    )

    for filename, to_add in corrected_filenames_to_add:
        if to_add:
            # The auto mapping
            filename = filename.replace(""_fast.py"", "".py"")
            file = (repo_path / ""src"" / ""transformers"" / ""models"" / ""auto"" / filename).read_text()
            # The regex has to be a bit complex like this as the tokenizer mapping has new lines everywhere
            matching_lines = re.findall(
                rf'( {{8,12}}\(\s*""{old_lowercase_name}"",.*?\),\n)(?: {{4,12}}\(|\])', file, re.DOTALL
            )
            for match in matching_lines:
                add_content_to_file(
                    repo_path / ""src"" / ""transformers"" / ""models"" / ""auto"" / filename,
                    new_content=match.replace(old_lowercase_name, new_lowercase_name).replace(
                        old_cased_name, new_cased_name
                    ),
                    add_after=match,
                )"
huggingface__transformers__src__transformers__cli__add_new_model_like.py__create_doc_file,create_doc_file,huggingface/transformers:src/transformers/cli/add_new_model_like.py,54,True,False,False,1,False,False,False,False,False,False,False,False,True,5,3,3,ml,"def create_doc_file(new_paper_name: str, public_classes: list[str]):
    """"""
    Create a new doc file to fill for the new model.

    Args:
        new_paper_name (`str`):
            The fully cased name (as in the official paper name) of the new model.
        public_classes (`list[str]`):
            A list of all the public classes that the model will have in the library.
    """"""
    added_note = (
        ""\n\n Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that ""
        ""may not be rendered properly in your Markdown viewer.\n\n-->\n\n""
    )
    copyright_for_markdown = re.sub(r""# ?"", """", COPYRIGHT).replace(""coding=utf-8\n"", ""<!--"") + added_note

    doc_template = textwrap.dedent(
        f""""""
        # {new_paper_name}

        ## Overview

        The {new_paper_name} model was proposed in [<INSERT PAPER NAME HERE>](<INSERT PAPER LINK HERE>) by <INSERT AUTHORS HERE>.
        <INSERT SHORT SUMMARY HERE>

        The abstract from the paper is the following:

        <INSERT PAPER ABSTRACT HERE>

        Tips:

        <INSERT TIPS ABOUT MODEL HERE>

        This model was contributed by [INSERT YOUR HF USERNAME HERE](https://huggingface.co/<INSERT YOUR HF USERNAME HERE>).
        The original code can be found [here](<INSERT LINK TO GITHUB REPO HERE>).

        ## Usage examples

        <INSERT SOME NICE EXAMPLES HERE>

        """"""
    )

    # Add public classes doc
    doc_for_classes = []
    for class_ in public_classes:
        doc = f""## {class_}\n\n[[autodoc]] {class_}""
        if ""Model"" in class_:
            doc += ""\n    - forward""
        doc_for_classes.append(doc)

    class_doc = ""\n\n"".join(doc_for_classes)

    return copyright_for_markdown + doc_template + class_doc"
huggingface__transformers__src__transformers__cli__add_new_model_like.py__insert_model_in_doc_toc,insert_model_in_doc_toc,huggingface/transformers:src/transformers/cli/add_new_model_like.py,23,False,False,False,0,False,False,False,False,False,False,False,False,True,5,8,2,ml,"def insert_model_in_doc_toc(
    repo_path: Path, old_lowercase_name: str, new_lowercase_name: str, new_model_paper_name: str
):
    """"""
    Insert the new model in the doc `_toctree.yaml`, in the same section as the old model.

    Args:
        old_lowercase_name (`str`):
            The old lowercase model name.
        new_lowercase_name (`str`):
            The old lowercase model name.
        new_model_paper_name (`str`):
            The fully cased name (as in the official paper name) of the new model.
    """"""
    toc_file = repo_path / ""docs"" / ""source"" / ""en"" / ""_toctree.yml""
    with open(toc_file, ""r"") as f:
        content = f.read()

    old_model_toc = re.search(rf""- local: model_doc/{old_lowercase_name}\n {{8}}title: .*?\n"", content).group(0)
    new_toc = f""      - local: model_doc/{new_lowercase_name}\n        title: {new_model_paper_name}\n""
    add_content_to_file(
        repo_path / ""docs"" / ""source"" / ""en"" / ""_toctree.yml"", new_content=new_toc, add_after=old_model_toc
    )"
huggingface__transformers__src__transformers__cli__add_new_model_like.py__create_init_file,create_init_file,huggingface/transformers:src/transformers/cli/add_new_model_like.py,36,False,False,False,0,True,False,True,False,False,False,False,False,True,4,1,3,ml,"def create_init_file(old_lowercase_name: str, new_lowercase_name: str, filenames_to_add: list[tuple[str, bool]]):
    """"""
    Create the `__init__.py` file to add in the new model folder.

    Args:
        old_lowercase_name (`str`):
            The old lowercase model name.
        new_lowercase_name (`str`):
            The new lowercase model name.
        filenames_to_add (`list[tuple[str, bool]]`):
            A list of tuples of all potential filenames to add for a new model, along a boolean flag describing if we
            should add this file or not. For example, [(`modeling_xxx.px`, True), (`configuration_xxx.py`, True), (`tokenization_xxx.py`, False),...]
    """"""
    filenames_to_add = [
        (filename.replace(old_lowercase_name, new_lowercase_name).replace("".py"", """"), to_add)
        for filename, to_add in filenames_to_add
    ]
    imports = ""\n            "".join(f""from .{file} import *"" for file, to_add in filenames_to_add if to_add)
    init_file = COPYRIGHT + textwrap.dedent(
        f""""""
        from typing import TYPE_CHECKING

        from ...utils import _LazyModule
        from ...utils.import_utils import define_import_structure


        if TYPE_CHECKING:
            {imports}
        else:
            import sys

            _file = globals()[""__file__""]
            sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)
        """"""
    )
    return init_file"
huggingface__transformers__src__transformers__cli__add_new_model_like.py__find_all_classes_from_file,find_all_classes_from_file,huggingface/transformers:src/transformers/cli/add_new_model_like.py,14,False,False,False,0,False,False,False,False,False,False,False,False,False,5,0,2,ml,"def find_all_classes_from_file(module_name: str) -> set:
    """"""
    Find the name of all classes defined in `module_name`, including public ones (defined in `__all__`).

    Args:
        module_name (`str`):
            The full path to the python module from which to extract classes.
    """"""
    with open(module_name, ""r"", encoding=""utf-8"") as file:
        source_code = file.read()
    module = cst.parse_module(source_code)
    visitor = ClassFinder()
    module.visit(visitor)
    return visitor.classes, visitor.public_classes"
huggingface__transformers__src__transformers__cli__add_new_model_like.py__find_modular_structure,find_modular_structure,huggingface/transformers:src/transformers/cli/add_new_model_like.py,23,False,False,False,0,True,False,True,False,False,False,False,False,True,7,0,4,ml,"def find_modular_structure(
    module_name: str, old_model_infos: ModelInfos, new_cased_name: str
) -> tuple[str, str, list]:
    """"""
    Extract the modular structure that will be needed to copy a file `module_name` using modular.

    Args:
        module_name (`str`):
            The full path to the python module to copy with modular.
        old_model_infos (`ModelInfos`):
            The structure containing the class information of the old model.
        new_cased_name (`str`):
            The new cased model name.
    """"""
    all_classes, public_classes = find_all_classes_from_file(module_name)
    import_location = ""."".join(module_name.parts[-2:]).replace("".py"", """")
    old_cased_name = old_model_infos.camelcase_name
    imports = f""from ..{import_location} import {', '.join(class_ for class_ in all_classes)}""
    modular_classes = ""\n\n"".join(
        f""class {class_.replace(old_cased_name, new_cased_name)}({class_}):\n    pass"" for class_ in all_classes
    )
    public_classes = [class_.replace(old_cased_name, new_cased_name) for class_ in public_classes]
    return imports, modular_classes, public_classes"
huggingface__transformers__src__transformers__cli__add_new_model_like.py__create_modular_file,create_modular_file,huggingface/transformers:src/transformers/cli/add_new_model_like.py,51,True,False,False,1,True,False,True,False,False,False,False,False,True,9,8,6,ml,"def create_modular_file(
    repo_path: Path,
    old_model_infos: ModelInfos,
    new_lowercase_name: str,
    filenames_to_add: list[tuple[str, bool]],
) -> str:
    """"""
    Create a new modular file which will copy the old model, based on the new name and the different filenames
    (modules) to add.

    Args:
        old_model_infos (`ModelInfos`):
            The structure containing the class information of the old model.
        new_lowercase_name (`str`):
            The new lowercase model name.
        filenames_to_add (`list[tuple[str, bool]]`):
            A list of tuples of all potential filenames to add for a new model, along a boolean flag describing if we
            should add this file or not. For example, [(`modeling_xxx.px`, True), (`configuration_xxx.py`, True), (`tokenization_xxx.py`, False),...]
    """"""
    new_cased_name = """".join(x.title() for x in new_lowercase_name.replace(""-"", ""_"").split(""_""))
    old_lowercase_name = old_model_infos.lowercase_name
    old_folder_root = repo_path / ""src"" / ""transformers"" / ""models"" / old_lowercase_name

    # Construct the modular file from the original (old) model, by subclassing each class
    all_imports = """"
    all_bodies = """"
    all_public_classes = []
    for filename, to_add in filenames_to_add:
        if to_add:
            imports, body, public_classes = find_modular_structure(
                old_folder_root / filename, old_model_infos, new_cased_name
            )
            all_imports += f""\n{imports}""
            all_bodies += f""\n\n{body}""
            all_public_classes.extend(public_classes)

    # Create the __all__ assignment
    public_classes_formatted = ""\n            "".join(f""{public_class},"" for public_class in all_public_classes)
    all_statement = textwrap.dedent(
        f""""""

        __all__ = [
            {public_classes_formatted}
        ]
        """"""
    )
    # Create the whole modular file
    modular_file = COPYRIGHT + all_imports + all_bodies + all_statement
    # Remove outer explicit quotes """" around the public class names before returning them
    all_public_classes = [public_class.replace('""', """") for public_class in all_public_classes]
    return modular_file, all_public_classes"
huggingface__transformers__src__transformers__cli__add_new_model_like.py__create_test_files,create_test_files,huggingface/transformers:src/transformers/cli/add_new_model_like.py,50,True,True,True,3,True,False,True,False,False,False,False,False,True,15,6,10,ml,"def create_test_files(
    repo_path: Path, old_model_infos: ModelInfos, new_lowercase_name, filenames_to_add: list[tuple[str, bool]]
):
    """"""
    Create the test files for the new model. It basically copies over the old test files and adjust the class names.

    Args:
        old_model_infos (`ModelInfos`):
            The structure containing the class information of the old model.
        new_lowercase_name (`str`):
            The new lowercase model name.
        filenames_to_add (`list[tuple[str, bool]]`):
            A list of tuples of all potential filenames to add for a new model, along a boolean flag describing if we
            should add this file or not. For example, [(`modeling_xxx.px`, True), (`configuration_xxx.py`, True), (`tokenization_xxx.py`, False),...]
    """"""
    new_cased_name = """".join(x.title() for x in new_lowercase_name.replace(""-"", ""_"").split(""_""))
    old_lowercase_name = old_model_infos.lowercase_name
    old_cased_name = old_model_infos.camelcase_name
    filenames_to_add = [
        (""test_"" + filename.replace(old_lowercase_name, new_lowercase_name), to_add)
        for filename, to_add in filenames_to_add[1:]
    ]
    # fast tokenizer/image processor have the same test files as normal ones
    corrected_filenames_to_add = []
    for file, to_add in filenames_to_add:
        if re.search(rf""test_(?:tokenization)|(?:image_processing)_{new_lowercase_name}_fast.py"", file):
            previous_file, previous_to_add = corrected_filenames_to_add[-1]
            corrected_filenames_to_add[-1] = (previous_file, previous_to_add or to_add)
        else:
            corrected_filenames_to_add.append((file, to_add))

    test_files = {}
    for new_file, to_add in corrected_filenames_to_add:
        if to_add:
            original_test_file = new_file.replace(new_lowercase_name, old_lowercase_name)
            original_test_path = repo_path / ""tests"" / ""models"" / old_lowercase_name / original_test_file
            # Sometimes, tests may not exist
            if not original_test_path.is_file():
                continue
            with open(original_test_path, ""r"") as f:
                test_code = f.read()
            # Remove old copyright and add new one
            test_lines = test_code.split(""\n"")
            idx = 0
            while test_lines[idx].startswith(""#""):
                idx += 1
            test_code = COPYRIGHT + ""\n"".join(test_lines[idx:])
            test_files[new_file] = test_code.replace(old_cased_name, new_cased_name)

    return test_files"
huggingface__transformers__src__transformers__cli__add_new_model_like.py__get_user_field,get_user_field,huggingface/transformers:src/transformers/cli/add_new_model_like.py,47,False,True,False,1,False,False,False,False,False,False,False,False,True,5,1,8,ml,"def get_user_field(
    question: str,
    default_value: str | None = None,
    convert_to: Callable | None = None,
    fallback_message: str | None = None,
) -> Any:
    """"""
    A utility function that asks a question to the user to get an answer, potentially looping until it gets a valid
    answer.

    Args:
        question (`str`):
            The question to ask the user.
        default_value (`str`, *optional*):
            A potential default value that will be used when the answer is empty.
        convert_to (`Callable`, *optional*):
            If set, the answer will be passed to this function. If this function raises an error on the provided
            answer, the question will be asked again.
        fallback_message (`str`, *optional*):
            A message that will be displayed each time the question is asked again to the user.

    Returns:
        `Any`: The answer provided by the user (or the default), passed through the potential conversion function.
    """"""
    if not question.endswith("" ""):
        question = question + "" ""
    if default_value is not None:
        question = f""{question} [{default_value}] ""

    valid_answer = False
    while not valid_answer:
        answer = input(question)
        if default_value is not None and len(answer) == 0:
            answer = default_value
        if convert_to is not None:
            try:
                answer = convert_to(answer)
                valid_answer = True
            except Exception:
                valid_answer = False
        else:
            valid_answer = True

        if not valid_answer:
            print(fallback_message)

    return answer"
huggingface__transformers__src__transformers__cli__add_new_model_like.py__get_user_input,get_user_input,huggingface/transformers:src/transformers/cli/add_new_model_like.py,111,False,True,False,1,False,False,True,False,False,False,False,False,True,23,0,14,ml,"def get_user_input():
    """"""
    Ask the user for the necessary inputs to add the new model.
    """"""
    from transformers.models.auto.configuration_auto import MODEL_NAMES_MAPPING

    model_types = list(MODEL_NAMES_MAPPING.keys())

    # Get old model type
    valid_model_type = False
    while not valid_model_type:
        old_model_type = input(
            ""What model would you like to duplicate? Please provide it as lowercase, e.g. `llama`): ""
        )
        if old_model_type in model_types:
            valid_model_type = True
        else:
            print(f""{old_model_type} is not a valid model type."")
            near_choices = difflib.get_close_matches(old_model_type, model_types)
            if len(near_choices) >= 1:
                if len(near_choices) > 1:
                    near_choices = "" or "".join(near_choices)
                print(f""Did you mean {near_choices}?"")

    old_model_infos = ModelInfos(old_model_type)

    # Ask for the new model name
    new_lowercase_name = get_user_field(
        ""What is the new model name? Please provide it as snake lowercase, e.g. `new_model`?""
    )
    new_model_paper_name = get_user_field(
        ""What is the fully cased name you would like to appear in the doc (e.g. `NeW ModEl`)? "",
        default_value="""".join(x.title() for x in new_lowercase_name.split(""_"")),
    )

    # Ask if we want to add individual processor classes as well
    add_tokenizer = False
    add_fast_tokenizer = False
    add_image_processor = False
    add_fast_image_processor = False
    add_video_processor = False
    add_feature_extractor = False
    add_processor = False
    if old_model_infos.tokenizer_class is not None:
        add_tokenizer = get_user_field(
            f""Do you want to create a new tokenizer? If `no`, it will use the same as {old_model_type} (y/n)?"",
            convert_to=convert_to_bool,
            fallback_message=""Please answer yes/no, y/n, true/false or 1/0. "",
        )
    if old_model_infos.fast_tokenizer_class is not None:
        add_fast_tokenizer = get_user_field(
            f""Do you want to create a new fast tokenizer? If `no`, it will use the same as {old_model_type} (y/n)?"",
            convert_to=convert_to_bool,
            fallback_message=""Please answer yes/no, y/n, true/false or 1/0. "",
        )
    if old_model_infos.image_processor_class is not None:
        add_image_processor = get_user_field(
            f""Do you want to create a new image processor? If `no`, it will use the same as {old_model_type} (y/n)?"",
            convert_to=convert_to_bool,
            fallback_message=""Please answer yes/no, y/n, true/false or 1/0. "",
        )
    if old_model_infos.fast_image_processor_class is not None:
        add_fast_image_processor = get_user_field(
            f""Do you want to create a new fast image processor? If `no`, it will use the same as {old_model_type} (y/n)?"",
            convert_to=convert_to_bool,
            fallback_message=""Please answer yes/no, y/n, true/false or 1/0. "",
        )
    if old_model_infos.video_processor_class is not None:
        add_video_processor = get_user_field(
            f""Do you want to create a new video processor? If `no`, it will use the same as {old_model_type} (y/n)?"",
            convert_to=convert_to_bool,
            fallback_message=""Please answer yes/no, y/n, true/false or 1/0. "",
        )
    if old_model_infos.feature_extractor_class is not None:
        add_feature_extractor = get_user_field(
            f""Do you want to create a new feature extractor? If `no`, it will use the same as {old_model_type} (y/n)?"",
            convert_to=convert_to_bool,
            fallback_message=""Please answer yes/no, y/n, true/false or 1/0. "",
        )
    if old_model_infos.processor_class is not None:
        add_processor = get_user_field(
            f""Do you want to create a new processor? If `no`, it will use the same as {old_model_type} (y/n)?"",
            convert_to=convert_to_bool,
            fallback_message=""Please answer yes/no, y/n, true/false or 1/0. "",
        )

    old_lowercase_name = old_model_infos.lowercase_name
    # A list of the old filenames, along whether we should copy them or not
    filenames_to_add = (
        (f""configuration_{old_lowercase_name}.py"", True),
        (f""modeling_{old_lowercase_name}.py"", True),
        (f""tokenization_{old_lowercase_name}.py"", add_tokenizer),
        (f""tokenization_{old_lowercase_name}_fast.py"", add_fast_tokenizer),
        (f""image_processing_{old_lowercase_name}.py"", add_image_processor),
        (f""image_processing_{old_lowercase_name}_fast.py"", add_fast_image_processor),
        (f""video_processing_{old_lowercase_name}.py"", add_video_processor),
        (f""feature_extraction_{old_lowercase_name}.py"", add_feature_extractor),
        (f""processing_{old_lowercase_name}.py"", add_processor),
    )

    create_fast_image_processor = False
    if add_image_processor and not add_fast_image_processor:
        create_fast_image_processor = get_user_field(
            ""A fast image processor can be created from the slow one, but modifications might be needed. ""
            ""Should we add a fast image processor class for this model (recommended) (y/n)? "",
            convert_to=convert_to_bool,
            default_value=""y"",
            fallback_message=""Please answer yes/no, y/n, true/false or 1/0."",
        )

    return old_model_infos, new_lowercase_name, new_model_paper_name, filenames_to_add, create_fast_image_processor"
huggingface__transformers__src__transformers__cli__chat.py__load_generation_config,load_generation_config,huggingface/transformers:src/transformers/cli/chat.py,10,False,False,False,0,False,False,False,False,False,False,False,False,False,5,0,3,ml,"def load_generation_config(generation_config: str | None) -> GenerationConfig:
    if generation_config is None:
        return GenerationConfig()

    if "".json"" in generation_config:  # is a local file
        dirname = os.path.dirname(generation_config)
        filename = os.path.basename(generation_config)
        return GenerationConfig.from_pretrained(dirname, filename)
    else:
        return GenerationConfig.from_pretrained(generation_config)"
huggingface__transformers__src__transformers__cli__chat.py__parse_generate_flags,parse_generate_flags,huggingface/transformers:src/transformers/cli/chat.py,52,False,False,False,0,True,True,False,False,False,False,False,False,True,22,4,11,ml,"def parse_generate_flags(generate_flags: list[str] | None) -> dict:
    """"""Parses the generate flags from the user input into a dictionary of `generate` kwargs.""""""
    if generate_flags is None or len(generate_flags) == 0:
        return {}

    # Assumption: `generate_flags` is a list of strings, each string being a `flag=value` pair, that can be parsed
    # into a json string if we:
    # 1. Add quotes around each flag name
    generate_flags_as_dict = {'""' + flag.split(""="")[0] + '""': flag.split(""="")[1] for flag in generate_flags}

    # 2. Handle types:
    # 2. a. booleans should be lowercase, None should be null
    generate_flags_as_dict = {
        k: v.lower() if v.lower() in [""true"", ""false""] else v for k, v in generate_flags_as_dict.items()
    }
    generate_flags_as_dict = {k: ""null"" if v == ""None"" else v for k, v in generate_flags_as_dict.items()}

    # 2. b. strings should be quoted
    def is_number(s: str) -> bool:
        # handle negative numbers
        s = s.removeprefix(""-"")
        return s.replace(""."", """", 1).isdigit()

    generate_flags_as_dict = {k: f'""{v}""' if not is_number(v) else v for k, v in generate_flags_as_dict.items()}
    # 2. c. [no processing needed] lists are lists of ints because `generate` doesn't take lists of strings :)
    # We also mention in the help message that we only accept lists of ints for now.

    # 3. Join the result into a comma separated string
    generate_flags_string = "", "".join([f""{k}: {v}"" for k, v in generate_flags_as_dict.items()])

    # 4. Add the opening/closing brackets
    generate_flags_string = ""{"" + generate_flags_string + ""}""

    # 5. Remove quotes around boolean/null and around lists
    generate_flags_string = generate_flags_string.replace('""null""', ""null"")
    generate_flags_string = generate_flags_string.replace('""true""', ""true"")
    generate_flags_string = generate_flags_string.replace('""false""', ""false"")
    generate_flags_string = generate_flags_string.replace('""[', ""["")
    generate_flags_string = generate_flags_string.replace(']""', ""]"")

    # 6. Replace the `=` with `:`
    generate_flags_string = generate_flags_string.replace(""="", "":"")

    try:
        processed_generate_flags = json.loads(generate_flags_string)
    except json.JSONDecodeError:
        raise ValueError(
            ""Failed to convert `generate_flags` into a valid JSON object.""
            ""\n`generate_flags` = {generate_flags}""
            ""\nConverted JSON string = {generate_flags_string}""
        )
    return processed_generate_flags"
huggingface__transformers__src__transformers__cli__system.py__env,env,huggingface/transformers:src/transformers/cli/system.py,88,False,False,False,0,True,False,False,False,False,True,False,False,True,26,0,17,ml,"def env(
    accelerate_config_file: Annotated[
        str | None,
        typer.Argument(help=""The accelerate config file to use for the default values in the launching script.""),
    ] = None,
) -> None:
    """"""Print information about the environment.""""""
    import safetensors

    safetensors_version = safetensors.__version__

    accelerate_version = ""not installed""
    accelerate_config = accelerate_config_str = ""not found""

    if is_accelerate_available():
        import accelerate
        from accelerate.commands.config import default_config_file, load_config_from_file

        accelerate_version = accelerate.__version__
        # Get the default from the config file.
        if accelerate_config_file is not None or os.path.isfile(default_config_file):
            accelerate_config = load_config_from_file(accelerate_config_file).to_dict()

        accelerate_config_str = (
            ""\n"".join([f""\t- {prop}: {val}"" for prop, val in accelerate_config.items()])
            if isinstance(accelerate_config, dict)
            else f""\t{accelerate_config}""
        )

    pt_version = ""not installed""
    pt_cuda_available = ""NA""
    pt_accelerator = ""NA""
    if is_torch_available():
        import torch

        pt_version = torch.__version__
        pt_cuda_available = torch.cuda.is_available()
        pt_xpu_available = is_torch_xpu_available()
        pt_npu_available = is_torch_npu_available()
        pt_hpu_available = is_torch_hpu_available()

        if pt_cuda_available:
            pt_accelerator = ""CUDA""
        elif pt_xpu_available:
            pt_accelerator = ""XPU""
        elif pt_npu_available:
            pt_accelerator = ""NPU""
        elif pt_hpu_available:
            pt_accelerator = ""HPU""

    deepspeed_version = ""not installed""
    if is_deepspeed_available():
        # Redirect command line output to silence deepspeed import output.
        with contextlib.redirect_stdout(io.StringIO()):
            import deepspeed
        deepspeed_version = deepspeed.__version__

    info = {
        ""`transformers` version"": __version__,
        ""Platform"": platform.platform(),
        ""Python version"": platform.python_version(),
        ""Huggingface_hub version"": huggingface_hub.__version__,
        ""Safetensors version"": f""{safetensors_version}"",
        ""Accelerate version"": f""{accelerate_version}"",
        ""Accelerate config"": f""{accelerate_config_str}"",
        ""DeepSpeed version"": f""{deepspeed_version}"",
        ""PyTorch version (accelerator?)"": f""{pt_version} ({pt_accelerator})"",
        ""Using distributed or parallel set-up in script?"": ""<fill in>"",
    }
    if is_torch_available():
        if pt_cuda_available:
            info[""Using GPU in script?""] = ""<fill in>""
            info[""GPU type""] = torch.cuda.get_device_name()
        elif pt_xpu_available:
            info[""Using XPU in script?""] = ""<fill in>""
            info[""XPU type""] = torch.xpu.get_device_name()
        elif pt_hpu_available:
            info[""Using HPU in script?""] = ""<fill in>""
            info[""HPU type""] = torch.hpu.get_device_name()
        elif pt_npu_available:
            info[""Using NPU in script?""] = ""<fill in>""
            info[""NPU type""] = torch.npu.get_device_name()
            info[""CANN version""] = torch.version.cann

    print(""\nCopy-and-paste the text below in your GitHub issue and FILL OUT the two last points.\n"")
    print(_format_dict(info))

    return info"
huggingface__transformers__src__transformers__conversion_mapping.py__get_model_conversion_mapping,get_model_conversion_mapping,huggingface/transformers:src/transformers/conversion_mapping.py,47,False,False,False,0,True,False,True,False,False,False,False,False,True,13,0,11,other,"def get_model_conversion_mapping(
    model: PreTrainedModel,
    key_mapping: dict[str, str] | None = None,
    hf_quantizer: HfQuantizer | None = None,
    add_legacy: bool = True,
) -> list[WeightConverter | WeightRenaming]:
    """"""
    For a given `model`, obtain the weight conversion mapping if any are registered either as a simple renaming
    `_checkpoint_conversion_mapping` class argument, or in the general WeightConverter mapping.
    """"""
    weight_conversions = []

    # Load models with explicit, user-provided key mapping
    if key_mapping is not None:
        weight_conversions = [WeightRenaming(source_patterns=k, target_patterns=v) for k, v in key_mapping.items()]
    elif any(
        allowed_name in class_name.__name__.lower()
        for class_name in model.__class__.__mro__[:-1]
        for allowed_name in VLMS
    ):
        weight_conversions = [
            WeightRenaming(source_patterns=k, target_patterns=v)
            for k, v in model._checkpoint_conversion_mapping.items()
        ]

    # TODO: should be checked recursively on submodels!!
    model_type = getattr(model.config, ""model_type"", None)
    if model_type is not None:
        model_specific_conversions = get_checkpoint_conversion_mapping(model_type)
        if model_specific_conversions is not None:
            weight_conversions.extend(model_specific_conversions)

    if add_legacy:
        weight_conversions.extend(get_checkpoint_conversion_mapping(""legacy""))

    # Add the ones from the quantizer as well if provided
    if hf_quantizer is not None:
        # NOTE: Since get_weight_conversions() only serve to dequantize, we would normally want to apply them first.
        # However, for now it's not possible to cascade converters (i.e., applying model-specific conversions on top
        # of tensors created by the dequantization conversions)
        # This means that if a model has model-specific conversions and is being dequantized, the model-specific conversion
        # that relies on tensors created by dequantization conversions will not be applied.
        # GptOss example: with Mxfp4Config(dequantize=True), Force16BytesAlignment converters are ignored because the tensors
        # ""mlp.experts.gate_up_proj$"" and ""mlp.experts.down_proj$"" are only created after dequantization conversions are applied.
        weight_conversions.extend(hf_quantizer.get_weight_conversions())

    return weight_conversions"
huggingface__transformers__src__transformers__convert_slow_tokenizers_checkpoints_to_fast.py__convert_slow_checkpoint_to_fast,convert_slow_checkpoint_to_fast,huggingface/transformers:src/transformers/convert_slow_tokenizers_checkpoints_to_fast.py,71,True,False,True,3,False,False,False,False,False,False,False,False,True,32,1,13,ml,"def convert_slow_checkpoint_to_fast(tokenizer_name, checkpoint_name, dump_path, force_download):
    if tokenizer_name is not None and tokenizer_name not in TOKENIZER_CLASSES:
        raise ValueError(f""Unrecognized tokenizer name, should be one of {list(TOKENIZER_CLASSES.keys())}."")

    if tokenizer_name is None:
        tokenizer_names = TOKENIZER_CLASSES
    else:
        tokenizer_names = {tokenizer_name: getattr(transformers, tokenizer_name + ""Fast"")}

    logger.info(f""Loading tokenizer classes: {tokenizer_names}"")

    for tokenizer_name in tokenizer_names:
        tokenizer_class = TOKENIZER_CLASSES[tokenizer_name]

        add_prefix = True
        if checkpoint_name is None:
            checkpoint_names = list(tokenizer_class.max_model_input_sizes.keys())
        else:
            checkpoint_names = [checkpoint_name]

        logger.info(f""For tokenizer {tokenizer_class.__class__.__name__} loading checkpoints: {checkpoint_names}"")

        for checkpoint in checkpoint_names:
            logger.info(f""Loading {tokenizer_class.__class__.__name__} {checkpoint}"")

            # Load tokenizer
            tokenizer = tokenizer_class.from_pretrained(checkpoint, force_download=force_download)

            # Save fast tokenizer
            logger.info(f""Save fast tokenizer to {dump_path} with prefix {checkpoint} add_prefix {add_prefix}"")

            # For organization names we create sub-directories
            if ""/"" in checkpoint:
                checkpoint_directory, checkpoint_prefix_name = checkpoint.split(""/"")
                dump_path_full = os.path.join(dump_path, checkpoint_directory)

                # Security check
                try:
                    Path(dump_path_full).resolve().relative_to(Path(dump_path).resolve())
                except ValueError:
                    raise ValueError(
                        f""Invalid checkpoint path: '{checkpoint}' attempts to escape `dump_path`: {dump_path}""
                    )

            elif add_prefix:
                checkpoint_prefix_name = checkpoint
                dump_path_full = dump_path
            else:
                checkpoint_prefix_name = None
                dump_path_full = dump_path

            logger.info(f""=> {dump_path_full} with prefix {checkpoint_prefix_name}, add_prefix {add_prefix}"")

            if checkpoint in list(tokenizer.pretrained_vocab_files_map.values())[0]:
                file_path = list(tokenizer.pretrained_vocab_files_map.values())[0][checkpoint]
                next_char = file_path.split(checkpoint)[-1][0]
                if next_char == ""/"":
                    dump_path_full = os.path.join(dump_path_full, checkpoint_prefix_name)
                    checkpoint_prefix_name = None

                logger.info(f""=> {dump_path_full} with prefix {checkpoint_prefix_name}, add_prefix {add_prefix}"")

            file_names = tokenizer.save_pretrained(
                dump_path_full, legacy_format=False, filename_prefix=checkpoint_prefix_name
            )
            logger.info(f""=> File names {file_names}"")

            for file_name in file_names:
                if not file_name.endswith(""tokenizer.json""):
                    os.remove(file_name)
                    logger.info(f""=> removing {file_name}"")"
huggingface__transformers__src__transformers__data__metrics____init__.py__glue_compute_metrics,glue_compute_metrics,huggingface/transformers:src/transformers/data/metrics/__init__.py,28,False,False,False,0,False,False,False,False,False,False,False,True,True,18,0,13,ml,"def glue_compute_metrics(task_name, preds, labels):
    warnings.warn(DEPRECATION_WARNING, FutureWarning)
    requires_backends(glue_compute_metrics, ""sklearn"")
    assert len(preds) == len(labels), f""Predictions and labels have mismatched lengths {len(preds)} and {len(labels)}""
    if task_name == ""cola"":
        return {""mcc"": matthews_corrcoef(labels, preds)}
    elif task_name == ""sst-2"":
        return {""acc"": simple_accuracy(preds, labels)}
    elif task_name == ""mrpc"":
        return acc_and_f1(preds, labels)
    elif task_name == ""sts-b"":
        return pearson_and_spearman(preds, labels)
    elif task_name == ""qqp"":
        return acc_and_f1(preds, labels)
    elif task_name == ""mnli"":
        return {""mnli/acc"": simple_accuracy(preds, labels)}
    elif task_name == ""mnli-mm"":
        return {""mnli-mm/acc"": simple_accuracy(preds, labels)}
    elif task_name == ""qnli"":
        return {""acc"": simple_accuracy(preds, labels)}
    elif task_name == ""rte"":
        return {""acc"": simple_accuracy(preds, labels)}
    elif task_name == ""wnli"":
        return {""acc"": simple_accuracy(preds, labels)}
    elif task_name == ""hans"":
        return {""acc"": simple_accuracy(preds, labels)}
    else:
        raise KeyError(task_name)"
huggingface__transformers__src__transformers__data__metrics__squad_metrics.py__normalize_answer,normalize_answer,huggingface/transformers:src/transformers/data/metrics/squad_metrics.py,18,False,False,False,0,False,False,True,False,False,False,False,False,True,11,0,2,other,"def normalize_answer(s):
    """"""Lower text and remove punctuation, articles and extra whitespace.""""""

    def remove_articles(text):
        regex = re.compile(r""\b(a|an|the)\b"", re.UNICODE)
        return re.sub(regex, "" "", text)

    def white_space_fix(text):
        return "" "".join(text.split())

    def remove_punc(text):
        exclude = set(string.punctuation)
        return """".join(ch for ch in text if ch not in exclude)

    def lower(text):
        return text.lower()

    return white_space_fix(remove_articles(remove_punc(lower(s))))"
huggingface__transformers__src__transformers__data__metrics__squad_metrics.py__compute_f1,compute_f1,huggingface/transformers:src/transformers/data/metrics/squad_metrics.py,14,False,False,False,0,False,False,False,False,False,False,False,False,False,11,8,3,other,"def compute_f1(a_gold, a_pred):
    gold_toks = get_tokens(a_gold)
    pred_toks = get_tokens(a_pred)
    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)
    num_same = sum(common.values())
    if len(gold_toks) == 0 or len(pred_toks) == 0:
        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise
        return int(gold_toks == pred_toks)
    if num_same == 0:
        return 0
    precision = 1.0 * num_same / len(pred_toks)
    recall = 1.0 * num_same / len(gold_toks)
    f1 = (2 * precision * recall) / (precision + recall)
    return f1"
huggingface__transformers__src__transformers__data__metrics__squad_metrics.py__get_raw_scores,get_raw_scores,huggingface/transformers:src/transformers/data/metrics/squad_metrics.py,24,True,False,False,1,True,False,True,False,False,False,False,False,True,6,0,7,other,"def get_raw_scores(examples, preds):
    """"""
    Computes the exact and f1 scores from the examples and the model predictions
    """"""
    exact_scores = {}
    f1_scores = {}

    for example in examples:
        qas_id = example.qas_id
        gold_answers = [answer[""text""] for answer in example.answers if normalize_answer(answer[""text""])]

        if not gold_answers:
            # For unanswerable questions, only correct answer is empty string
            gold_answers = [""""]

        if qas_id not in preds:
            print(f""Missing prediction for {qas_id}"")
            continue

        prediction = preds[qas_id]
        exact_scores[qas_id] = max(compute_exact(a, prediction) for a in gold_answers)
        f1_scores[qas_id] = max(compute_f1(a, prediction) for a in gold_answers)

    return exact_scores, f1_scores"
huggingface__transformers__src__transformers__data__metrics__squad_metrics.py__make_eval_dict,make_eval_dict,huggingface/transformers:src/transformers/data/metrics/squad_metrics.py,19,False,False,False,0,False,False,True,False,False,False,False,False,False,10,8,4,other,"def make_eval_dict(exact_scores, f1_scores, qid_list=None):
    if not qid_list:
        total = len(exact_scores)
        return collections.OrderedDict(
            [
                (""exact"", 100.0 * sum(exact_scores.values()) / total),
                (""f1"", 100.0 * sum(f1_scores.values()) / total),
                (""total"", total),
            ]
        )
    else:
        total = len(qid_list)
        return collections.OrderedDict(
            [
                (""exact"", 100.0 * sum(exact_scores[k] for k in qid_list) / total),
                (""f1"", 100.0 * sum(f1_scores[k] for k in qid_list) / total),
                (""total"", total),
            ]
        )"
huggingface__transformers__src__transformers__data__metrics__squad_metrics.py__find_best_thresh_v2,find_best_thresh_v2,huggingface/transformers:src/transformers/data/metrics/squad_metrics.py,32,True,False,False,2,False,False,True,False,False,False,False,False,False,3,4,10,other,"def find_best_thresh_v2(preds, scores, na_probs, qid_to_has_ans):
    num_no_ans = sum(1 for k in qid_to_has_ans if not qid_to_has_ans[k])
    cur_score = num_no_ans
    best_score = cur_score
    best_thresh = 0.0
    qid_list = sorted(na_probs, key=lambda k: na_probs[k])
    for qid in qid_list:
        if qid not in scores:
            continue
        if qid_to_has_ans[qid]:
            diff = scores[qid]
        else:
            if preds[qid]:
                diff = -1
            else:
                diff = 0
        cur_score += diff
        if cur_score > best_score:
            best_score = cur_score
            best_thresh = na_probs[qid]

    has_ans_score, has_ans_cnt = 0, 0
    for qid in qid_list:
        if not qid_to_has_ans[qid]:
            continue
        has_ans_cnt += 1

        if qid not in scores:
            continue
        has_ans_score += scores[qid]

    return 100.0 * best_score / len(scores), best_thresh, 1.0 * has_ans_score / has_ans_cnt"
huggingface__transformers__src__transformers__data__metrics__squad_metrics.py__find_best_thresh,find_best_thresh,huggingface/transformers:src/transformers/data/metrics/squad_metrics.py,21,True,False,False,1,False,False,True,False,False,False,False,False,False,4,2,7,other,"def find_best_thresh(preds, scores, na_probs, qid_to_has_ans):
    num_no_ans = sum(1 for k in qid_to_has_ans if not qid_to_has_ans[k])
    cur_score = num_no_ans
    best_score = cur_score
    best_thresh = 0.0
    qid_list = sorted(na_probs, key=lambda k: na_probs[k])
    for _, qid in enumerate(qid_list):
        if qid not in scores:
            continue
        if qid_to_has_ans[qid]:
            diff = scores[qid]
        else:
            if preds[qid]:
                diff = -1
            else:
                diff = 0
        cur_score += diff
        if cur_score > best_score:
            best_score = cur_score
            best_thresh = na_probs[qid]
    return 100.0 * best_score / len(scores), best_thresh"
huggingface__transformers__src__transformers__data__metrics__squad_metrics.py__squad_evaluate,squad_evaluate,huggingface/transformers:src/transformers/data/metrics/squad_metrics.py,29,False,False,False,0,True,True,False,False,False,False,False,False,False,13,0,8,other,"def squad_evaluate(examples, preds, no_answer_probs=None, no_answer_probability_threshold=1.0):
    qas_id_to_has_answer = {example.qas_id: bool(example.answers) for example in examples}
    has_answer_qids = [qas_id for qas_id, has_answer in qas_id_to_has_answer.items() if has_answer]
    no_answer_qids = [qas_id for qas_id, has_answer in qas_id_to_has_answer.items() if not has_answer]

    if no_answer_probs is None:
        no_answer_probs = dict.fromkeys(preds, 0.0)

    exact, f1 = get_raw_scores(examples, preds)

    exact_threshold = apply_no_ans_threshold(
        exact, no_answer_probs, qas_id_to_has_answer, no_answer_probability_threshold
    )
    f1_threshold = apply_no_ans_threshold(f1, no_answer_probs, qas_id_to_has_answer, no_answer_probability_threshold)

    evaluation = make_eval_dict(exact_threshold, f1_threshold)

    if has_answer_qids:
        has_ans_eval = make_eval_dict(exact_threshold, f1_threshold, qid_list=has_answer_qids)
        merge_eval(evaluation, has_ans_eval, ""HasAns"")

    if no_answer_qids:
        no_ans_eval = make_eval_dict(exact_threshold, f1_threshold, qid_list=no_answer_qids)
        merge_eval(evaluation, no_ans_eval, ""NoAns"")

    if no_answer_probs:
        find_all_best_thresh(evaluation, preds, exact, f1, no_answer_probs, qas_id_to_has_answer)

    return evaluation"
huggingface__transformers__src__transformers__data__metrics__squad_metrics.py__get_final_text,get_final_text,huggingface/transformers:src/transformers/data/metrics/squad_metrics.py,92,True,False,False,2,False,False,False,False,False,False,False,False,True,19,3,16,other,"def get_final_text(pred_text, orig_text, do_lower_case, verbose_logging=False):
    """"""Project the tokenized prediction back to the original text.""""""

    # When we created the data, we kept track of the alignment between original
    # (whitespace tokenized) tokens and our WordPiece tokenized tokens. So
    # now `orig_text` contains the span of our original text corresponding to the
    # span that we predicted.
    #
    # However, `orig_text` may contain extra characters that we don't want in
    # our prediction.
    #
    # For example, let's say:
    #   pred_text = steve smith
    #   orig_text = Steve Smith's
    #
    # We don't want to return `orig_text` because it contains the extra ""'s"".
    #
    # We don't want to return `pred_text` because it's already been normalized
    # (the SQuAD eval script also does punctuation stripping/lower casing but
    # our tokenizer does additional normalization like stripping accent
    # characters).
    #
    # What we really want to return is ""Steve Smith"".
    #
    # Therefore, we have to apply a semi-complicated alignment heuristic between
    # `pred_text` and `orig_text` to get a character-to-character alignment. This
    # can fail in certain cases in which case we just return `orig_text`.

    def _strip_spaces(text):
        ns_chars = []
        ns_to_s_map = collections.OrderedDict()
        for i, c in enumerate(text):
            if c == "" "":
                continue
            ns_to_s_map[len(ns_chars)] = i
            ns_chars.append(c)
        ns_text = """".join(ns_chars)
        return (ns_text, ns_to_s_map)

    # We first tokenize `orig_text`, strip whitespace from the result
    # and `pred_text`, and check if they are the same length. If they are
    # NOT the same length, the heuristic has failed. If they are the same
    # length, we assume the characters are one-to-one aligned.
    tokenizer = BasicTokenizer(do_lower_case=do_lower_case)

    tok_text = "" "".join(tokenizer.tokenize(orig_text))

    start_position = tok_text.find(pred_text)
    if start_position == -1:
        if verbose_logging:
            logger.info(f""Unable to find text: '{pred_text}' in '{orig_text}'"")
        return orig_text
    end_position = start_position + len(pred_text) - 1

    (orig_ns_text, orig_ns_to_s_map) = _strip_spaces(orig_text)
    (tok_ns_text, tok_ns_to_s_map) = _strip_spaces(tok_text)

    if len(orig_ns_text) != len(tok_ns_text):
        if verbose_logging:
            logger.info(f""Length not equal after stripping spaces: '{orig_ns_text}' vs '{tok_ns_text}'"")
        return orig_text

    # We then project the characters in `pred_text` back to `orig_text` using
    # the character-to-character alignment.
    tok_s_to_ns_map = {}
    for i, tok_index in tok_ns_to_s_map.items():
        tok_s_to_ns_map[tok_index] = i

    orig_start_position = None
    if start_position in tok_s_to_ns_map:
        ns_start_position = tok_s_to_ns_map[start_position]
        if ns_start_position in orig_ns_to_s_map:
            orig_start_position = orig_ns_to_s_map[ns_start_position]

    if orig_start_position is None:
        if verbose_logging:
            logger.info(""Couldn't map start position"")
        return orig_text

    orig_end_position = None
    if end_position in tok_s_to_ns_map:
        ns_end_position = tok_s_to_ns_map[end_position]
        if ns_end_position in orig_ns_to_s_map:
            orig_end_position = orig_ns_to_s_map[ns_end_position]

    if orig_end_position is None:
        if verbose_logging:
            logger.info(""Couldn't map end position"")
        return orig_text

    output_text = orig_text[orig_start_position : (orig_end_position + 1)]
    return output_text"
huggingface__transformers__src__transformers__data__processors__squad.py__squad_convert_examples_to_features,squad_convert_examples_to_features,huggingface/transformers:src/transformers/data/processors/squad.py,118,True,False,True,2,True,False,False,True,False,True,False,False,False,26,0,17,ml,"def squad_convert_examples_to_features(
    examples,
    tokenizer,
    max_seq_length,
    doc_stride,
    max_query_length,
    is_training,
    padding_strategy=""max_length"",
    return_dataset=False,
    threads=1,
    tqdm_enabled=True,
):
    """"""
    Converts a list of examples into a list of features that can be directly given as input to a model. It is
    model-dependant and takes advantage of many of the tokenizer's features to create the model's inputs.

    Args:
        examples: list of [`~data.processors.squad.SquadExample`]
        tokenizer: an instance of a child of [`PreTrainedTokenizer`]
        max_seq_length: The maximum sequence length of the inputs.
        doc_stride: The stride used when the context is too large and is split across several features.
        max_query_length: The maximum length of the query.
        is_training: whether to create features for model evaluation or model training.
        padding_strategy: Default to ""max_length"". Which padding strategy to use
        return_dataset: Default False. Can also be 'pt'.
            if 'pt': returns a torch.data.TensorDataset.
        threads: multiple processing threads.


    Returns:
        list of [`~data.processors.squad.SquadFeatures`]

    Example:

    ```python
    processor = SquadV2Processor()
    examples = processor.get_dev_examples(data_dir)

    features = squad_convert_examples_to_features(
        examples=examples,
        tokenizer=tokenizer,
        max_seq_length=args.max_seq_length,
        doc_stride=args.doc_stride,
        max_query_length=args.max_query_length,
        is_training=not evaluate,
    )
    ```""""""

    threads = min(threads, cpu_count())
    pool_cls = ThreadPool if is_torch_hpu_available() else Pool
    with pool_cls(threads, initializer=squad_convert_example_to_features_init, initargs=(tokenizer,)) as p:
        annotate_ = partial(
            squad_convert_example_to_features,
            max_seq_length=max_seq_length,
            doc_stride=doc_stride,
            max_query_length=max_query_length,
            padding_strategy=padding_strategy,
            is_training=is_training,
        )
        features = list(
            tqdm(
                p.imap(annotate_, examples, chunksize=32),
                total=len(examples),
                desc=""convert squad examples to features"",
                disable=not tqdm_enabled,
            )
        )

    new_features = []
    unique_id = 1000000000
    example_index = 0
    for example_features in tqdm(
        features, total=len(features), desc=""add example index and unique id"", disable=not tqdm_enabled
    ):
        if not example_features:
            continue
        for example_feature in example_features:
            example_feature.example_index = example_index
            example_feature.unique_id = unique_id
            new_features.append(example_feature)
            unique_id += 1
        example_index += 1
    features = new_features
    del new_features
    if return_dataset == ""pt"":
        if not is_torch_available():
            raise RuntimeError(""PyTorch must be installed to return a PyTorch dataset."")

        # Convert to Tensors and build dataset
        all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)
        all_attention_masks = torch.tensor([f.attention_mask for f in features], dtype=torch.long)
        all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)
        all_cls_index = torch.tensor([f.cls_index for f in features], dtype=torch.long)
        all_p_mask = torch.tensor([f.p_mask for f in features], dtype=torch.float)
        all_is_impossible = torch.tensor([f.is_impossible for f in features], dtype=torch.float)

        if not is_training:
            all_feature_index = torch.arange(all_input_ids.size(0), dtype=torch.long)
            dataset = TensorDataset(
                all_input_ids, all_attention_masks, all_token_type_ids, all_feature_index, all_cls_index, all_p_mask
            )
        else:
            all_start_positions = torch.tensor([f.start_position for f in features], dtype=torch.long)
            all_end_positions = torch.tensor([f.end_position for f in features], dtype=torch.long)
            dataset = TensorDataset(
                all_input_ids,
                all_attention_masks,
                all_token_type_ids,
                all_start_positions,
                all_end_positions,
                all_cls_index,
                all_p_mask,
                all_is_impossible,
            )

        return features, dataset
    else:
        return features"
huggingface__transformers__src__transformers__debug_utils.py__detect_overflow,detect_overflow,huggingface/transformers:src/transformers/debug_utils.py,44,False,False,False,0,False,False,False,False,False,True,False,False,True,31,0,9,ml,"def detect_overflow(var, ctx):
    """"""
    Report whether the tensor contains any `nan` or `inf` entries.

    This is useful for detecting overflows/underflows and best to call right after the function that did some math that
    modified the tensor in question.

    This function contains a few other helper features that you can enable and tweak directly if you want to track
    various other things.

    Args:
        var: the tensor variable to check
        ctx: the message to print as a context

    Return:
        `True` if `inf` or `nan` was detected, `False` otherwise
    """"""
    detected = False
    if torch.isnan(var).any().item():
        detected = True
        print(f""{ctx} has nans"")
    if torch.isinf(var).any().item():
        detected = True
        print(f""{ctx} has infs"")

    # if needed to monitor large elements can enable the following
    if 0:  # and detected:
        n100 = var[torch.ge(var.abs(), 100)]
        if n100.numel() > 0:
            print(f""{ctx}:  n100={n100.numel()}"")
        n1000 = var[torch.ge(var.abs(), 1000)]
        if n1000.numel() > 0:
            print(f""{ctx}: n1000={n1000.numel()}"")
        n10000 = var[torch.ge(var.abs(), 10000)]
        if n10000.numel() > 0:
            print(f""{ctx}: n10000={n10000.numel()}"")

    if 0:
        print(f""min={var.min():9.2e} max={var.max():9.2e}"")

    if 0:
        print(f""min={var.min():9.2e} max={var.max():9.2e} var={var.var():9.2e} mean={var.mean():9.2e} ({ctx})"")

    return detected"
huggingface__transformers__src__transformers__dynamic_module_utils.py__init_hf_modules,init_hf_modules,huggingface/transformers:src/transformers/dynamic_module_utils.py,14,False,False,False,0,False,False,False,False,False,False,False,False,False,6,1,3,ml,"def init_hf_modules():
    """"""
    Creates the cache directory for modules with an init, and adds it to the Python path.
    """"""
    # This function has already been executed if HF_MODULES_CACHE already is in the Python path.
    if HF_MODULES_CACHE in sys.path:
        return

    sys.path.append(HF_MODULES_CACHE)
    os.makedirs(HF_MODULES_CACHE, exist_ok=True)
    init_path = Path(HF_MODULES_CACHE) / ""__init__.py""
    if not init_path.exists():
        init_path.touch()
        importlib.invalidate_caches()"
huggingface__transformers__src__transformers__dynamic_module_utils.py__create_dynamic_module,create_dynamic_module,huggingface/transformers:src/transformers/dynamic_module_utils.py,20,False,False,False,0,False,False,False,False,False,False,False,False,False,9,2,3,ml,"def create_dynamic_module(name: str | os.PathLike) -> None:
    """"""
    Creates a dynamic module in the cache directory for modules.

    Args:
        name (`str` or `os.PathLike`):
            The name of the dynamic module to create.
    """"""
    init_hf_modules()
    dynamic_module_path = (Path(HF_MODULES_CACHE) / name).resolve()
    # If the parent module does not exist yet, recursively create it.
    if not dynamic_module_path.parent.exists():
        create_dynamic_module(dynamic_module_path.parent)
    os.makedirs(dynamic_module_path, exist_ok=True)
    init_path = dynamic_module_path / ""__init__.py""
    if not init_path.exists():
        init_path.touch()
        # It is extremely important to invalidate the cache when we change stuff in those modules, or users end up
        # with errors about module that do not exist. Same for all other `invalidate_caches` in this file.
        importlib.invalidate_caches()"
huggingface__transformers__src__transformers__dynamic_module_utils.py__get_relative_imports,get_relative_imports,huggingface/transformers:src/transformers/dynamic_module_utils.py,19,False,False,False,0,False,False,False,False,False,False,False,False,False,6,0,2,ml,"def get_relative_imports(module_file: str | os.PathLike) -> list[str]:
    """"""
    Get the list of modules that are relatively imported in a module file.

    Args:
        module_file (`str` or `os.PathLike`): The module file to inspect.

    Returns:
        `list[str]`: The list of relative imports in the module.
    """"""
    with open(module_file, encoding=""utf-8"") as f:
        content = f.read()

    # Imports of the form `import .xxx`
    relative_imports = re.findall(r""^\s*import\s+\.(\S+)\s*$"", content, flags=re.MULTILINE)
    # Imports of the form `from .xxx import yyy`
    relative_imports += re.findall(r""^\s*from\s+\.(\S+)\s+import"", content, flags=re.MULTILINE)
    # Unique-ify
    return list(set(relative_imports))"
huggingface__transformers__src__transformers__dynamic_module_utils.py__get_relative_import_files,get_relative_import_files,huggingface/transformers:src/transformers/dynamic_module_utils.py,30,True,True,True,2,True,False,False,False,False,False,False,False,True,6,1,5,ml,"def get_relative_import_files(module_file: str | os.PathLike) -> list[str]:
    """"""
    Get the list of all files that are needed for a given module. Note that this function recurses through the relative
    imports (if a imports b and b imports c, it will return module files for b and c).

    Args:
        module_file (`str` or `os.PathLike`): The module file to inspect.

    Returns:
        `list[str]`: The list of all relative imports a given module needs (recursively), which will give us the list
        of module files a given module needs.
    """"""
    no_change = False
    files_to_check = [module_file]
    all_relative_imports = []

    # Let's recurse through all relative imports
    while not no_change:
        new_imports = []
        for f in files_to_check:
            new_imports.extend(get_relative_imports(f))

        module_path = Path(module_file).parent
        new_import_files = [f""{str(module_path / m)}.py"" for m in new_imports]
        files_to_check = [f for f in new_import_files if f not in all_relative_imports]

        no_change = len(files_to_check) == 0
        all_relative_imports.extend(files_to_check)

    return all_relative_imports"
huggingface__transformers__src__transformers__dynamic_module_utils.py__get_imports,get_imports,huggingface/transformers:src/transformers/dynamic_module_utils.py,53,True,False,False,3,False,False,False,False,False,False,False,False,True,22,0,14,ml,"def get_imports(filename: str | os.PathLike) -> list[str]:
    """"""
    Extracts all the libraries (not relative imports this time) that are imported in a file.

    Args:
        filename (`str` or `os.PathLike`): The module file to inspect.

    Returns:
        `list[str]`: The list of all packages required to use the input module.
    """"""
    with open(filename, encoding=""utf-8"") as f:
        content = f.read()
    imported_modules = set()

    import transformers.utils

    def recursive_look_for_imports(node):
        if isinstance(node, ast.Try):
            return  # Don't recurse into Try blocks and ignore imports in them
        elif isinstance(node, ast.If):
            test = node.test
            for condition_node in ast.walk(test):
                if isinstance(condition_node, ast.Call):
                    check_function = getattr(condition_node.func, ""id"", """")
                    if (
                        check_function.endswith(""available"")
                        and check_function.startswith(""is_flash_attn"")
                        or hasattr(transformers.utils.import_utils, check_function)
                    ):
                        # Don't recurse into ""if flash_attn_available()"" or any ""if library_available"" blocks
                        # that appears in `transformers.utils.import_utils` and ignore imports in them
                        return
        elif isinstance(node, ast.Import):
            # Handle 'import x' statements
            for alias in node.names:
                top_module = alias.name.split(""."")[0]
                if top_module:
                    imported_modules.add(top_module)
        elif isinstance(node, ast.ImportFrom):
            # Handle 'from x import y' statements, ignoring relative imports
            if node.level == 0 and node.module:
                top_module = node.module.split(""."")[0]
                if top_module:
                    imported_modules.add(top_module)

        # Recursively visit all children
        for child in ast.iter_child_nodes(node):
            recursive_look_for_imports(child)

    tree = ast.parse(content)
    recursive_look_for_imports(tree)

    return sorted(imported_modules)"
huggingface__transformers__src__transformers__dynamic_module_utils.py__check_imports,check_imports,huggingface/transformers:src/transformers/dynamic_module_utils.py,33,True,False,False,1,False,False,False,False,False,False,False,False,True,10,0,5,ml,"def check_imports(filename: str | os.PathLike) -> list[str]:
    """"""
    Check if the current Python environment contains all the libraries that are imported in a file. Will raise if a
    library is missing.

    Args:
        filename (`str` or `os.PathLike`): The module file to check.

    Returns:
        `list[str]`: The list of relative imports in the file.
    """"""
    imports = get_imports(filename)
    missing_packages = []
    for imp in imports:
        try:
            importlib.import_module(imp)
        except ImportError as exception:
            logger.warning(f""Encountered exception while importing {imp}: {exception}"")
            # Some packages can fail with an ImportError because of a dependency issue.
            # This check avoids hiding such errors.
            # See https://github.com/huggingface/transformers/issues/33604
            if ""No module named"" in str(exception):
                missing_packages.append(imp)
            else:
                raise

    if len(missing_packages) > 0:
        raise ImportError(
            ""This modeling file requires the following packages that were not found in your environment: ""
            f""{', '.join(missing_packages)}. Run `pip install {' '.join(missing_packages)}`""
        )

    return get_relative_imports(filename)"
huggingface__transformers__src__transformers__dynamic_module_utils.py__get_class_in_module,get_class_in_module,huggingface/transformers:src/transformers/dynamic_module_utils.py,46,False,False,False,0,False,False,True,False,False,False,False,False,True,20,3,6,ml,"def get_class_in_module(
    class_name: str,
    module_path: str | os.PathLike,
    *,
    force_reload: bool = False,
) -> type:
    """"""
    Import a module on the cache directory for modules and extract a class from it.

    Args:
        class_name (`str`): The name of the class to import.
        module_path (`str` or `os.PathLike`): The path to the module to import.
        force_reload (`bool`, *optional*, defaults to `False`):
            Whether to reload the dynamic module from file if it already exists in `sys.modules`.
            Otherwise, the module is only reloaded if the file has changed.

    Returns:
        `typing.Type`: The class looked for.
    """"""
    name = os.path.normpath(module_path)
    name = name.removesuffix("".py"")
    name = name.replace(os.path.sep, ""."")
    module_file: Path = Path(HF_MODULES_CACHE) / module_path
    with _HF_REMOTE_CODE_LOCK:
        if force_reload:
            sys.modules.pop(name, None)
            importlib.invalidate_caches()
        cached_module: ModuleType | None = sys.modules.get(name)
        module_spec = importlib.util.spec_from_file_location(name, location=module_file)

        # Hash the module file and all its relative imports to check if we need to reload it
        module_files: list[Path] = [module_file] + sorted(map(Path, get_relative_import_files(module_file)))
        module_hash: str = hashlib.sha256(b"""".join(bytes(f) + f.read_bytes() for f in module_files)).hexdigest()

        module: ModuleType
        if cached_module is None:
            module = importlib.util.module_from_spec(module_spec)
            # insert it into sys.modules before any loading begins
            sys.modules[name] = module
        else:
            module = cached_module
        # reload in both cases, unless the module is already imported and the hash hits
        if getattr(module, ""__transformers_module_hash__"", """") != module_hash:
            module_spec.loader.exec_module(module)
            module.__transformers_module_hash__ = module_hash
        return getattr(module, class_name)"
huggingface__transformers__src__transformers__dynamic_module_utils.py__get_class_from_dynamic_module,get_class_from_dynamic_module,huggingface/transformers:src/transformers/dynamic_module_utils.py,108,False,False,False,0,False,False,False,False,False,False,False,False,True,4,1,3,ml,"def get_class_from_dynamic_module(
    class_reference: str,
    pretrained_model_name_or_path: str | os.PathLike,
    cache_dir: str | os.PathLike | None = None,
    force_download: bool = False,
    proxies: dict[str, str] | None = None,
    token: bool | str | None = None,
    revision: str | None = None,
    local_files_only: bool = False,
    repo_type: str | None = None,
    code_revision: str | None = None,
    **kwargs,
) -> type:
    """"""
    Extracts a class from a module file, present in the local folder or repository of a model.

    <Tip warning={true}>

    Calling this function will execute the code in the module file found locally or downloaded from the Hub. It should
    therefore only be called on trusted repos.

    </Tip>



    Args:
        class_reference (`str`):
            The full name of the class to load, including its module and optionally its repo.
        pretrained_model_name_or_path (`str` or `os.PathLike`):
            This can be either:

            - a string, the *model id* of a pretrained model configuration hosted inside a model repo on
              huggingface.co.
            - a path to a *directory* containing a configuration file saved using the
              [`~PreTrainedTokenizer.save_pretrained`] method, e.g., `./my_model_directory/`.

            This is used when `class_reference` does not specify another repo.
        module_file (`str`):
            The name of the module file containing the class to look for.
        class_name (`str`):
            The name of the class to import in the module.
        cache_dir (`str` or `os.PathLike`, *optional*):
            Path to a directory in which a downloaded pretrained model configuration should be cached if the standard
            cache should not be used.
        force_download (`bool`, *optional*, defaults to `False`):
            Whether or not to force to (re-)download the configuration files and override the cached versions if they
            exist.
        proxies (`dict[str, str]`, *optional*):
            A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',
            'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.
        token (`str` or `bool`, *optional*):
            The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated
            when running `hf auth login` (stored in `~/.huggingface`).
        revision (`str`, *optional*, defaults to `""main""`):
            The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
            git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any
            identifier allowed by git.
        local_files_only (`bool`, *optional*, defaults to `False`):
            If `True`, will only try to load the tokenizer configuration from local files.
        repo_type (`str`, *optional*):
            Specify the repo type (useful when downloading from a space for instance).
        code_revision (`str`, *optional*, defaults to `""main""`):
            The specific revision to use for the code on the Hub, if the code leaves in a different repository than the
            rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based system for
            storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git.

    <Tip>

    Passing `token=True` is required when you want to use a private model.

    </Tip>

    Returns:
        `typing.Type`: The class, dynamically imported from the module.

    Examples:

    ```python
    # Download module `modeling.py` from huggingface.co and cache then extract the class `MyBertModel` from this
    # module.
    cls = get_class_from_dynamic_module(""modeling.MyBertModel"", ""sgugger/my-bert-model"")

    # Download module `modeling.py` from a given repo and cache then extract the class `MyBertModel` from this
    # module.
    cls = get_class_from_dynamic_module(""sgugger/my-bert-model--modeling.MyBertModel"", ""sgugger/another-bert-model"")
    ```""""""
    # Catch the name of the repo if it's specified in `class_reference`
    if ""--"" in class_reference:
        repo_id, class_reference = class_reference.split(""--"")
    else:
        repo_id = pretrained_model_name_or_path
    module_file, class_name = class_reference.split(""."")

    if code_revision is None and pretrained_model_name_or_path == repo_id:
        code_revision = revision
    # And lastly we get the class inside our newly created module
    final_module = get_cached_module_file(
        repo_id,
        module_file + "".py"",
        cache_dir=cache_dir,
        force_download=force_download,
        proxies=proxies,
        token=token,
        revision=code_revision,
        local_files_only=local_files_only,
        repo_type=repo_type,
    )
    return get_class_in_module(class_name, final_module, force_reload=force_download)"
huggingface__transformers__src__transformers__dynamic_module_utils.py__custom_object_save,custom_object_save,huggingface/transformers:src/transformers/dynamic_module_utils.py,74,True,False,False,2,False,False,False,False,False,False,False,False,True,21,2,11,ml,"def custom_object_save(obj: Any, folder: str | os.PathLike, config: dict | None = None) -> list[str]:
    """"""
    Save the modeling files corresponding to a custom model/configuration/tokenizer etc. in a given folder. Optionally
    adds the proper fields in a config.

    Args:
        obj (`Any`): The object for which to save the module files.
        folder (`str` or `os.PathLike`): The folder where to save.
        config (`PreTrainedConfig` or dictionary, `optional`):
            A config in which to register the auto_map corresponding to this custom object.

    Returns:
        `list[str]`: The list of files saved.
    """"""
    if obj.__module__ == ""__main__"":
        logger.warning(
            f""We can't save the code defining {obj} in {folder} as it's been defined in __main__. You should put ""
            ""this code in a separate module so we can include it in the saved folder and make it easier to share via ""
            ""the Hub.""
        )
        return

    def _set_auto_map_in_config(_config):
        module_name = obj.__class__.__module__
        last_module = module_name.split(""."")[-1]
        full_name = f""{last_module}.{obj.__class__.__name__}""
        # Special handling for tokenizers
        if ""Tokenizer"" in full_name:
            slow_tokenizer_class = None
            fast_tokenizer_class = None
            if obj.__class__.__name__.endswith(""Fast""):
                # Fast tokenizer: we have the fast tokenizer class and we may have the slow one has an attribute.
                fast_tokenizer_class = f""{last_module}.{obj.__class__.__name__}""
                if getattr(obj, ""slow_tokenizer_class"", None) is not None:
                    slow_tokenizer = getattr(obj, ""slow_tokenizer_class"")
                    slow_tok_module_name = slow_tokenizer.__module__
                    last_slow_tok_module = slow_tok_module_name.split(""."")[-1]
                    slow_tokenizer_class = f""{last_slow_tok_module}.{slow_tokenizer.__name__}""
            else:
                # Slow tokenizer: no way to have the fast class
                slow_tokenizer_class = f""{last_module}.{obj.__class__.__name__}""

            full_name = (slow_tokenizer_class, fast_tokenizer_class)

        if isinstance(_config, dict):
            auto_map = _config.get(""auto_map"", {})
            auto_map[obj._auto_class] = full_name
            _config[""auto_map""] = auto_map
        elif getattr(_config, ""auto_map"", None) is not None:
            _config.auto_map[obj._auto_class] = full_name
        else:
            _config.auto_map = {obj._auto_class: full_name}

    # Add object class to the config auto_map
    if isinstance(config, (list, tuple)):
        for cfg in config:
            _set_auto_map_in_config(cfg)
    elif config is not None:
        _set_auto_map_in_config(config)

    result = []
    # Copy module file to the output folder.
    object_file = sys.modules[obj.__module__].__file__
    dest_file = Path(folder) / (Path(object_file).name)
    shutil.copy(object_file, dest_file)
    result.append(dest_file)

    # Gather all relative imports recursively and make sure they are copied as well.
    for needed_file in get_relative_import_files(object_file):
        dest_file = Path(folder) / (Path(needed_file).name)
        shutil.copy(needed_file, dest_file)
        result.append(dest_file)

    return result"
huggingface__transformers__src__transformers__dynamic_module_utils.py__resolve_trust_remote_code,resolve_trust_remote_code,huggingface/transformers:src/transformers/dynamic_module_utils.py,83,False,True,False,1,False,False,False,False,False,False,False,False,True,13,0,14,ml,"def resolve_trust_remote_code(
    trust_remote_code, model_name, has_local_code, has_remote_code, error_message=None, upstream_repo=None
):
    """"""
    Resolves the `trust_remote_code` argument. If there is remote code to be loaded, the user must opt-in to loading
    it.

    Args:
        trust_remote_code (`bool` or `None`):
            User-defined `trust_remote_code` value.
        model_name (`str`):
            The name of the model repository in huggingface.co.
        has_local_code (`bool`):
            Whether the model has local code.
        has_remote_code (`bool`):
            Whether the model has remote code.
        error_message (`str`, *optional*):
            Custom error message to display if there is remote code to load and the user didn't opt-in. If unset, the error
            message will be regarding loading a model with custom code.

    Returns:
        The resolved `trust_remote_code` value.
    """"""
    if error_message is None:
        if upstream_repo is not None:
            error_message = (
                f""The repository {model_name} references custom code contained in {upstream_repo} which ""
                f""must be executed to correctly load the model. You can inspect the repository ""
                f""content at https://hf.co/{upstream_repo} .\n""
            )
        elif os.path.isdir(model_name):
            error_message = (
                f""The repository {model_name} contains custom code which must be executed ""
                f""to correctly load the model. You can inspect the repository ""
                f""content at {os.path.abspath(model_name)} .\n""
            )
        else:
            error_message = (
                f""The repository {model_name} contains custom code which must be executed ""
                f""to correctly load the model. You can inspect the repository ""
                f""content at https://hf.co/{model_name} .\n""
            )

    if trust_remote_code is None:
        if has_local_code:
            trust_remote_code = False
        elif has_remote_code and TIME_OUT_REMOTE_CODE > 0:
            prev_sig_handler = None
            try:
                prev_sig_handler = signal.signal(signal.SIGALRM, _raise_timeout_error)
                signal.alarm(TIME_OUT_REMOTE_CODE)
                while trust_remote_code is None:
                    answer = input(
                        f""{error_message} You can inspect the repository content at https://hf.co/{model_name}.\n""
                        f""You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\n""
                        f""Do you wish to run the custom code? [y/N] ""
                    )
                    if answer.lower() in [""yes"", ""y"", ""1""]:
                        trust_remote_code = True
                    elif answer.lower() in [""no"", ""n"", ""0"", """"]:
                        trust_remote_code = False
                signal.alarm(0)
            except Exception:
                # OS which does not support signal.SIGALRM
                raise ValueError(
                    f""{error_message} You can inspect the repository content at https://hf.co/{model_name}.\n""
                    f""Please pass the argument `trust_remote_code=True` to allow custom code to be run.""
                )
            finally:
                if prev_sig_handler is not None:
                    signal.signal(signal.SIGALRM, prev_sig_handler)
                    signal.alarm(0)
        elif has_remote_code:
            # For the CI which puts the timeout at 0
            _raise_timeout_error(None, None)

    if has_remote_code and not has_local_code and not trust_remote_code:
        raise ValueError(
            f""{error_message} You can inspect the repository content at https://hf.co/{model_name}.\n""
            f""Please pass the argument `trust_remote_code=True` to allow custom code to be run.""
        )

    return trust_remote_code"
huggingface__transformers__src__transformers__dynamic_module_utils.py__check_python_requirements,check_python_requirements,huggingface/transformers:src/transformers/dynamic_module_utils.py,54,True,False,False,1,False,False,False,False,False,False,False,False,True,15,1,10,ml,"def check_python_requirements(path_or_repo_id, requirements_file=""requirements.txt"", **kwargs):
    """"""
    Tries to locate `requirements_file` in a local folder or repo, and confirms that the environment has all the
    python dependencies installed.

    Args:
        path_or_repo_id (`str` or `os.PathLike`):
            This can be either:
            - a string, the *model id* of a model repo on huggingface.co.
            - a path to a *directory* potentially containing the file.
        kwargs (`dict[str, Any]`, *optional*):
            Additional arguments to pass to `cached_file`.
    """"""
    failed = []  # error messages regarding requirements
    try:
        requirements = cached_file(path_or_repo_id=path_or_repo_id, filename=requirements_file, **kwargs)
        with open(requirements, ""r"") as f:
            requirements = f.readlines()

        for requirement in requirements:
            requirement = requirement.strip()
            if not requirement or requirement.startswith(""#""):  # skip empty lines and comments
                continue

            try:
                # e.g. ""torch>2.6.0"" -> ""torch"", "">"", ""2.6.0""
                package_name, delimiter, version_number = split_package_version(requirement)
            except ValueError:  # e.g. ""torch"", as opposed to ""torch>2.6.0""
                package_name = requirement
                delimiter, version_number = None, None

            try:
                local_package_version = importlib.metadata.version(package_name)
            except importlib.metadata.PackageNotFoundError:
                failed.append(f""{requirement} (installed: None)"")
                continue

            if delimiter is not None and version_number is not None:
                is_satisfied = VersionComparison.from_string(delimiter).value(
                    version.parse(local_package_version), version.parse(version_number)
                )
            else:
                is_satisfied = True

            if not is_satisfied:
                failed.append(f""{requirement} (installed: {local_package_version})"")

    except OSError:  # no requirements.txt
        pass

    if failed:
        raise ImportError(
            f""Missing requirements in your local environment for `{path_or_repo_id}`:\n"" + ""\n"".join(failed)
        )"
huggingface__transformers__src__transformers__generation__continuous_batching__cache.py__group_layers_by_attn_type,group_layers_by_attn_type,huggingface/transformers:src/transformers/generation/continuous_batching/cache.py,31,True,False,True,3,True,False,False,False,False,True,False,False,False,12,2,9,ml,"def group_layers_by_attn_type(config: PreTrainedConfig) -> tuple[list[list[int]], list[str]]:
    """"""
    Group layers depending on the attention mix, according to VLLM's hybrid allocator rules:
        - Layers in each group need to have the same type of attention
        - All groups have the same number of layers

    For a model with the following layer types: [""sliding"", ""full"", ""full"", ""sliding"", ""full"", ""full"", ""full"", ""full""]
    We would get four groups: [0, 3], [1, 2], [4,5] and [6,7].
    """"""
    # If the config has no layer_type attribute, it means all layers are the same attention type
    layer_types = getattr(config, ""layer_types"", None)
    if layer_types is None:
        attn_type = ""sliding_attention"" if getattr(config, ""sliding_window"", None) is not None else ""full_attention""
        layer_types = [attn_type for _ in range(config.num_hidden_layers)]

    # We then count the number of layers of each type
    layer_counts = {}
    for i, layer_type in enumerate(layer_types):
        layer_counts[layer_type] = layer_counts.get(layer_type, []) + [i]

    # The size of all groups is the greatest common divisor of the number of layers of each type
    group_size = gcd(*[len(indices) for indices in layer_counts.values()])

    # We then group the layers by type
    layer_groups = []
    for layer_type, indices in layer_counts.items():
        for i in range(0, len(indices), group_size):
            layer_groups.append(indices[i : i + group_size])
    # And note the layer types
    group_types = [layer_types[lg[0]] for lg in layer_groups]
    return layer_groups, group_types"
huggingface__transformers__src__transformers__generation__continuous_batching__input_ouputs.py__build_attention_mask,build_attention_mask,huggingface/transformers:src/transformers/generation/continuous_batching/input_ouputs.py,82,True,False,False,1,False,False,False,False,False,True,False,False,False,8,11,4,ml,"def build_attention_mask(
    attention_mask: torch.Tensor,
    cumulative_seqlens_q: list[int],
    cumulative_seqlens_k: list[int],
    sliding_window: int = 1,
) -> None:
    """"""Builds an attention mask inplace using the cumulative seqlens of the query and key. If given a sliding window, it
    will also apply a sliding window mask on top. The attention mask is not boolean, it uses zeroes and -inf (or its
    equivalent) so it's more of an attention score bias tensor.
    The attention mask is a block-diagonal matrix, with each block an attention mask for a single query-key pair.
    Each of those block is built from a causal mask and, if there is a sliding window, a sliding window mask.

    An example is represented below, with seqlen_k = 8, seqlen_q = 4 and sliding_window = 6:

    CAUSAL MASK:

                  
                  
                  
                  

    SLIDING WINDOW MASK:
          seqlen_k - seqlen_q - sliding_window = 8 - 4 - 6 = -2 offset to the left
       <>
       |        
       |        
       |        
       |        

    ATTENTION MASK (sum of causal and sliding window masks):

                  
                  
                  
                  

    Another example with seqlen_k = 5, seqlen_q = 3 and sliding_window = 2:

    CAUSAL MASK:

               
               
               

    SLIDING WINDOW MASK:
          seqlen_k - seqlen_q - sliding_window = 5 - 3 - 2 = 0 offset to the left
        <>
         |     
         |     
         |     

    ATTENTION MASK (sum of causal and sliding window masks):

               
               
               

    """"""
    min_value = torch.finfo(attention_mask.dtype).min
    for i in range(len(cumulative_seqlens_q) - 1):
        seqlen_q = cumulative_seqlens_q[i + 1] - cumulative_seqlens_q[i]
        seqlen_k = cumulative_seqlens_k[i + 1] - cumulative_seqlens_k[i]
        if seqlen_q < seqlen_k and seqlen_q >= 1:
            causal_diagonal = seqlen_k - seqlen_q + 1
        else:
            causal_diagonal = 1
        query_range = slice(cumulative_seqlens_q[i], cumulative_seqlens_q[i + 1])
        key_range = slice(cumulative_seqlens_k[i], cumulative_seqlens_k[i + 1])
        # Apply causal mask
        minus_inf = torch.full(
            attention_mask[..., query_range, key_range].shape,
            min_value,
            dtype=attention_mask.dtype,
            device=attention_mask.device,
        )
        masked = torch.triu(minus_inf, diagonal=causal_diagonal)
        # Apply sliding window mask if needed
        if sliding_window > 1:
            sliding_diagonal = seqlen_k - seqlen_q - sliding_window
            masked += torch.tril(minus_inf, diagonal=sliding_diagonal)
        # Replace in attention mask
        attention_mask[..., query_range, key_range] = masked"
huggingface__transformers__src__transformers__generation__continuous_batching__requests.py__get_device_and_memory_breakdown,get_device_and_memory_breakdown,huggingface/transformers:src/transformers/generation/continuous_batching/requests.py,37,False,False,False,0,False,False,False,False,False,True,False,False,False,25,1,5,ml,"def get_device_and_memory_breakdown() -> tuple[torch.device, int, int, int]:
    if torch.cuda.is_available():
        device = torch.device(""cuda"")
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        total_memory = torch.cuda.get_device_properties(device).total_memory
        reserved_memory = torch.cuda.memory_reserved(device)
        allocated_memory = torch.cuda.memory_allocated(device)
    elif is_torch_xpu_available():
        device = torch.device(""xpu"")
        torch.xpu.empty_cache()
        torch.xpu.synchronize()
        total_memory = torch.xpu.get_device_properties(device).total_memory
        reserved_memory = torch.xpu.memory_reserved(device)
        allocated_memory = torch.xpu.memory_allocated(device)
    elif torch.backends.mps.is_available() and torch.backends.mps.is_built():
        device = torch.device(""mps"")
        # MPS memory reporting (PyTorch 2.0+)
        total_memory = torch.mps.driver_allocated_memory()
        allocated_memory = total_memory - torch.mps.recommended_max_memory()
        reserved_memory = 0  # MPS does not track reserved separately
    else:
        device = torch.device(""cpu"")
        if is_psutil_available():
            total_memory = psutil.virtual_memory().total
            allocated_memory = psutil.Process().memory_info().rss
            reserved_memory = allocated_memory
        else:
            logger.error(
                ""Cannot get memory breakdown on CPU without psutil: returning 0 for all memory values. Please install ""
                ""psutil to get an actual memory breakdown.""
            )
            total_memory = 0
            reserved_memory = 0
            allocated_memory = 0

    return device, total_memory, reserved_memory, allocated_memory"
huggingface__transformers__src__transformers__hf_argparser.py__string_to_bool,string_to_bool,huggingface/transformers:src/transformers/hf_argparser.py,11,False,False,False,0,False,False,False,False,False,False,False,False,True,4,0,4,other,"def string_to_bool(v):
    if isinstance(v, bool):
        return v
    if v.lower() in (""yes"", ""true"", ""t"", ""y"", ""1""):
        return True
    elif v.lower() in (""no"", ""false"", ""f"", ""n"", ""0""):
        return False
    else:
        raise ArgumentTypeError(
            f""Truthy value expected: got {v} but expected one of yes/no, true/false, t/f, y/n, 1/0 (case insensitive).""
        )"
huggingface__transformers__src__transformers__hf_argparser.py__make_choice_type_function,make_choice_type_function,huggingface/transformers:src/transformers/hf_argparser.py,13,False,False,False,0,False,True,False,False,False,False,False,False,False,2,0,2,other,"def make_choice_type_function(choices: list) -> Callable[[str], Any]:
    """"""
    Creates a mapping function from each choices string representation to the actual value. Used to support multiple
    value types for a single argument.

    Args:
        choices (list): List of choices.

    Returns:
        Callable[[str], Any]: Mapping function from string representation to actual value for each choice.
    """"""
    str_to_choice = {str(choice): choice for choice in choices}
    return lambda arg: str_to_choice.get(arg, arg)"
huggingface__transformers__src__transformers__hf_argparser.py__HfArg,HfArg,huggingface/transformers:src/transformers/hf_argparser.py,45,False,False,False,0,False,False,False,False,False,False,False,False,False,1,0,4,other,"def HfArg(
    *,
    aliases: str | list[str] | None = None,
    help: str | None = None,
    default: Any = dataclasses.MISSING,
    default_factory: Callable[[], Any] = dataclasses.MISSING,
    metadata: dict | None = None,
    **kwargs,
) -> dataclasses.Field:
    """"""Argument helper enabling a concise syntax to create dataclass fields for parsing with `HfArgumentParser`.

    Example comparing the use of `HfArg` and `dataclasses.field`:
    ```
    @dataclass
    class Args:
        regular_arg: str = dataclasses.field(default=""Huggingface"", metadata={""aliases"": [""--example"", ""-e""], ""help"": ""This syntax could be better!""})
        hf_arg: str = HfArg(default=""Huggingface"", aliases=[""--example"", ""-e""], help=""What a nice syntax!"")
    ```

    Args:
        aliases (Union[str, list[str]], optional):
            Single string or list of strings of aliases to pass on to argparse, e.g. `aliases=[""--example"", ""-e""]`.
            Defaults to None.
        help (str, optional): Help string to pass on to argparse that can be displayed with --help. Defaults to None.
        default (Any, optional):
            Default value for the argument. If not default or default_factory is specified, the argument is required.
            Defaults to dataclasses.MISSING.
        default_factory (Callable[[], Any], optional):
            The default_factory is a 0-argument function called to initialize a field's value. It is useful to provide
            default values for mutable types, e.g. lists: `default_factory=list`. Mutually exclusive with `default=`.
            Defaults to dataclasses.MISSING.
        metadata (dict, optional): Further metadata to pass on to `dataclasses.field`. Defaults to None.

    Returns:
        Field: A `dataclasses.Field` with the desired properties.
    """"""
    if metadata is None:
        # Important, don't use as default param in function signature because dict is mutable and shared across function calls
        metadata = {}
    if aliases is not None:
        metadata[""aliases""] = aliases
    if help is not None:
        metadata[""help""] = help

    return dataclasses.field(metadata=metadata, default=default, default_factory=default_factory, **kwargs)"
huggingface__transformers__src__transformers__hyperparameter_search.py__default_hp_search_backend,default_hp_search_backend,huggingface/transformers:src/transformers/hyperparameter_search.py,16,False,False,False,0,True,False,True,False,False,False,False,False,True,10,1,5,other,"def default_hp_search_backend() -> str:
    available_backends = [backend for backend in ALL_HYPERPARAMETER_SEARCH_BACKENDS.values() if backend.is_available()]
    if len(available_backends) > 0:
        name = available_backends[0].name
        if len(available_backends) > 1:
            logger.info(
                f""{len(available_backends)} hyperparameter search backends available. Using {name} as the default.""
            )
        return name
    raise RuntimeError(
        ""No hyperparameter search backend available.\n""
        + ""\n"".join(
            f"" - To install {backend.name} run {backend.pip_install()}""
            for backend in ALL_HYPERPARAMETER_SEARCH_BACKENDS.values()
        )
    )"
huggingface__transformers__src__transformers__image_processing_utils.py__convert_to_size_dict,convert_to_size_dict,huggingface/transformers:src/transformers/image_processing_utils.py,26,False,False,False,0,False,False,False,True,False,False,False,False,True,7,0,9,data_processing,"def convert_to_size_dict(
    size, max_size: int | None = None, default_to_square: bool = True, height_width_order: bool = True
):
    # By default, if size is an int we assume it represents a tuple of (size, size).
    if isinstance(size, int) and default_to_square:
        if max_size is not None:
            raise ValueError(""Cannot specify both size as an int, with default_to_square=True and max_size"")
        return {""height"": size, ""width"": size}
    # In other configs, if size is an int and default_to_square is False, size represents the length of
    # the shortest edge after resizing.
    elif isinstance(size, int) and not default_to_square:
        size_dict = {""shortest_edge"": size}
        if max_size is not None:
            size_dict[""longest_edge""] = max_size
        return size_dict
    # Otherwise, if size is a tuple it's either (height, width) or (width, height)
    elif isinstance(size, (tuple, list)) and height_width_order:
        return {""height"": size[0], ""width"": size[1]}
    elif isinstance(size, (tuple, list)) and not height_width_order:
        return {""height"": size[1], ""width"": size[0]}
    elif size is None and max_size is not None:
        if default_to_square:
            raise ValueError(""Cannot specify both default_to_square=True and max_size"")
        return {""longest_edge"": max_size}

    raise ValueError(f""Could not convert size input to size dict: {size}"")"
huggingface__transformers__src__transformers__image_processing_utils.py__get_size_dict,get_size_dict,huggingface/transformers:src/transformers/image_processing_utils.py,42,False,False,False,0,False,False,False,True,False,False,False,False,True,6,0,3,data_processing,"def get_size_dict(
    size: int | Iterable[int] | dict[str, int] | None = None,
    max_size: int | None = None,
    height_width_order: bool = True,
    default_to_square: bool = True,
    param_name=""size"",
) -> dict:
    """"""
    Converts the old size parameter in the config into the new dict expected in the config. This is to ensure backwards
    compatibility with the old image processor configs and removes ambiguity over whether the tuple is in (height,
    width) or (width, height) format.

    - If `size` is tuple, it is converted to `{""height"": size[0], ""width"": size[1]}` or `{""height"": size[1], ""width"":
    size[0]}` if `height_width_order` is `False`.
    - If `size` is an int, and `default_to_square` is `True`, it is converted to `{""height"": size, ""width"": size}`.
    - If `size` is an int and `default_to_square` is False, it is converted to `{""shortest_edge"": size}`. If `max_size`
      is set, it is added to the dict as `{""longest_edge"": max_size}`.

    Args:
        size (`Union[int, Iterable[int], dict[str, int]]`, *optional*):
            The `size` parameter to be cast into a size dictionary.
        max_size (`Optional[int]`, *optional*):
            The `max_size` parameter to be cast into a size dictionary.
        height_width_order (`bool`, *optional*, defaults to `True`):
            If `size` is a tuple, whether it's in (height, width) or (width, height) order.
        default_to_square (`bool`, *optional*, defaults to `True`):
            If `size` is an int, whether to default to a square image or not.
    """"""
    if not isinstance(size, dict):
        size_dict = convert_to_size_dict(size, max_size, default_to_square, height_width_order)
        logger.info(
            f""{param_name} should be a dictionary on of the following set of keys: {VALID_SIZE_DICT_KEYS}, got {size}.""
            f"" Converted to {size_dict}."",
        )
    else:
        size_dict = size

    if not is_valid_size_dict(size_dict):
        raise ValueError(
            f""{param_name} must have one of the following set of keys: {VALID_SIZE_DICT_KEYS}, got {size_dict.keys()}""
        )
    return size_dict"
huggingface__transformers__src__transformers__image_processing_utils.py__select_best_resolution,select_best_resolution,huggingface/transformers:src/transformers/image_processing_utils.py,36,True,False,False,1,False,False,False,True,False,False,False,False,False,5,8,3,data_processing,"def select_best_resolution(original_size: tuple, possible_resolutions: list) -> tuple:
    """"""
    Selects the best resolution from a list of possible resolutions based on the original size.

    This is done by calculating the effective and wasted resolution for each possible resolution.

    The best fit resolution is the one that maximizes the effective resolution and minimizes the wasted resolution.

    Args:
        original_size (tuple):
            The original size of the image in the format (height, width).
        possible_resolutions (list):
            A list of possible resolutions in the format [(height1, width1), (height2, width2), ...].

    Returns:
        tuple: The best fit resolution in the format (height, width).
    """"""
    original_height, original_width = original_size
    best_fit = None
    max_effective_resolution = 0
    min_wasted_resolution = float(""inf"")

    for height, width in possible_resolutions:
        scale = min(width / original_width, height / original_height)
        downscaled_width, downscaled_height = int(original_width * scale), int(original_height * scale)
        effective_resolution = min(downscaled_width * downscaled_height, original_width * original_height)
        wasted_resolution = (width * height) - effective_resolution

        if effective_resolution > max_effective_resolution or (
            effective_resolution == max_effective_resolution and wasted_resolution < min_wasted_resolution
        ):
            max_effective_resolution = effective_resolution
            min_wasted_resolution = wasted_resolution
            best_fit = (height, width)

    return best_fit"
huggingface__transformers__src__transformers__image_processing_utils.py__get_patch_output_size,get_patch_output_size,huggingface/transformers:src/transformers/image_processing_utils.py,18,False,False,False,0,False,False,False,True,False,False,False,False,False,5,4,2,data_processing,"def get_patch_output_size(image, target_resolution, input_data_format):
    """"""
    Given an image and a target resolution, calculate the output size of the image after cropping to the target
    """"""
    original_height, original_width = get_image_size(image, channel_dim=input_data_format)
    target_height, target_width = target_resolution

    scale_w = target_width / original_width
    scale_h = target_height / original_height

    if scale_w < scale_h:
        new_width = target_width
        new_height = min(math.ceil(original_height * scale_w), target_height)
    else:
        new_height = target_height
        new_width = min(math.ceil(original_width * scale_h), target_width)

    return new_height, new_width"
huggingface__transformers__src__transformers__image_processing_utils_fast.py__validate_fast_preprocess_arguments,validate_fast_preprocess_arguments,huggingface/transformers:src/transformers/image_processing_utils_fast.py,36,False,False,False,0,False,False,False,True,False,True,False,False,False,4,0,3,ml,"def validate_fast_preprocess_arguments(
    do_rescale: bool | None = None,
    rescale_factor: float | None = None,
    do_normalize: bool | None = None,
    image_mean: float | list[float] | None = None,
    image_std: float | list[float] | None = None,
    do_center_crop: bool | None = None,
    crop_size: SizeDict | None = None,
    do_resize: bool | None = None,
    size: SizeDict | None = None,
    interpolation: Optional[""tvF.InterpolationMode""] = None,
    return_tensors: str | TensorType | None = None,
    data_format: ChannelDimension = ChannelDimension.FIRST,
):
    """"""
    Checks validity of typically used arguments in an `ImageProcessorFast` `preprocess` method.
    Raises `ValueError` if arguments incompatibility is caught.
    """"""
    validate_preprocess_arguments(
        do_rescale=do_rescale,
        rescale_factor=rescale_factor,
        do_normalize=do_normalize,
        image_mean=image_mean,
        image_std=image_std,
        do_center_crop=do_center_crop,
        crop_size=crop_size,
        do_resize=do_resize,
        size=size,
        interpolation=interpolation,
    )
    # Extra checks for ImageProcessorFast
    if return_tensors is not None and return_tensors != ""pt"":
        raise ValueError(""Only returning PyTorch tensors is currently supported."")

    if data_format != ChannelDimension.FIRST:
        raise ValueError(""Only channel first data format is currently supported."")"
huggingface__transformers__src__transformers__image_processing_utils_fast.py__safe_squeeze,safe_squeeze,huggingface/transformers:src/transformers/image_processing_utils_fast.py,11,False,False,False,0,False,False,False,True,False,True,False,False,False,2,0,3,ml,"def safe_squeeze(tensor: ""torch.Tensor"", axis: int | None = None) -> ""torch.Tensor"":
    """"""
    Squeezes a tensor, but only if the axis specified has dim 1.
    """"""
    if axis is None:
        return tensor.squeeze()

    try:
        return tensor.squeeze(axis=axis)
    except ValueError:
        return tensor"
huggingface__transformers__src__transformers__image_processing_utils_fast.py__divide_to_patches,divide_to_patches,huggingface/transformers:src/transformers/image_processing_utils_fast.py,22,True,False,True,2,False,False,False,True,False,True,False,False,False,4,2,3,ml,"def divide_to_patches(
    image: Union[np.ndarray, ""torch.Tensor""], patch_size: int
) -> list[Union[np.ndarray, ""torch.Tensor""]]:
    """"""
    Divides an image into patches of a specified size.

    Args:
        image (`Union[np.array, ""torch.Tensor""]`):
            The input image.
        patch_size (`int`):
            The size of each patch.
    Returns:
        list: A list of Union[np.array, ""torch.Tensor""] representing the patches.
    """"""
    patches = []
    height, width = get_image_size(image, channel_dim=ChannelDimension.FIRST)
    for i in range(0, height, patch_size):
        for j in range(0, width, patch_size):
            patch = image[:, i : i + patch_size, j : j + patch_size]
            patches.append(patch)

    return patches"
huggingface__transformers__src__transformers__image_transforms.py__to_channel_dimension_format,to_channel_dimension_format,huggingface/transformers:src/transformers/image_transforms.py,41,False,False,False,0,False,False,False,True,False,True,False,False,True,12,10,6,ml,"def to_channel_dimension_format(
    image: np.ndarray,
    channel_dim: ChannelDimension | str,
    input_channel_dim: ChannelDimension | str | None = None,
) -> np.ndarray:
    """"""
    Converts `image` to the channel dimension format specified by `channel_dim`. The input
    can have arbitrary number of leading dimensions. Only last three dimension will be permuted
    to format the `image`.

    Args:
        image (`numpy.ndarray`):
            The image to have its channel dimension set.
        channel_dim (`ChannelDimension`):
            The channel dimension format to use.
        input_channel_dim (`ChannelDimension`, *optional*):
            The channel dimension format of the input image. If not provided, it will be inferred from the input image.

    Returns:
        `np.ndarray`: The image with the channel dimension set to `channel_dim`.
    """"""
    if not isinstance(image, np.ndarray):
        raise TypeError(f""Input image must be of type np.ndarray, got {type(image)}"")

    if input_channel_dim is None:
        input_channel_dim = infer_channel_dimension_format(image)

    target_channel_dim = ChannelDimension(channel_dim)
    if input_channel_dim == target_channel_dim:
        return image

    if target_channel_dim == ChannelDimension.FIRST:
        axes = list(range(image.ndim - 3)) + [image.ndim - 1, image.ndim - 3, image.ndim - 2]
        image = image.transpose(axes)
    elif target_channel_dim == ChannelDimension.LAST:
        axes = list(range(image.ndim - 3)) + [image.ndim - 2, image.ndim - 1, image.ndim - 3]
        image = image.transpose(axes)
    else:
        raise ValueError(f""Unsupported channel dimension format: {channel_dim}"")

    return image"
huggingface__transformers__src__transformers__image_transforms.py__rescale,rescale,huggingface/transformers:src/transformers/image_transforms.py,36,False,False,False,0,False,False,False,True,False,True,False,False,True,6,1,3,ml,"def rescale(
    image: np.ndarray,
    scale: float,
    data_format: ChannelDimension | None = None,
    dtype: np.dtype = np.float32,
    input_data_format: str | ChannelDimension | None = None,
) -> np.ndarray:
    """"""
    Rescales `image` by `scale`.

    Args:
        image (`np.ndarray`):
            The image to rescale.
        scale (`float`):
            The scale to use for rescaling the image.
        data_format (`ChannelDimension`, *optional*):
            The channel dimension format of the image. If not provided, it will be the same as the input image.
        dtype (`np.dtype`, *optional*, defaults to `np.float32`):
            The dtype of the output image. Defaults to `np.float32`. Used for backwards compatibility with feature
            extractors.
        input_data_format (`ChannelDimension`, *optional*):
            The channel dimension format of the input image. If not provided, it will be inferred from the input image.

    Returns:
        `np.ndarray`: The rescaled image.
    """"""
    if not isinstance(image, np.ndarray):
        raise TypeError(f""Input image must be of type np.ndarray, got {type(image)}"")

    rescaled_image = image.astype(np.float64) * scale  # Numpy type promotion has changed, so always upcast first
    if data_format is not None:
        rescaled_image = to_channel_dimension_format(rescaled_image, data_format, input_data_format)

    rescaled_image = rescaled_image.astype(dtype)  # Finally downcast to the desired dtype at the end

    return rescaled_image"
huggingface__transformers__src__transformers__image_transforms.py__to_pil_image,to_pil_image,huggingface/transformers:src/transformers/image_transforms.py,50,False,False,False,0,False,False,False,True,False,True,False,False,True,13,0,7,ml,"def to_pil_image(
    image: Union[np.ndarray, ""PIL.Image.Image"", ""torch.Tensor""],
    do_rescale: bool | None = None,
    image_mode: str | None = None,
    input_data_format: str | ChannelDimension | None = None,
) -> ""PIL.Image.Image"":
    """"""
    Converts `image` to a PIL Image. Optionally rescales it and puts the channel dimension back as the last axis if
    needed.

    Args:
        image (`PIL.Image.Image` or `numpy.ndarray` or `torch.Tensor`):
            The image to convert to the `PIL.Image` format.
        do_rescale (`bool`, *optional*):
            Whether or not to apply the scaling factor (to make pixel values integers between 0 and 255). Will default
            to `True` if the image type is a floating type and casting to `int` would result in a loss of precision,
            and `False` otherwise.
        image_mode (`str`, *optional*):
            The mode to use for the PIL image. If unset, will use the default mode for the input image type.
        input_data_format (`ChannelDimension`, *optional*):
            The channel dimension format of the input image. If unset, will use the inferred format from the input.

    Returns:
        `PIL.Image.Image`: The converted image.
    """"""
    requires_backends(to_pil_image, [""vision""])

    if isinstance(image, PIL.Image.Image):
        return image

    # Convert all tensors to numpy arrays before converting to PIL image
    if is_torch_tensor(image):
        image = image.numpy()
    elif not isinstance(image, np.ndarray):
        raise ValueError(f""Input image type not supported: {type(image)}"")

    # If the channel has been moved to first dim, we put it back at the end.
    image = to_channel_dimension_format(image, ChannelDimension.LAST, input_data_format)

    # If there is a single channel, we squeeze it, as otherwise PIL can't handle it.
    image = np.squeeze(image, axis=-1) if image.shape[-1] == 1 else image

    # PIL.Image can only store uint8 values so we rescale the image to be between 0 and 255 if needed.
    do_rescale = _rescale_for_pil_conversion(image) if do_rescale is None else do_rescale

    if do_rescale:
        image = rescale(image, 255)

    image = image.astype(np.uint8)
    return PIL.Image.fromarray(image, mode=image_mode)"
huggingface__transformers__src__transformers__image_transforms.py__get_size_with_aspect_ratio,get_size_with_aspect_ratio,huggingface/transformers:src/transformers/image_transforms.py,37,False,False,False,0,False,False,False,True,False,True,False,False,False,10,12,7,ml,"def get_size_with_aspect_ratio(image_size, size, max_size=None) -> tuple[int, int]:
    """"""
    Computes the output image size given the input image size and the desired output size.

    Args:
        image_size (`tuple[int, int]`):
            The input image size.
        size (`int`):
            The desired output size.
        max_size (`int`, *optional*):
            The maximum allowed output size.
    """"""
    height, width = image_size
    raw_size = None
    if max_size is not None:
        min_original_size = float(min((height, width)))
        max_original_size = float(max((height, width)))
        if max_original_size / min_original_size * size > max_size:
            raw_size = max_size * min_original_size / max_original_size
            size = int(round(raw_size))

    if (height <= width and height == size) or (width <= height and width == size):
        oh, ow = height, width
    elif width < height:
        ow = size
        if max_size is not None and raw_size is not None:
            oh = int(raw_size * height / width)
        else:
            oh = int(size * height / width)
    else:
        oh = size
        if max_size is not None and raw_size is not None:
            ow = int(raw_size * width / height)
        else:
            ow = int(size * width / height)

    return (oh, ow)"
huggingface__transformers__src__transformers__image_transforms.py__get_resize_output_image_size,get_resize_output_image_size,huggingface/transformers:src/transformers/image_transforms.py,65,False,False,False,0,False,False,False,True,False,True,False,False,True,9,4,10,ml,"def get_resize_output_image_size(
    input_image: np.ndarray,
    size: int | tuple[int, int] | list[int] | tuple[int, ...],
    default_to_square: bool = True,
    max_size: int | None = None,
    input_data_format: str | ChannelDimension | None = None,
) -> tuple:
    """"""
    Find the target (height, width) dimension of the output image after resizing given the input image and the desired
    size.

    Args:
        input_image (`np.ndarray`):
            The image to resize.
        size (`int` or `tuple[int, int]` or list[int] or `tuple[int]`):
            The size to use for resizing the image. If `size` is a sequence like (h, w), output size will be matched to
            this.

            If `size` is an int and `default_to_square` is `True`, then image will be resized to (size, size). If
            `size` is an int and `default_to_square` is `False`, then smaller edge of the image will be matched to this
            number. i.e, if height > width, then image will be rescaled to (size * height / width, size).
        default_to_square (`bool`, *optional*, defaults to `True`):
            How to convert `size` when it is a single int. If set to `True`, the `size` will be converted to a square
            (`size`,`size`). If set to `False`, will replicate
            [`torchvision.transforms.Resize`](https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.Resize)
            with support for resizing only the smallest edge and providing an optional `max_size`.
        max_size (`int`, *optional*):
            The maximum allowed for the longer edge of the resized image: if the longer edge of the image is greater
            than `max_size` after being resized according to `size`, then the image is resized again so that the longer
            edge is equal to `max_size`. As a result, `size` might be overruled, i.e the smaller edge may be shorter
            than `size`. Only used if `default_to_square` is `False`.
        input_data_format (`ChannelDimension`, *optional*):
            The channel dimension format of the input image. If unset, will use the inferred format from the input.

    Returns:
        `tuple`: The target (height, width) dimension of the output image after resizing.
    """"""
    if isinstance(size, (tuple, list)):
        if len(size) == 2:
            return tuple(size)
        elif len(size) == 1:
            # Perform same logic as if size was an int
            size = size[0]
        else:
            raise ValueError(""size must have 1 or 2 elements if it is a list or tuple"")

    if default_to_square:
        return (size, size)

    height, width = get_image_size(input_image, input_data_format)
    short, long = (width, height) if width <= height else (height, width)
    requested_new_short = size

    new_short, new_long = requested_new_short, int(requested_new_short * long / short)

    if max_size is not None:
        if max_size <= requested_new_short:
            raise ValueError(
                f""max_size = {max_size} must be strictly greater than the requested ""
                f""size for the smaller edge size = {size}""
            )
        if new_long > max_size:
            new_short, new_long = int(max_size * new_short / new_long), max_size

    return (new_long, new_short) if width <= height else (new_short, new_long)"
huggingface__transformers__src__transformers__image_transforms.py__resize,resize,huggingface/transformers:src/transformers/image_transforms.py,69,False,False,False,0,False,False,False,True,False,True,False,False,False,12,1,9,ml,"def resize(
    image: np.ndarray,
    size: tuple[int, int],
    resample: Optional[""PILImageResampling""] = None,
    reducing_gap: int | None = None,
    data_format: ChannelDimension | None = None,
    return_numpy: bool = True,
    input_data_format: str | ChannelDimension | None = None,
) -> np.ndarray:
    """"""
    Resizes `image` to `(height, width)` specified by `size` using the PIL library.

    Args:
        image (`np.ndarray`):
            The image to resize.
        size (`tuple[int, int]`):
            The size to use for resizing the image.
        resample (`int`, *optional*, defaults to `PILImageResampling.BILINEAR`):
            The filter to user for resampling.
        reducing_gap (`int`, *optional*):
            Apply optimization by resizing the image in two steps. The bigger `reducing_gap`, the closer the result to
            the fair resampling. See corresponding Pillow documentation for more details.
        data_format (`ChannelDimension`, *optional*):
            The channel dimension format of the output image. If unset, will use the inferred format from the input.
        return_numpy (`bool`, *optional*, defaults to `True`):
            Whether or not to return the resized image as a numpy array. If False a `PIL.Image.Image` object is
            returned.
        input_data_format (`ChannelDimension`, *optional*):
            The channel dimension format of the input image. If unset, will use the inferred format from the input.

    Returns:
        `np.ndarray`: The resized image.
    """"""
    requires_backends(resize, [""vision""])

    resample = resample if resample is not None else PILImageResampling.BILINEAR

    if not len(size) == 2:
        raise ValueError(""size must have 2 elements"")

    # For all transformations, we want to keep the same data format as the input image unless otherwise specified.
    # The resized image from PIL will always have channels last, so find the input format first.
    if input_data_format is None:
        input_data_format = infer_channel_dimension_format(image)
    data_format = input_data_format if data_format is None else data_format

    # To maintain backwards compatibility with the resizing done in previous image feature extractors, we use
    # the pillow library to resize the image and then convert back to numpy
    do_rescale = False
    if not isinstance(image, PIL.Image.Image):
        do_rescale = _rescale_for_pil_conversion(image)
        image = to_pil_image(image, do_rescale=do_rescale, input_data_format=input_data_format)
    height, width = size
    # PIL images are in the format (width, height)
    resized_image = image.resize((width, height), resample=resample, reducing_gap=reducing_gap)

    if return_numpy:
        resized_image = np.array(resized_image)
        # If the input image channel dimension was of size 1, then it is dropped when converting to a PIL image
        # so we need to add it back if necessary.
        resized_image = np.expand_dims(resized_image, axis=-1) if resized_image.ndim == 2 else resized_image
        # The image is always in channels last format after converting from a PIL image
        resized_image = to_channel_dimension_format(
            resized_image, data_format, input_channel_dim=ChannelDimension.LAST
        )
        # If an image was rescaled to be in the range [0, 255] before converting to a PIL image, then we need to
        # rescale it back to the original range.
        resized_image = rescale(resized_image, 1 / 255) if do_rescale else resized_image
    return resized_image"
huggingface__transformers__src__transformers__image_transforms.py__normalize,normalize,huggingface/transformers:src/transformers/image_transforms.py,59,False,False,False,0,False,False,False,True,False,True,False,False,True,17,6,10,ml,"def normalize(
    image: np.ndarray,
    mean: float | Collection[float],
    std: float | Collection[float],
    data_format: ChannelDimension | None = None,
    input_data_format: str | ChannelDimension | None = None,
) -> np.ndarray:
    """"""
    Normalizes `image` using the mean and standard deviation specified by `mean` and `std`.

    image = (image - mean) / std

    Args:
        image (`np.ndarray`):
            The image to normalize.
        mean (`float` or `Collection[float]`):
            The mean to use for normalization.
        std (`float` or `Collection[float]`):
            The standard deviation to use for normalization.
        data_format (`ChannelDimension`, *optional*):
            The channel dimension format of the output image. If unset, will use the inferred format from the input.
        input_data_format (`ChannelDimension`, *optional*):
            The channel dimension format of the input image. If unset, will use the inferred format from the input.
    """"""
    if not isinstance(image, np.ndarray):
        raise TypeError(""image must be a numpy array"")

    if input_data_format is None:
        input_data_format = infer_channel_dimension_format(image)

    channel_axis = get_channel_dimension_axis(image, input_data_format=input_data_format)
    num_channels = image.shape[channel_axis]

    # We cast to float32 to avoid errors that can occur when subtracting uint8 values.
    # We preserve the original dtype if it is a float type to prevent upcasting float16.
    if not np.issubdtype(image.dtype, np.floating):
        image = image.astype(np.float32)

    if isinstance(mean, Collection):
        if len(mean) != num_channels:
            raise ValueError(f""mean must have {num_channels} elements if it is an iterable, got {len(mean)}"")
    else:
        mean = [mean] * num_channels
    mean = np.array(mean, dtype=image.dtype)

    if isinstance(std, Collection):
        if len(std) != num_channels:
            raise ValueError(f""std must have {num_channels} elements if it is an iterable, got {len(std)}"")
    else:
        std = [std] * num_channels
    std = np.array(std, dtype=image.dtype)

    if input_data_format == ChannelDimension.LAST:
        image = (image - mean) / std
    else:
        image = ((image.T - mean) / std).T

    image = to_channel_dimension_format(image, data_format, input_data_format) if data_format is not None else image
    return image"
huggingface__transformers__src__transformers__image_transforms.py__center_crop,center_crop,huggingface/transformers:src/transformers/image_transforms.py,82,False,False,False,0,False,False,False,True,False,True,False,False,True,23,13,6,ml,"def center_crop(
    image: np.ndarray,
    size: tuple[int, int],
    data_format: str | ChannelDimension | None = None,
    input_data_format: str | ChannelDimension | None = None,
) -> np.ndarray:
    """"""
    Crops the `image` to the specified `size` using a center crop. Note that if the image is too small to be cropped to
    the size given, it will be padded (so the returned result will always be of size `size`).

    Args:
        image (`np.ndarray`):
            The image to crop.
        size (`tuple[int, int]`):
            The target size for the cropped image.
        data_format (`str` or `ChannelDimension`, *optional*):
            The channel dimension format for the output image. Can be one of:
                - `""channels_first""` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.
                - `""channels_last""` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.
            If unset, will use the inferred format of the input image.
        input_data_format (`str` or `ChannelDimension`, *optional*):
            The channel dimension format for the input image. Can be one of:
                - `""channels_first""` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.
                - `""channels_last""` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.
            If unset, will use the inferred format of the input image.
    Returns:
        `np.ndarray`: The cropped image.
    """"""
    requires_backends(center_crop, [""vision""])

    if not isinstance(image, np.ndarray):
        raise TypeError(f""Input image must be of type np.ndarray, got {type(image)}"")

    if not isinstance(size, Iterable) or len(size) != 2:
        raise ValueError(""size must have 2 elements representing the height and width of the output image"")

    if input_data_format is None:
        input_data_format = infer_channel_dimension_format(image)
    output_data_format = data_format if data_format is not None else input_data_format

    # We perform the crop in (C, H, W) format and then convert to the output format
    image = to_channel_dimension_format(image, ChannelDimension.FIRST, input_data_format)

    orig_height, orig_width = get_image_size(image, ChannelDimension.FIRST)
    crop_height, crop_width = size
    crop_height, crop_width = int(crop_height), int(crop_width)

    # In case size is odd, (image_shape[0] + size[0]) // 2 won't give the proper result.
    top = (orig_height - crop_height) // 2
    bottom = top + crop_height
    # In case size is odd, (image_shape[1] + size[1]) // 2 won't give the proper result.
    left = (orig_width - crop_width) // 2
    right = left + crop_width

    # Check if cropped area is within image boundaries
    if top >= 0 and bottom <= orig_height and left >= 0 and right <= orig_width:
        image = image[..., top:bottom, left:right]
        image = to_channel_dimension_format(image, output_data_format, ChannelDimension.FIRST)
        return image

    # Otherwise, we may need to pad if the image is too small. Oh joy...
    new_height = max(crop_height, orig_height)
    new_width = max(crop_width, orig_width)
    new_shape = image.shape[:-2] + (new_height, new_width)
    new_image = np.zeros_like(image, shape=new_shape)

    # If the image is too small, pad it with zeros
    top_pad = ceil((new_height - orig_height) / 2)
    bottom_pad = top_pad + orig_height
    left_pad = ceil((new_width - orig_width) / 2)
    right_pad = left_pad + orig_width
    new_image[..., top_pad:bottom_pad, left_pad:right_pad] = image

    top += top_pad
    bottom += top_pad
    left += left_pad
    right += left_pad

    new_image = new_image[..., max(0, top) : min(new_height, bottom), max(0, left) : min(new_width, right)]
    new_image = to_channel_dimension_format(new_image, output_data_format, ChannelDimension.FIRST)

    return new_image"
huggingface__transformers__src__transformers__image_transforms.py__center_to_corners_format,center_to_corners_format,huggingface/transformers:src/transformers/image_transforms.py,16,False,False,False,0,False,False,False,True,False,True,False,False,True,6,0,3,ml,"def center_to_corners_format(bboxes_center: TensorType) -> TensorType:
    """"""
    Converts bounding boxes from center format to corners format.

    center format: contains the coordinate for the center of the box and its width, height dimensions
        (center_x, center_y, width, height)
    corners format: contains the coordinates for the top-left and bottom-right corners of the box
        (top_left_x, top_left_y, bottom_right_x, bottom_right_y)
    """"""
    # Function is used during model forward pass, so we use torch if relevant, without converting to numpy
    if is_torch_tensor(bboxes_center):
        return _center_to_corners_format_torch(bboxes_center)
    elif isinstance(bboxes_center, np.ndarray):
        return _center_to_corners_format_numpy(bboxes_center)

    raise ValueError(f""Unsupported input type {type(bboxes_center)}"")"
huggingface__transformers__src__transformers__image_transforms.py__corners_to_center_format,corners_to_center_format,huggingface/transformers:src/transformers/image_transforms.py,16,False,False,False,0,False,False,False,True,False,True,False,False,True,6,0,3,ml,"def corners_to_center_format(bboxes_corners: TensorType) -> TensorType:
    """"""
    Converts bounding boxes from corners format to center format.

    corners format: contains the coordinates for the top-left and bottom-right corners of the box
        (top_left_x, top_left_y, bottom_right_x, bottom_right_y)
    center format: contains the coordinate for the center of the box and its the width, height dimensions
        (center_x, center_y, width, height)
    """"""
    # Inverse function accepts different input types so implemented here too
    if is_torch_tensor(bboxes_corners):
        return _corners_to_center_format_torch(bboxes_corners)
    elif isinstance(bboxes_corners, np.ndarray):
        return _corners_to_center_format_numpy(bboxes_corners)

    raise ValueError(f""Unsupported input type {type(bboxes_corners)}"")"
huggingface__transformers__src__transformers__image_transforms.py__id_to_rgb,id_to_rgb,huggingface/transformers:src/transformers/image_transforms.py,17,True,False,False,2,False,False,False,True,False,True,False,False,False,8,3,4,ml,"def id_to_rgb(id_map):
    """"""
    Converts unique ID to RGB color.
    """"""
    if isinstance(id_map, np.ndarray):
        id_map_copy = id_map.copy()
        rgb_shape = tuple(list(id_map.shape) + [3])
        rgb_map = np.zeros(rgb_shape, dtype=np.uint8)
        for i in range(3):
            rgb_map[..., i] = id_map_copy % 256
            id_map_copy //= 256
        return rgb_map
    color = []
    for _ in range(3):
        color.append(id_map % 256)
        id_map //= 256
    return color"
huggingface__transformers__src__transformers__image_transforms.py__pad,pad,huggingface/transformers:src/transformers/image_transforms.py,84,False,False,False,0,False,False,False,True,False,True,False,False,True,19,0,13,ml,"def pad(
    image: np.ndarray,
    padding: int | tuple[int, int] | Iterable[tuple[int, int]],
    mode: PaddingMode = PaddingMode.CONSTANT,
    constant_values: float | Iterable[float] = 0.0,
    data_format: str | ChannelDimension | None = None,
    input_data_format: str | ChannelDimension | None = None,
) -> np.ndarray:
    """"""
    Pads the `image` with the specified (height, width) `padding` and `mode`.

    Args:
        image (`np.ndarray`):
            The image to pad.
        padding (`int` or `tuple[int, int]` or `Iterable[tuple[int, int]]`):
            Padding to apply to the edges of the height, width axes. Can be one of three formats:
            - `((before_height, after_height), (before_width, after_width))` unique pad widths for each axis.
            - `((before, after),)` yields same before and after pad for height and width.
            - `(pad,)` or int is a shortcut for before = after = pad width for all axes.
        mode (`PaddingMode`):
            The padding mode to use. Can be one of:
                - `""constant""`: pads with a constant value.
                - `""reflect""`: pads with the reflection of the vector mirrored on the first and last values of the
                  vector along each axis.
                - `""replicate""`: pads with the replication of the last value on the edge of the array along each axis.
                - `""symmetric""`: pads with the reflection of the vector mirrored along the edge of the array.
        constant_values (`float` or `Iterable[float]`, *optional*):
            The value to use for the padding if `mode` is `""constant""`.
        data_format (`str` or `ChannelDimension`, *optional*):
            The channel dimension format for the output image. Can be one of:
                - `""channels_first""` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.
                - `""channels_last""` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.
            If unset, will use same as the input image.
        input_data_format (`str` or `ChannelDimension`, *optional*):
            The channel dimension format for the input image. Can be one of:
                - `""channels_first""` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.
                - `""channels_last""` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.
            If unset, will use the inferred format of the input image.

    Returns:
        `np.ndarray`: The padded image.

    """"""
    if input_data_format is None:
        input_data_format = infer_channel_dimension_format(image)

    def _expand_for_data_format(values):
        """"""
        Convert values to be in the format expected by np.pad based on the data format.
        """"""
        if isinstance(values, (int, float)):
            values = ((values, values), (values, values))
        elif isinstance(values, tuple) and len(values) == 1:
            values = ((values[0], values[0]), (values[0], values[0]))
        elif isinstance(values, tuple) and len(values) == 2 and isinstance(values[0], int):
            values = (values, values)
        elif isinstance(values, tuple) and len(values) == 2 and isinstance(values[0], tuple):
            pass
        else:
            raise ValueError(f""Unsupported format: {values}"")

        # add 0 for channel dimension
        values = ((0, 0), *values) if input_data_format == ChannelDimension.FIRST else (*values, (0, 0))

        # Add additional padding if there's a batch dimension
        values = ((0, 0), *values) if image.ndim == 4 else values
        return values

    padding = _expand_for_data_format(padding)

    if mode == PaddingMode.CONSTANT:
        constant_values = _expand_for_data_format(constant_values)
        image = np.pad(image, padding, mode=""constant"", constant_values=constant_values)
    elif mode == PaddingMode.REFLECT:
        image = np.pad(image, padding, mode=""reflect"")
    elif mode == PaddingMode.REPLICATE:
        image = np.pad(image, padding, mode=""edge"")
    elif mode == PaddingMode.SYMMETRIC:
        image = np.pad(image, padding, mode=""symmetric"")
    else:
        raise ValueError(f""Invalid padding mode: {mode}"")

    image = to_channel_dimension_format(image, data_format, input_data_format) if data_format is not None else image
    return image"
huggingface__transformers__src__transformers__image_transforms.py__convert_to_rgb,convert_to_rgb,huggingface/transformers:src/transformers/image_transforms.py,18,False,False,False,0,False,False,False,True,False,True,False,False,False,3,0,3,ml,"def convert_to_rgb(image: ImageInput) -> ImageInput:
    """"""
    Converts an image to RGB format. Only converts if the image is of type PIL.Image.Image, otherwise returns the image
    as is.
    Args:
        image (Image):
            The image to convert.
    """"""
    requires_backends(convert_to_rgb, [""vision""])

    if not isinstance(image, PIL.Image.Image):
        return image

    if image.mode == ""RGB"":
        return image

    image = image.convert(""RGB"")
    return image"
huggingface__transformers__src__transformers__image_transforms.py__flip_channel_order,flip_channel_order,huggingface/transformers:src/transformers/image_transforms.py,36,False,False,False,0,False,False,False,True,False,True,False,False,True,3,0,5,ml,"def flip_channel_order(
    image: np.ndarray,
    data_format: ChannelDimension | None = None,
    input_data_format: str | ChannelDimension | None = None,
) -> np.ndarray:
    """"""
    Flips the channel order of the image.

    If the image is in RGB format, it will be converted to BGR and vice versa.

    Args:
        image (`np.ndarray`):
            The image to flip.
        data_format (`ChannelDimension`, *optional*):
            The channel dimension format for the output image. Can be one of:
                - `ChannelDimension.FIRST`: image in (num_channels, height, width) format.
                - `ChannelDimension.LAST`: image in (height, width, num_channels) format.
            If unset, will use same as the input image.
        input_data_format (`ChannelDimension`, *optional*):
            The channel dimension format for the input image. Can be one of:
                - `ChannelDimension.FIRST`: image in (num_channels, height, width) format.
                - `ChannelDimension.LAST`: image in (height, width, num_channels) format.
            If unset, will use the inferred format of the input image.
    """"""
    input_data_format = infer_channel_dimension_format(image) if input_data_format is None else input_data_format

    if input_data_format == ChannelDimension.LAST:
        image = image[..., ::-1]
    elif input_data_format == ChannelDimension.FIRST:
        image = image[::-1, ...]
    else:
        raise ValueError(f""Unsupported channel dimension: {input_data_format}"")

    if data_format is not None:
        image = to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format)
    return image"
huggingface__transformers__src__transformers__image_transforms.py__group_images_by_shape,group_images_by_shape,huggingface/transformers:src/transformers/image_transforms.py,63,False,False,False,0,True,True,False,True,False,True,False,False,False,10,0,9,ml,"def group_images_by_shape(
    images: Union[list[""torch.Tensor""], ""torch.Tensor""],
    *paired_inputs,
    disable_grouping: bool | None,
    is_nested: bool = False,
) -> tuple[dict, ...]:
    """"""
    Groups images by shape.
    Returns a dictionary with the shape as key and a list of images with that shape as value,
    and a dictionary with the index of the image in the original list as key and the shape and index in the grouped list as value.

    The function supports both flat lists of tensors and nested structures.
    The input must be either all flat or all nested, not a mix of both.

    Args:
        images (Union[list[""torch.Tensor""], ""torch.Tensor""]):
            A list of images or a single tensor
        paired_inputs (Any, *optional*):
            Zero or more lists that mirror the structure of `images` (flat list, or list of lists when
            `is_nested=True`). Each element is paired 1:1 with the corresponding image so it can be grouped by the
            same shape key. These paired values are grouped alongside `images` but are not stacked in the output, so
            they do not need to be tensors.
        disable_grouping (bool):
            Whether to disable grouping. If None, will be set to True if the images are on CPU, and False otherwise.
            This choice is based on empirical observations, as detailed here: https://github.com/huggingface/transformers/pull/38157
        is_nested (bool, *optional*, defaults to False):
            Whether the images are nested.

    Returns:
        tuple[dict, ...]:
            - A dictionary with shape as key and list/batch of images with that shape as value
            - Zero or more dictionaries (one per argument in `*paired_inputs`) grouped consistently with `images`; these carry
              the corresponding per-item values and are not stacked
            - A dictionary mapping original indices to (shape, index) tuples
    """"""
    # If disable grouping is not explicitly provided, we favor disabling it if the images are on CPU, and enabling it otherwise.
    if disable_grouping is None:
        device = _get_device_from_images(images, is_nested)
        disable_grouping = device == ""cpu""

    if disable_grouping:
        grouped_images_index = {key: (key, 0) for key, _ in _iterate_items(images, is_nested)}
        if is_nested:
            grouped_images_index[""_num_sublists""] = len(images)

        return (
            {key: img.unsqueeze(0) for key, img in _iterate_items(images, is_nested)},
            *[
                {key: item.unsqueeze(0) for key, item in _iterate_items(paired_list, is_nested)}
                for paired_list in paired_inputs
            ],
            grouped_images_index,
        )

    # Handle single level nested structure
    grouped_images, *paired_grouped_values, grouped_images_index = _group_images_by_shape(
        images, *paired_inputs, is_nested=is_nested
    )

    # Stack images with the same shape
    grouped_images = {shape: torch.stack(images_list, dim=0) for shape, images_list in grouped_images.items()}

    return grouped_images, *paired_grouped_values, grouped_images_index"
huggingface__transformers__src__transformers__image_transforms.py__reorder_images,reorder_images,huggingface/transformers:src/transformers/image_transforms.py,30,False,False,False,0,True,False,False,True,False,True,False,False,False,3,0,3,ml,"def reorder_images(
    processed_images: dict[tuple[int, int], ""torch.Tensor""],
    grouped_images_index: dict[int | tuple[int, int], tuple[tuple[int, int], int]],
    is_nested: bool = False,
) -> Union[list[""torch.Tensor""], ""torch.Tensor""]:
    """"""
    Reconstructs images in the original order, preserving the original structure (nested or not).
    The input structure is either all flat or all nested.

    Args:
        processed_images (dict[tuple[int, int], ""torch.Tensor""]):
            Dictionary mapping shapes to batched processed images.
        grouped_images_index (dict[Union[int, tuple[int, int]], tuple[tuple[int, int], int]]):
            Dictionary mapping original indices to (shape, index) tuples.
        is_nested (bool, *optional*, defaults to False):
            Whether the images are nested. Cannot be inferred from the input, as some processing functions outputs nested images.
            even with non nested images,e.g functions splitting images into patches. We thus can't deduce is_nested from the input.


    Returns:
        Union[list[""torch.Tensor""], ""torch.Tensor""]:
            Images in the original structure.
    """"""
    if not is_nested:
        return [
            processed_images[grouped_images_index[i][0]][grouped_images_index[i][1]]
            for i in range(len(grouped_images_index))
        ]

    return _reconstruct_nested_structure(grouped_images_index, processed_images)"
huggingface__transformers__src__transformers__image_utils.py__valid_images,valid_images,huggingface/transformers:src/transformers/image_utils.py,10,True,False,False,1,False,False,False,True,False,True,False,False,False,3,0,5,ml,"def valid_images(imgs):
    # If we have an list of images, make sure every image is valid
    if isinstance(imgs, (list, tuple)):
        for img in imgs:
            if not valid_images(img):
                return False
    # If not a list of tuple, we have been given a single image or batched tensor of images
    elif not is_valid_image(imgs):
        return False
    return True"
huggingface__transformers__src__transformers__image_utils.py__make_list_of_images,make_list_of_images,huggingface/transformers:src/transformers/image_utils.py,36,False,False,False,0,False,False,False,True,False,True,False,False,True,7,2,6,ml,"def make_list_of_images(images, expected_ndims: int = 3) -> list[ImageInput]:
    """"""
    Ensure that the output is a list of images. If the input is a single image, it is converted to a list of length 1.
    If the input is a batch of images, it is converted to a list of images.

    Args:
        images (`ImageInput`):
            Image of images to turn into a list of images.
        expected_ndims (`int`, *optional*, defaults to 3):
            Expected number of dimensions for a single input image. If the input image has a different number of
            dimensions, an error is raised.
    """"""
    if is_batched(images):
        return images

    # Either the input is a single image, in which case we create a list of length 1
    if is_pil_image(images):
        # PIL images are never batched
        return [images]

    if is_valid_image(images):
        if images.ndim == expected_ndims + 1:
            # Batch of images
            images = list(images)
        elif images.ndim == expected_ndims:
            # Single image
            images = [images]
        else:
            raise ValueError(
                f""Invalid image shape. Expected either {expected_ndims + 1} or {expected_ndims} dimensions, but got""
                f"" {images.ndim} dimensions.""
            )
        return images
    raise ValueError(
        f""Invalid image type. Expected either PIL.Image.Image, numpy.ndarray, or torch.Tensor, but got {type(images)}.""
    )"
huggingface__transformers__src__transformers__image_utils.py__make_flat_list_of_images,make_flat_list_of_images,huggingface/transformers:src/transformers/image_utils.py,36,False,False,False,0,True,False,True,True,False,True,False,False,True,12,2,14,ml,"def make_flat_list_of_images(
    images: list[ImageInput] | ImageInput,
    expected_ndims: int = 3,
) -> ImageInput:
    """"""
    Ensure that the output is a flat list of images. If the input is a single image, it is converted to a list of length 1.
    If the input is a nested list of images, it is converted to a flat list of images.
    Args:
        images (`Union[list[ImageInput], ImageInput]`):
            The input image.
        expected_ndims (`int`, *optional*, defaults to 3):
            The expected number of dimensions for a single input image.
    Returns:
        list: A list of images or a 4d array of images.
    """"""
    # If the input is a nested list of images, we flatten it
    if (
        isinstance(images, (list, tuple))
        and all(isinstance(images_i, (list, tuple)) for images_i in images)
        and all(is_valid_list_of_images(images_i) or not images_i for images_i in images)
    ):
        return [img for img_list in images for img in img_list]

    if isinstance(images, (list, tuple)) and is_valid_list_of_images(images):
        if is_pil_image(images[0]) or images[0].ndim == expected_ndims:
            return images
        if images[0].ndim == expected_ndims + 1:
            return [img for img_list in images for img in img_list]

    if is_valid_image(images):
        if is_pil_image(images) or images.ndim == expected_ndims:
            return [images]
        if images.ndim == expected_ndims + 1:
            return list(images)

    raise ValueError(f""Could not make a flat list of images from {images}"")"
huggingface__transformers__src__transformers__image_utils.py__make_nested_list_of_images,make_nested_list_of_images,huggingface/transformers:src/transformers/image_utils.py,37,False,False,False,0,True,False,True,True,False,True,False,False,False,13,2,11,ml,"def make_nested_list_of_images(
    images: list[ImageInput] | ImageInput,
    expected_ndims: int = 3,
) -> list[ImageInput]:
    """"""
    Ensure that the output is a nested list of images.
    Args:
        images (`Union[list[ImageInput], ImageInput]`):
            The input image.
        expected_ndims (`int`, *optional*, defaults to 3):
            The expected number of dimensions for a single input image.
    Returns:
        list: A list of list of images or a list of 4d array of images.
    """"""
    # If it's a list of batches, it's already in the right format
    if (
        isinstance(images, (list, tuple))
        and all(isinstance(images_i, (list, tuple)) for images_i in images)
        and all(is_valid_list_of_images(images_i) or not images_i for images_i in images)
    ):
        return images

    # If it's a list of images, it's a single batch, so convert it to a list of lists
    if isinstance(images, (list, tuple)) and is_valid_list_of_images(images):
        if is_pil_image(images[0]) or images[0].ndim == expected_ndims:
            return [images]
        if images[0].ndim == expected_ndims + 1:
            return [list(image) for image in images]

    # If it's a single image, convert it to a list of lists
    if is_valid_image(images):
        if is_pil_image(images) or images.ndim == expected_ndims:
            return [[images]]
        if images.ndim == expected_ndims + 1:
            return [list(images)]

    raise ValueError(""Invalid input type. Must be a single image, a list of images, or a list of batches of images."")"
huggingface__transformers__src__transformers__image_utils.py__infer_channel_dimension_format,infer_channel_dimension_format,huggingface/transformers:src/transformers/image_utils.py,37,False,False,False,0,False,False,False,True,False,True,False,False,True,4,0,9,ml,"def infer_channel_dimension_format(
    image: np.ndarray, num_channels: int | tuple[int, ...] | None = None
) -> ChannelDimension:
    """"""
    Infers the channel dimension format of `image`.

    Args:
        image (`np.ndarray`):
            The image to infer the channel dimension of.
        num_channels (`int` or `tuple[int, ...]`, *optional*, defaults to `(1, 3)`):
            The number of channels of the image.

    Returns:
        The channel dimension of the image.
    """"""
    num_channels = num_channels if num_channels is not None else (1, 3)
    num_channels = (num_channels,) if isinstance(num_channels, int) else num_channels

    if image.ndim == 3:
        first_dim, last_dim = 0, 2
    elif image.ndim == 4:
        first_dim, last_dim = 1, 3
    elif image.ndim == 5:
        first_dim, last_dim = 2, 4
    else:
        raise ValueError(f""Unsupported number of image dimensions: {image.ndim}"")

    if image.shape[first_dim] in num_channels and image.shape[last_dim] in num_channels:
        logger.warning(
            f""The channel dimension is ambiguous. Got image shape {image.shape}. Assuming channels are the first dimension. Use the [input_data_format](https://huggingface.co/docs/transformers/main/internal/image_processing_utils#transformers.image_transforms.rescale.input_data_format) parameter to assign the channel dimension.""
        )
        return ChannelDimension.FIRST
    elif image.shape[first_dim] in num_channels:
        return ChannelDimension.FIRST
    elif image.shape[last_dim] in num_channels:
        return ChannelDimension.LAST
    raise ValueError(""Unable to infer channel dimension format"")"
huggingface__transformers__src__transformers__image_utils.py__get_channel_dimension_axis,get_channel_dimension_axis,huggingface/transformers:src/transformers/image_utils.py,20,False,False,False,0,False,False,False,True,False,True,False,False,True,2,2,4,ml,"def get_channel_dimension_axis(image: np.ndarray, input_data_format: ChannelDimension | str | None = None) -> int:
    """"""
    Returns the channel dimension axis of the image.

    Args:
        image (`np.ndarray`):
            The image to get the channel dimension axis of.
        input_data_format (`ChannelDimension` or `str`, *optional*):
            The channel dimension format of the image. If `None`, will infer the channel dimension from the image.

    Returns:
        The channel dimension axis of the image.
    """"""
    if input_data_format is None:
        input_data_format = infer_channel_dimension_format(image)
    if input_data_format == ChannelDimension.FIRST:
        return image.ndim - 3
    elif input_data_format == ChannelDimension.LAST:
        return image.ndim - 1
    raise ValueError(f""Unsupported data format: {input_data_format}"")"
huggingface__transformers__src__transformers__image_utils.py__get_image_size,get_image_size,huggingface/transformers:src/transformers/image_utils.py,22,False,False,False,0,False,False,False,True,False,True,False,False,True,2,0,4,ml,"def get_image_size(image: np.ndarray, channel_dim: ChannelDimension | None = None) -> tuple[int, int]:
    """"""
    Returns the (height, width) dimensions of the image.

    Args:
        image (`np.ndarray`):
            The image to get the dimensions of.
        channel_dim (`ChannelDimension`, *optional*):
            Which dimension the channel dimension is in. If `None`, will infer the channel dimension from the image.

    Returns:
        A tuple of the image's height and width.
    """"""
    if channel_dim is None:
        channel_dim = infer_channel_dimension_format(image)

    if channel_dim == ChannelDimension.FIRST:
        return image.shape[-2], image.shape[-1]
    elif channel_dim == ChannelDimension.LAST:
        return image.shape[-3], image.shape[-2]
    else:
        raise ValueError(f""Unsupported data format: {channel_dim}"")"
huggingface__transformers__src__transformers__image_utils.py__is_valid_annotation_coco_detection,is_valid_annotation_coco_detection,huggingface/transformers:src/transformers/image_utils.py,13,False,False,False,0,False,False,False,True,False,True,False,False,False,4,0,2,ml,"def is_valid_annotation_coco_detection(annotation: dict[str, list | tuple]) -> bool:
    if (
        isinstance(annotation, dict)
        and ""image_id"" in annotation
        and ""annotations"" in annotation
        and isinstance(annotation[""annotations""], (list, tuple))
        and (
            # an image can have no annotations
            len(annotation[""annotations""]) == 0 or isinstance(annotation[""annotations""][0], dict)
        )
    ):
        return True
    return False"
huggingface__transformers__src__transformers__image_utils.py__is_valid_annotation_coco_panoptic,is_valid_annotation_coco_panoptic,huggingface/transformers:src/transformers/image_utils.py,14,False,False,False,0,False,False,False,True,False,True,False,False,False,4,0,2,ml,"def is_valid_annotation_coco_panoptic(annotation: dict[str, list | tuple]) -> bool:
    if (
        isinstance(annotation, dict)
        and ""image_id"" in annotation
        and ""segments_info"" in annotation
        and ""file_name"" in annotation
        and isinstance(annotation[""segments_info""], (list, tuple))
        and (
            # an image can have no segments
            len(annotation[""segments_info""]) == 0 or isinstance(annotation[""segments_info""][0], dict)
        )
    ):
        return True
    return False"
huggingface__transformers__src__transformers__image_utils.py__load_image,load_image,huggingface/transformers:src/transformers/image_utils.py,40,False,False,False,0,False,False,False,True,False,True,False,False,True,20,0,7,ml,"def load_image(image: Union[str, ""PIL.Image.Image""], timeout: float | None = None) -> ""PIL.Image.Image"":
    """"""
    Loads `image` to a PIL Image.

    Args:
        image (`str` or `PIL.Image.Image`):
            The image to convert to the PIL Image format.
        timeout (`float`, *optional*):
            The timeout value in seconds for the URL request.

    Returns:
        `PIL.Image.Image`: A PIL Image.
    """"""
    requires_backends(load_image, [""vision""])
    if isinstance(image, str):
        if image.startswith(""http://"") or image.startswith(""https://""):
            # We need to actually check for a real protocol, otherwise it's impossible to use a local file
            # like http_huggingface_co.png
            image = PIL.Image.open(BytesIO(httpx.get(image, timeout=timeout, follow_redirects=True).content))
        elif os.path.isfile(image):
            image = PIL.Image.open(image)
        else:
            if image.startswith(""data:image/""):
                image = image.split("","")[1]

            # Try to load as base64
            try:
                b64 = base64.decodebytes(image.encode())
                image = PIL.Image.open(BytesIO(b64))
            except Exception as e:
                raise ValueError(
                    f""Incorrect image source. Must be a valid URL starting with `http://` or `https://`, a valid path to an image file, or a base64 encoded string. Got {image}. Failed with {e}""
                )
    elif not isinstance(image, PIL.Image.Image):
        raise TypeError(
            ""Incorrect format used for image. Should be an url linking to an image, a base64 string, a local path, or a PIL image.""
        )
    image = PIL.ImageOps.exif_transpose(image)
    image = image.convert(""RGB"")
    return image"
huggingface__transformers__src__transformers__image_utils.py__load_images,load_images,huggingface/transformers:src/transformers/image_utils.py,19,False,False,False,0,True,False,False,True,False,True,False,False,False,6,0,6,ml,"def load_images(
    images: Union[list, tuple, str, ""PIL.Image.Image""], timeout: float | None = None
) -> Union[""PIL.Image.Image"", list[""PIL.Image.Image""], list[list[""PIL.Image.Image""]]]:
    """"""Loads images, handling different levels of nesting.

    Args:
      images: A single image, a list of images, or a list of lists of images to load.
      timeout: Timeout for loading images.

    Returns:
      A single image, a list of images, a list of lists of images.
    """"""
    if isinstance(images, (list, tuple)):
        if len(images) and isinstance(images[0], (list, tuple)):
            return [[load_image(image, timeout=timeout) for image in image_group] for image_group in images]
        else:
            return [load_image(image, timeout=timeout) for image in images]
    else:
        return load_image(images, timeout=timeout)"
huggingface__transformers__src__transformers__image_utils.py__validate_preprocess_arguments,validate_preprocess_arguments,huggingface/transformers:src/transformers/image_utils.py,50,False,False,False,0,False,False,False,True,False,True,False,False,False,6,0,7,ml,"def validate_preprocess_arguments(
    do_rescale: bool | None = None,
    rescale_factor: float | None = None,
    do_normalize: bool | None = None,
    image_mean: float | list[float] | None = None,
    image_std: float | list[float] | None = None,
    do_pad: bool | None = None,
    pad_size: dict[str, int] | int | None = None,
    do_center_crop: bool | None = None,
    crop_size: dict[str, int] | None = None,
    do_resize: bool | None = None,
    size: dict[str, int] | None = None,
    resample: Optional[""PILImageResampling""] = None,
    interpolation: Optional[""InterpolationMode""] = None,
):
    """"""
    Checks validity of typically used arguments in an `ImageProcessor` `preprocess` method.
    Raises `ValueError` if arguments incompatibility is caught.
    Many incompatibilities are model-specific. `do_pad` sometimes needs `size_divisor`,
    sometimes `size_divisibility`, and sometimes `size`. New models and processors added should follow
    existing arguments when possible.

    """"""
    if do_rescale and rescale_factor is None:
        raise ValueError(""`rescale_factor` must be specified if `do_rescale` is `True`."")

    if do_pad and pad_size is None:
        # Processors pad images using different args depending on the model, so the below check is pointless
        # but we keep it for BC for now. TODO: remove in v5
        # Usually padding can be called with:
        #   - ""pad_size/size"" if we're padding to specific values
        #   - ""size_divisor"" if we're padding to any value divisible by X
        #   - ""None"" if we're padding to the maximum size image in batch
        raise ValueError(
            ""Depending on the model, `size_divisor` or `pad_size` or `size` must be specified if `do_pad` is `True`.""
        )

    if do_normalize and (image_mean is None or image_std is None):
        raise ValueError(""`image_mean` and `image_std` must both be specified if `do_normalize` is `True`."")

    if do_center_crop and crop_size is None:
        raise ValueError(""`crop_size` must be specified if `do_center_crop` is `True`."")

    if interpolation is not None and resample is not None:
        raise ValueError(
            ""Only one of `interpolation` and `resample` should be specified, depending on image processor type.""
        )

    if do_resize and not (size is not None and (resample is not None or interpolation is not None)):
        raise ValueError(""`size` and `resample/interpolation` must be specified if `do_resize` is `True`."")"
huggingface__transformers__src__transformers__image_utils.py__validate_annotations,validate_annotations,huggingface/transformers:src/transformers/image_utils.py,23,False,False,False,0,False,False,False,True,False,True,False,False,True,5,0,6,ml,"def validate_annotations(
    annotation_format: AnnotationFormat,
    supported_annotation_formats: tuple[AnnotationFormat, ...],
    annotations: list[dict],
) -> None:
    if annotation_format not in supported_annotation_formats:
        raise ValueError(f""Unsupported annotation format: {format} must be one of {supported_annotation_formats}"")

    if annotation_format is AnnotationFormat.COCO_DETECTION:
        if not valid_coco_detection_annotations(annotations):
            raise ValueError(
                ""Invalid COCO detection annotations. Annotations must a dict (single image) or list of dicts ""
                ""(batch of images) with the following keys: `image_id` and `annotations`, with the latter ""
                ""being a list of annotations in the COCO format.""
            )

    if annotation_format is AnnotationFormat.COCO_PANOPTIC:
        if not valid_coco_panoptic_annotations(annotations):
            raise ValueError(
                ""Invalid COCO panoptic annotations. Annotations must a dict (single image) or list of dicts ""
                ""(batch of images) with the following keys: `image_id`, `file_name` and `segments_info`, with ""
                ""the latter being a list of annotations in the COCO format.""
            )"
huggingface__transformers__src__transformers__initialization.py__kaiming_uniform_,kaiming_uniform_,huggingface/transformers:src/transformers/initialization.py,12,False,False,False,0,False,False,False,False,False,True,False,False,False,2,0,2,ml,"def kaiming_uniform_(
    tensor: torch.Tensor,
    a: float = 0,
    mode: str = ""fan_in"",
    nonlinearity: str = ""leaky_relu"",
    generator: torch.Generator | None = None,
) -> torch.Tensor:
    if not getattr(tensor, ""_is_hf_initialized"", False):
        return TORCH_INIT_FUNCTIONS[""kaiming_uniform_""](
            tensor, a=a, mode=mode, nonlinearity=nonlinearity, generator=generator
        )
    return tensor"
huggingface__transformers__src__transformers__initialization.py__kaiming_normal_,kaiming_normal_,huggingface/transformers:src/transformers/initialization.py,12,False,False,False,0,False,False,False,False,False,True,False,False,False,2,0,2,ml,"def kaiming_normal_(
    tensor: torch.Tensor,
    a: float = 0,
    mode: str = ""fan_in"",
    nonlinearity: str = ""leaky_relu"",
    generator: torch.Generator | None = None,
) -> torch.Tensor:
    if not getattr(tensor, ""_is_hf_initialized"", False):
        return TORCH_INIT_FUNCTIONS[""kaiming_normal_""](
            tensor, a=a, mode=mode, nonlinearity=nonlinearity, generator=generator
        )
    return tensor"
huggingface__transformers__src__transformers__initialization.py__trunc_normal_,trunc_normal_,huggingface/transformers:src/transformers/initialization.py,11,False,False,False,0,False,False,False,False,False,True,False,False,False,2,0,2,ml,"def trunc_normal_(
    tensor: torch.Tensor,
    mean: float = 0.0,
    std: float = 1.0,
    a: float = -2.0,
    b: float = 2.0,
    generator: torch.Generator | None = None,
) -> torch.Tensor:
    if not getattr(tensor, ""_is_hf_initialized"", False):
        return TORCH_INIT_FUNCTIONS[""trunc_normal_""](tensor, mean=mean, std=std, a=a, b=b, generator=generator)
    return tensor"
huggingface__transformers__src__transformers__initialization.py__guard_torch_init_functions,guard_torch_init_functions,huggingface/transformers:src/transformers/initialization.py,24,True,False,True,4,False,False,False,False,False,True,False,False,False,9,0,7,ml,"def guard_torch_init_functions():
    """"""
    Guard the `torch.nn.init` primitive functions to behave exactly like the functions in this file, i.e. be
    protected against the `_is_hf_initialized` flag to avoid re-init if the param was already loaded.

    Usually, all models are using the init from `transformers` which are already guarded, but just to make extra sure
    and for remote code, we also use this context manager.
    """"""
    originals = defaultdict(dict)
    try:
        # Replace all torch funcs by the ones in this file
        for module_name in TORCH_MODULES_TO_PATCH:
            if module_name in sys.modules:
                module = sys.modules[module_name]
                for func_name in TORCH_INIT_FUNCTIONS.keys():
                    if hasattr(module, func_name):
                        originals[module][func_name] = getattr(module, func_name)
                        setattr(module, func_name, globals()[func_name])
        yield
    finally:
        # Set back the original functions on all modules
        for module, functions in originals.items():
            for func_name, func in functions.items():
                setattr(module, func_name, func)"
huggingface__transformers__src__transformers__initialization.py__no_init_weights,no_init_weights,huggingface/transformers:src/transformers/initialization.py,34,True,False,True,4,False,False,False,False,False,True,False,False,False,8,0,7,ml,"def no_init_weights():
    """"""
    Disable weight initialization both at the torch-level, and at the transformers-level (`init_weights`).
    This is used to speed-up initializing an empty model with deepspeed, as we do not initialize the model on meta device
    with deepspeed, but we still don't need to run expensive weight initializations as we are loading params afterwards.
    """"""
    from .modeling_utils import PreTrainedModel

    def empty_func(*args, **kwargs):
        pass

    originals = defaultdict(dict)
    try:
        # Replace all torch funcs by empty ones
        for module_name in TORCH_MODULES_TO_PATCH:
            if module_name in sys.modules:
                module = sys.modules[module_name]
                for func_name in TORCH_INIT_FUNCTIONS.keys():
                    if hasattr(module, func_name):
                        originals[module][func_name] = getattr(module, func_name)
                        setattr(module, func_name, empty_func)

        # Also patch our own `init_weights`
        original_init_weights = PreTrainedModel.init_weights
        PreTrainedModel.init_weights = empty_func

        yield
    finally:
        # Set back the original torch functions on all modules
        for module, functions in originals.items():
            for func_name, func in functions.items():
                setattr(module, func_name, func)
        # Set back `init_weights`
        PreTrainedModel.init_weights = original_init_weights"
huggingface__transformers__src__transformers__integrations__accelerate.py__get_module_size_with_ties,get_module_size_with_ties,huggingface/transformers:src/transformers/integrations/accelerate.py,34,True,False,False,2,True,False,False,False,False,True,False,False,False,6,2,5,ml,"def get_module_size_with_ties(
    tied_params,
    module_size,
    module_sizes,
    modules_to_treat,
) -> tuple[int, list[str], list[nn.Module]]:
    """"""
    Calculate the total size of a module, including its tied parameters.

    Args:
        tied_params (`List[str]`): The list of tied parameters.
        module_size (`int`): The size of the module without tied parameters.
        module_sizes (`Dict[str, int]`): A dictionary mapping each layer name to its size.
        modules_to_treat (`List[Tuple[str, nn.Module]]`): The list of named modules to treat.

    Returns:
        `Tuple[int, List[str], List[nn.Module]]`: The total size of the module, the names of the tied modules, and the
        tied modules.
    """"""
    if len(tied_params) < 1:
        return module_size, [], []
    tied_module_names = []
    tied_modules = []

    for tied_param in tied_params:
        tied_module_index = [i for i, (n, _) in enumerate(modules_to_treat) if tied_param.startswith(n + ""."")][0]
        tied_module_names.append(modules_to_treat[tied_module_index][0])
        tied_modules.append(modules_to_treat[tied_module_index][1])

    module_size_with_ties = module_size
    for tied_param, tied_module_name in zip(tied_params, tied_module_names):
        module_size_with_ties += module_sizes[tied_module_name] - module_sizes[tied_param]

    return module_size_with_ties, tied_module_names, tied_modules"
huggingface__transformers__src__transformers__integrations__accelerate.py__check_and_set_device_map,check_and_set_device_map,huggingface/transformers:src/transformers/integrations/accelerate.py,46,False,False,False,0,False,False,False,False,False,True,False,False,True,16,0,12,ml,"def check_and_set_device_map(device_map: ""torch.device | int | str | dict | None"") -> dict | str | None:
    from ..modeling_utils import get_torch_context_manager_or_global_device

    # Potentially detect context manager or global device, and use it (only if no device_map was provided)
    if device_map is None and not is_deepspeed_zero3_enabled():
        device_in_context = get_torch_context_manager_or_global_device()
        if device_in_context == torch.device(""meta""):
            raise RuntimeError(
                ""You are using `from_pretrained` with a meta device context manager or `torch.set_default_device('meta')`.\n""
                ""This is an anti-pattern as `from_pretrained` wants to load existing weights.\nIf you want to initialize an ""
                ""empty model on the meta device, use the context manager or global device with `from_config`, or `ModelClass(config)`""
            )
        device_map = device_in_context

    # change device_map into a map if we passed an int, a str or a torch.device
    if isinstance(device_map, torch.device):
        device_map = {"""": device_map}
    elif isinstance(device_map, str) and device_map not in [""auto"", ""balanced"", ""balanced_low_0"", ""sequential""]:
        try:
            if device_map == ""cuda"":
                # setting to the local rank
                local_rank = int(os.environ.get(""LOCAL_RANK"", 0))
                device_map = f""cuda:{local_rank}""
            device_map = {"""": torch.device(device_map)}
        except RuntimeError:
            raise ValueError(
                ""When passing device_map as a string, the value needs to be a device name (e.g. cpu, cuda:0) or ""
                f""'auto', 'balanced', 'balanced_low_0', 'sequential' but found {device_map}.""
            )
    elif isinstance(device_map, int):
        if device_map < 0:
            raise ValueError(
                ""You can't pass device_map as a negative int. If you want to put the model on the cpu, pass device_map = 'cpu' ""
            )
        else:
            device_map = {"""": device_map}

    if device_map is not None:
        if is_deepspeed_zero3_enabled():
            raise ValueError(""DeepSpeed Zero-3 is not compatible with passing a `device_map`."")
        if not is_accelerate_available():
            raise ValueError(
                ""Using a `device_map`, `tp_plan`, `torch.device` context manager or setting `torch.set_default_device(device)` ""
                ""requires `accelerate`. You can install it with `pip install accelerate`""
            )
    return device_map"
huggingface__transformers__src__transformers__integrations__accelerate.py__compute_module_sizes,compute_module_sizes,huggingface/transformers:src/transformers/integrations/accelerate.py,48,True,False,True,2,False,False,False,False,False,True,False,False,True,16,1,8,ml,"def compute_module_sizes(
    model: ""PreTrainedModel"",
    hf_quantizer: ""HfQuantizer | None"" = None,
    buffers_only: bool = False,
    only_modules: bool = True,
) -> tuple[dict[str, int], dict[str, int]]:
    """"""
    Compute the size of each submodule of a given model (in bytes).
    Returns a tuple of 2 dicts, the fist one containing a mapping of all the modules and the corresponding size
    in bytes, and the 2nd one containing a mapping from all leaf modules (modules containing parameters, the end of
    the model graph) and the corresponding sizes.
    If `only_modules` is set to False, the first mapping will not only contain the size of all modules, but also
    the size of all parameters and buffers.
    """"""
    all_module_sizes = defaultdict(int)
    leaves_module_sizes = defaultdict(int)

    if buffers_only:
        iterator = model.named_buffers()
    else:
        # We need parameters + buffers here, as state_dict does not count non-persistent buffers which are taking space
        def all_tensors():
            yield from model.named_parameters()
            yield from model.named_buffers()

        iterator = all_tensors()

    tied_keys = getattr(model, ""all_tied_weights_keys"", {}).keys()
    for name, param in iterator:
        # Do not count tied keys (the model is usually not tied yet here, so they will appear in the iterator)
        # If the model is already tied, then they simply do not appear in the iterator anyway (remove_duplicates=True by default)
        if name in tied_keys:
            continue
        if hf_quantizer is not None:
            dtype_size = hf_quantizer.param_element_size(model, name, param)
        else:
            dtype_size = param.element_size()
        size = param.numel() * dtype_size
        name_parts = name.split(""."")
        for idx in range(len(name_parts)):
            all_module_sizes[""."".join(name_parts[:idx])] += size
        if ""."" in name:
            leaves_module_sizes[name.rsplit(""."", 1)[0]] += size
        # If we want to also have the full leaves in `all_module_sizes`
        if not only_modules:
            all_module_sizes[name] += size

    return all_module_sizes, leaves_module_sizes"
huggingface__transformers__src__transformers__integrations__accelerate.py__get_balanced_memory,get_balanced_memory,huggingface/transformers:src/transformers/integrations/accelerate.py,103,True,False,False,3,True,False,True,False,False,True,False,False,True,34,5,21,ml,"def get_balanced_memory(
    model: ""PreTrainedModel"",
    max_memory: dict[int | str, int | str] | None = None,
    no_split_module_classes: set[str] | None = None,
    hf_quantizer: ""HfQuantizer | None"" = None,
    low_zero: bool = False,
):
    """"""
    Compute a `max_memory` dictionary for [`infer_auto_device_map`] that will balance the use of each available GPU.

    <Tip>

    All computation is done analyzing sizes and dtypes of the model parameters. As a result, the model can be on the
    meta device (as it would if initialized within the `init_empty_weights` context manager).

    </Tip>

    Args:
        model (`PreTrainedModel`):
            The model to analyze.
        max_memory (`Dict`, *optional*):
            A dictionary device identifier to maximum memory. Will default to the maximum memory available if unset.
            Example: `max_memory={0: ""1GB""}`.
        no_split_module_classes (`set[str]`, *optional*):
            A set of layer class names that should never be split across device (for instance any layer that has a
            residual connection).
        hf_quantizer (`HfQuantizer`, *optional*):
            A quantizer for the model.
        low_zero (`bool`, *optional*):
            Minimizes the number of weights on GPU 0, which is convenient when it's used for other operations (like the
            Transformers generate function).
    """"""
    # Get default / clean up max_memory
    user_not_set_max_memory = max_memory is None
    max_memory = get_max_memory(max_memory)
    # Check the number of accelerators available
    accelerator_max_memory = copy.deepcopy(max_memory)
    _, _ = accelerator_max_memory.pop(""cpu"", None), accelerator_max_memory.pop(""disk"", None)
    num_devices = len([d for d in accelerator_max_memory if accelerator_max_memory[d] > 0])

    if num_devices == 0:
        return max_memory

    if num_devices == 1:
        # We cannot do low_zero on just one GPU, but we will still reserve some memory for the buffer
        low_zero = False
        # If user just asked us to handle memory usage, we should avoid OOM
        if user_not_set_max_memory:
            for key in max_memory.keys():
                if isinstance(key, int):
                    max_memory[key] *= 0.9  # 90% is a good compromise
                    logger.info(
                        f""We will use 90% of the memory on device {key} for storing the model, and 10% for the buffer to avoid OOM. ""
                        ""You can set `max_memory` in to a higher value to use more memory (at your own risk).""
                    )
                    break  # only one device

    module_sizes, leave_modules_sizes = compute_module_sizes(model, hf_quantizer)
    per_gpu = module_sizes[""""] // (num_devices - 1 if low_zero else num_devices)

    # We can't just set the memory to model_size // num_devices as it will end being too small: each GPU will get
    # slightly less layers and some layers will end up offload at the end. So this function computes a buffer size to
    # add which is the biggest of:
    # - the size of no split block (if applicable)
    # - the mean of the layer sizes
    if no_split_module_classes is None:
        no_split_module_classes = []
    elif not isinstance(no_split_module_classes, (list, tuple, set)):
        no_split_module_classes = [no_split_module_classes]

    # Identify the size of the no_split_block modules
    buffer = 0
    if len(no_split_module_classes) > 0:
        no_split_children = {}
        for name, size in module_sizes.items():
            if name == """":
                continue
            submodule = model.get_submodule(name)
            class_name = submodule.__class__.__name__
            if class_name in no_split_module_classes and class_name not in no_split_children:
                no_split_children[class_name] = size

            if set(no_split_children.keys()) == set(no_split_module_classes):
                break
        buffer = max(no_split_children.values()) if len(no_split_children) > 0 else 0

    mean_leaves = int(sum(leave_modules_sizes.values()) / max(len(leave_modules_sizes), 1))
    buffer = int(1.25 * max(buffer, mean_leaves))
    per_gpu += buffer

    # Sorted list of GPUs id (we may have some gpu ids not included in the our max_memory list - let's ignore them)
    gpus_idx_list = sorted(
        device_id for device_id, device_mem in max_memory.items() if isinstance(device_id, int) and device_mem > 0
    )
    # The last device is left with max_memory just in case the buffer is not enough.
    for idx in gpus_idx_list[:-1]:
        max_memory[idx] = min(max_memory[0] if low_zero and idx == 0 else per_gpu, max_memory[idx])

    if low_zero:
        min_zero = max(0, module_sizes[""""] - sum([max_memory[i] for i in range(1, num_devices)]))
        max_memory[0] = min(min_zero, max_memory[0])

    return max_memory"
huggingface__transformers__src__transformers__integrations__accelerate.py__accelerate_dispatch,accelerate_dispatch,huggingface/transformers:src/transformers/integrations/accelerate.py,26,False,False,False,0,False,False,False,False,False,True,False,False,False,8,0,5,ml,"def accelerate_dispatch(model, hf_quantizer, device_map, offload_folder, offload_index, offload_buffers):
    device_map_kwargs = {
        ""device_map"": device_map,
        ""offload_dir"": offload_folder,
        ""offload_index"": offload_index,
        ""offload_buffers"": offload_buffers,
    }
    if ""skip_keys"" in inspect.signature(dispatch_model).parameters:
        device_map_kwargs[""skip_keys""] = model._skip_keys_device_placement
    # For HQQ method we force-set the hooks for single GPU envs
    if (
        ""force_hooks"" in inspect.signature(dispatch_model).parameters
        and hf_quantizer is not None
        and hf_quantizer.quantization_config.quant_method == QuantizationMethod.HQQ
    ):
        device_map_kwargs[""force_hooks""] = True
    if (
        hf_quantizer is not None
        and hf_quantizer.quantization_config.quant_method == QuantizationMethod.FBGEMM_FP8
        and isinstance(device_map, dict)
        and (""cpu"" in device_map.values() or ""disk"" in device_map.values())
    ):
        device_map_kwargs[""offload_buffers""] = True

    if not is_fsdp_enabled() and not is_deepspeed_zero3_enabled():
        dispatch_model(model, **device_map_kwargs)"
huggingface__transformers__src__transformers__integrations__accelerate.py__expand_device_map,expand_device_map,huggingface/transformers:src/transformers/integrations/accelerate.py,17,True,False,False,1,False,False,True,False,False,True,False,False,True,10,0,5,ml,"def expand_device_map(device_map: dict | None, param_names: list[str]):
    """"""
    Expand a device map to return the correspondence parameter name to device.
    """"""
    if device_map is None:
        return dict.fromkeys(param_names, ""cpu"")

    # Here, we first sort by number of submodules, then length of the full string, to make sure to match correctly
    device_map_regex = re.compile(
        ""|"".join(rf""({k})"" for k in sorted(device_map.keys(), key=lambda x: (x.count("".""), len(x)), reverse=True))
    )
    new_device_map = {}
    for param in param_names:
        device_match = device_map_regex.match(param)
        new_device_map[param] = device_map[device_match.group()] if device_match else device_map.get("""", ""cpu"")

    return new_device_map"
huggingface__transformers__src__transformers__integrations__accelerate.py__accelerate_disk_offload,accelerate_disk_offload,huggingface/transformers:src/transformers/integrations/accelerate.py,58,False,False,False,0,True,True,False,False,False,True,False,False,True,17,0,10,ml,"def accelerate_disk_offload(
    model: ""PreTrainedModel"",
    disk_offload_folder: str | None,
    checkpoint_files: list[str] | None,
    device_map: dict,
    sharded_metadata: dict | None,
    dtype: torch.dtype | None,
    weight_mapping=None,
):
    """"""
    Prepare the `disk_offload_index` that will be used for reading offloaded parameters. If reading from a safetensors
    file, parameters which do not need any special WeightConverter operation during loading (i.e. they are used as-is, or only
    renamed) will be mapped to where they already reside on disk. Otherwise, the parameters will be resaved inside
    `disk_offload_folder` during loading.
    """"""
    from ..core_model_loading import WeightRenaming, rename_source_key

    if disk_offload_folder is not None:
        os.makedirs(disk_offload_folder, exist_ok=True)
    is_offloaded_safetensors = checkpoint_files is not None and checkpoint_files[0].endswith("".safetensors"")

    renamings = []
    if weight_mapping is not None:
        renamings = [entry for entry in weight_mapping if isinstance(entry, WeightRenaming)]

    # In this case, the offload index is simply the existing safetensors (except if using custom weight loading
    # Operation, e.g. the MoE models, where we need to resave the weights that were changed at loading time)
    if is_offloaded_safetensors:
        meta_state_dict = model.state_dict()
        param_device_map = expand_device_map(device_map, meta_state_dict.keys())
        str_dtype = str(dtype).replace(""torch."", """") if dtype is not None else ""float32""
        if sharded_metadata is None:
            weight_map = dict.fromkeys(safe_open(checkpoint_files[0], framework=""pt"").keys(), checkpoint_files[0])
        else:
            folder = os.path.sep.join(checkpoint_files[0].split(os.path.sep)[:-1])
            weight_map = {k: os.path.join(folder, v) for k, v in sharded_metadata[""weight_map""].items()}

        # Update the weight names according to the `weight_mapping`
        weight_renaming_map = {
            rename_source_key(k, renamings, [], model.base_model_prefix, meta_state_dict)[0]: k for k in weight_map
        }

        # Prepare the index using existing safetensors files
        disk_offload_index = {
            target_name: {
                ""safetensors_file"": weight_map[source_name],
                ""weight_name"": source_name,
                ""dtype"": str_dtype,
            }
            for target_name, source_name in weight_renaming_map.items()
            # Need to check if it's in the mapping in case of unexpected keys that would result in KeyError (we skip them)
            if target_name in param_device_map and param_device_map[target_name] == ""disk""
        }
    # In this case we will resave every offloaded weight
    else:
        disk_offload_index = {}

    return disk_offload_index"
huggingface__transformers__src__transformers__integrations__accelerate.py__offload_weight,offload_weight,huggingface/transformers:src/transformers/integrations/accelerate.py,18,False,False,False,0,False,False,False,False,False,True,False,False,True,5,0,2,ml,"def offload_weight(weight: torch.Tensor, weight_name: str, offload_folder: str | None, offload_index: dict) -> dict:
    """"""Write `weight` to disk inside `offload_folder`, and update `offload_index` accordingly. Everything is
    saved in `safetensors` format.""""""

    if offload_folder is None:
        raise ValueError(
            ""The current `device_map` had weights offloaded to the disk, which needed to be re-saved. This is either ""
            ""because the weights are not in `safetensors` format, or because the model uses an internal weight format ""
            ""different than the one saved (i.e. most MoE models). Please provide an `offload_folder` for them in ""
            ""`from_pretrained`.""
        )
    # Write the weight to disk
    safetensor_file = os.path.join(offload_folder, f""{weight_name}.safetensors"")
    save_file({weight_name: weight}, safetensor_file)
    # Update the offloading index
    str_dtype = str(weight.dtype).replace(""torch."", """")
    offload_index[weight_name] = {""safetensors_file"": safetensor_file, ""weight_name"": weight_name, ""dtype"": str_dtype}
    return offload_index"
huggingface__transformers__src__transformers__integrations__accelerate.py__load_offloaded_parameter,load_offloaded_parameter,huggingface/transformers:src/transformers/integrations/accelerate.py,24,True,False,False,1,True,False,False,False,False,True,False,False,True,8,1,5,ml,"def load_offloaded_parameter(model: ""PreTrainedModel"", param_name: str) -> torch.Tensor:
    """"""Load `param_name` from disk, if it was offloaded due to the device_map, and thus lives as a meta parameter
    inside `model`.
    This is needed when resaving a model, when some parameters were offloaded (we need to load them from disk, to
    then resave them to disk in the correct shard...).""""""
    # Start from the most inner module, and try to find the hook that was used for offloading the param
    module_parts = param_name.split(""."")
    modules_to_check = [""."".join(module_parts[:-idx]) for idx in range(1, len(module_parts))] + [""""]
    for parent_name in modules_to_check:
        parent = model.get_submodule(parent_name)
        if hasattr(parent, ""_hf_hook""):
            weights_map = parent._hf_hook.weights_map
            truncated_param_name = param_name.replace(f""{parent_name}."" if parent_name != """" else parent_name, """")
            break
    # If we did not break the loop, something is wrong
    else:
        raise ValueError(
            f""{param_name} is on the meta device because it was offloaded, but we could not find ""
            ""the corresponding hook for it""
        )

    # This call loads it from disk
    tensor = weights_map[truncated_param_name]
    return tensor"
huggingface__transformers__src__transformers__integrations__accelerate.py__check_tied_parameters_on_same_device,check_tied_parameters_on_same_device,huggingface/transformers:src/transformers/integrations/accelerate.py,21,True,False,True,2,False,False,False,False,False,True,False,False,True,5,0,4,ml,"def check_tied_parameters_on_same_device(tied_params, device_map):
    """"""
    Check if tied parameters are on the same device

    Args:
        tied_params (`List[List[str]]`):
            A list of lists of parameter names being all tied together.

        device_map (`Dict[str, Union[int, str, torch.device]]`):
            A map that specifies where each submodule should go.

    """"""
    for tie_param in tied_params:
        tie_param_devices = {}
        for param in tie_param:
            tie_param_devices[param] = _get_param_device(param, device_map)
        if len(set(tie_param_devices.values())) > 1:
            logger.warning(
                f""Tied parameters are on different devices: {tie_param_devices}. ""
                ""Please modify your custom device map or set `device_map='auto'`. ""
            )"
huggingface__transformers__src__transformers__integrations__aqlm.py__replace_with_aqlm_linear,replace_with_aqlm_linear,huggingface/transformers:src/transformers/integrations/aqlm.py,44,True,False,False,1,False,False,False,False,False,True,False,False,False,9,0,6,ml,"def replace_with_aqlm_linear(model, modules_to_not_convert: list[str] | None = None, quantization_config=None):
    """"""
    Public method that recursively replaces the Linear layers of the given model with AQLM quantized layers.

    Args:
        model (`torch.nn.Module`):
            The model to convert, can be any `torch.nn.Module` instance.
        modules_to_not_convert (`list[str]`, *optional*, defaults to `None`):
            A list of nn.Linear weights to not convert. If a parameter path is in the list (e.g. `lm_head.weight`), the corresponding module will not be
            converted.
        quantization_config (`AqlmConfig`):
            The quantization config object that contains the quantization parameters.
    """"""
    from aqlm import QuantizedLinear

    has_been_replaced = False
    # we need this to correctly materialize the weights during quantization
    for module_name, module in model.named_modules():
        if not should_convert_module(module_name, modules_to_not_convert):
            continue
        with torch.device(""meta""):
            if isinstance(module, nn.Linear):
                new_module = QuantizedLinear(
                    module.in_features,
                    module.out_features,
                    bias=module.bias is not None,
                    in_group_size=quantization_config.in_group_size,
                    out_group_size=quantization_config.out_group_size,
                    num_codebooks=quantization_config.num_codebooks,
                    nbits_per_codebook=quantization_config.nbits_per_codebook,
                )
                new_module.source_cls = type(module)
                new_module.requires_grad_(False)
                model.set_submodule(module_name, new_module)
                has_been_replaced = True

    if not has_been_replaced:
        logger.warning(
            ""You are loading your model using eetq but no linear modules were found in your model.""
            "" Please double check your model architecture, or submit an issue on github if you think this is""
            "" a bug.""
        )

    return model"
huggingface__transformers__src__transformers__integrations__awq.py__replace_quantization_scales,replace_quantization_scales,huggingface/transformers:src/transformers/integrations/awq.py,15,True,False,False,1,False,False,False,False,False,True,False,False,False,6,0,4,ml,"def replace_quantization_scales(model, model_type):
    from gptqmodel.quantization.awq.modules.act import ScaledActivation

    if model_type not in AWQ_SCALES_MAPPINGS:
        return model
    for name, module in model.named_children():
        act_name = AWQ_SCALES_MAPPINGS[model_type][""act""]
        layer_before_act_name = AWQ_SCALES_MAPPINGS[model_type][""layer_before_act""]
        if name == act_name and hasattr(model, layer_before_act_name):
            layer_before_act = getattr(model, AWQ_SCALES_MAPPINGS[model_type][""layer_before_act""])
            size = layer_before_act.out_features
            scale_like = torch.ones(size)
            model._modules[name] = ScaledActivation(module, scale_like)
        _ = replace_quantization_scales(module, model_type)
    return model"
huggingface__transformers__src__transformers__integrations__awq.py__replace_with_awq_linear,replace_with_awq_linear,huggingface/transformers:src/transformers/integrations/awq.py,64,True,False,False,1,False,False,False,False,False,True,False,False,False,9,0,6,ml,"def replace_with_awq_linear(
    model,
    modules_to_not_convert=None,
    quantization_config=None,
    device_map: str | dict | None = None,
) -> bool:
    """"""
    Public method that replaces the linear layers of the given model with awq quantized layers.

    Args:
        model (`torch.nn.Module`):
            The model to convert, can be any `torch.nn.Module` instance.
        quantization_config (`AwqConfig`):
            The quantization config object that contains the quantization parameters.
        modules_to_not_convert (`list[str]`, *optional*, defaults to `None`):
            A list of nn.Linear weights to not convert. If a parameter path is in the list (e.g. `lm_head.weight`), the corresponding module will not be
            converted.
        device_map (`Union[str, dict]`, *optional*, defaults to `None`):
            The device map that maps the parameters to the device
    """"""
    from gptqmodel.quantization import METHOD
    from gptqmodel.utils.importer import hf_select_quant_linear_v2

    target_cls = hf_select_quant_linear_v2(
        bits=quantization_config.bits,
        group_size=quantization_config.group_size,
        desc_act=False,
        sym=False,
        format=quantization_config.format,
        backend=quantization_config.backend,
        device_map=device_map,
        quant_method=METHOD.AWQ,
        zero_point=quantization_config.zero_point,
        pack=False,
    )

    for module_name, module in model.named_modules():
        if not should_convert_module(module_name, modules_to_not_convert):
            continue
        with torch.device(""meta""):
            if isinstance(module, nn.Linear):
                new_module = target_cls(
                    bits=quantization_config.bits,
                    sym=quantization_config.sym,
                    desc_act=quantization_config.desc_act,
                    group_size=quantization_config.group_size,
                    in_features=module.in_features,
                    out_features=module.out_features,
                    bias=module.bias is not None,
                    dev=module.weight.device,
                    register_buffers=True,
                )
                new_module.requires_grad_(False)
                model.set_submodule(module_name, new_module)
                has_been_replaced = True

    if not has_been_replaced:
        logger.warning(
            ""You are loading your model using eetq but no linear modules were found in your model.""
            "" Please double check your model architecture, or submit an issue on github if you think this is""
            "" a bug.""
        )

    return model"
huggingface__transformers__src__transformers__integrations__bitnet.py__pack_weights,pack_weights,huggingface/transformers:src/transformers/integrations/bitnet.py,36,True,False,False,1,False,False,False,False,False,True,False,False,False,6,9,3,ml,"def pack_weights(quantized_weights: torch.Tensor) -> torch.Tensor:
    """"""
    Packs a tensor of quantized weights into a compact format using 2 bits per value.

    Parameters:
    -----------
    quantized_weights : torch.Tensor
        A tensor containing ternary quantized weights with values in {-1, 0, 1}. These values are adjusted to
        {0, 1, 2} before being packed.

    Returns:
    --------
    torch.Tensor
        A packed tensor where each element stores 4 quantized values (each using 2 bits) in an 8-bit format.
    """"""

    original_shape = quantized_weights.shape

    row_dim = (original_shape[0] + VALUES_PER_ITEM - 1) // VALUES_PER_ITEM

    if len(original_shape) == 1:
        packed_tensor_shape = (row_dim,)
    else:
        packed_tensor_shape = (row_dim, *original_shape[1:])

    quantized_weights += 1
    packed = torch.zeros(packed_tensor_shape, device=quantized_weights.device, dtype=torch.uint8)
    unpacked = quantized_weights.to(torch.uint8)

    it = min(VALUES_PER_ITEM, (original_shape[0] // row_dim) + 1)
    for i in range(it):
        start = i * row_dim
        end = min(start + row_dim, original_shape[0])
        packed[: (end - start)] |= unpacked[start:end] << 2 * i

    return packed"
huggingface__transformers__src__transformers__integrations__bitnet.py__unpack_weights,unpack_weights,huggingface/transformers:src/transformers/integrations/bitnet.py,66,True,False,False,1,False,False,False,False,False,True,False,False,False,4,7,3,ml,"def unpack_weights(packed: torch.Tensor, dtype: torch.dtype) -> torch.Tensor:
    """"""
    Unpacks a tensor of quantized weights that were stored in a packed format using 2 bits per value.

    Parameters:
    -----------
    packed : torch.Tensor
        A tensor containing packed weights where each element represents 4 quantized values (using 2 bits per value).
    dtype : torch.dtype
        The dtype of the returned Tensor
    Returns:
    --------
    torch.Tensor
        A tensor of unpacked weights, where each value is converted from its packed 2-bit representation.

    Example:
    --------
    packed = torch.tensor([[0b10100001, 0b00011000],
                           [0b10010000, 0b00001010]], dtype=torch.uint8)

    # Unpack the values
    unpacked = unpack_weights(packed)

    # Resulting unpacked tensor
    print(unpacked)
    # Output: tensor([[ 0, -1],
                      [-1,  1],
                      [-1,  1],
                      [-1,  1],
                      [ 1,  0],
                      [ 0, -1],
                      [ 1, -1],
                      [ 1, -1]])

    Explanation of the example:
    ---------------------------
    Let's take the first value for example 0b10100001, we will only focus on the first column,
    because every element is unpacked across the first dimension
    - First 2 bits: `01`  0 at [0][0]
    - Second 2 bits: `00`  -1 at [0][2]
    - Third 2 bits: `10`  1 at [0][4]
    - Fourth 2 bits: `10`  1 at [0][6]
    the second value of the same row (0b10010000) will give the values for [0][1], [0][3], [0][5], [0][7]

    We subtract 1 because during the packing process, it's easier to work with values like 0, 1, and 2. To make this possible,
    we add 1 to the original ternary weights (which are typically -1, 0, and 1) when packing them. When unpacking, we reverse
    this by subtracting 1 to restore the original ternary values.
    """"""
    packed_shape = packed.shape

    if len(packed_shape) == 1:
        original_row_dim = packed_shape[0] * VALUES_PER_ITEM
        unpacked_shape = (original_row_dim,)
    else:
        original_row_dim = packed_shape[0] * VALUES_PER_ITEM
        unpacked_shape = (original_row_dim, *packed_shape[1:])

    unpacked = torch.zeros(unpacked_shape, device=packed.device, dtype=torch.uint8)

    for i in range(VALUES_PER_ITEM):
        start = i * packed_shape[0]
        end = start + packed_shape[0]
        mask = 3 << (2 * i)
        unpacked[start:end] = (packed & mask) >> (2 * i)

    return unpacked.to(dtype) - 1"
huggingface__transformers__src__transformers__integrations__bitnet.py__replace_with_bitnet_linear,replace_with_bitnet_linear,huggingface/transformers:src/transformers/integrations/bitnet.py,56,True,False,False,1,False,False,False,False,False,True,False,False,False,10,0,10,ml,"def replace_with_bitnet_linear(model, modules_to_not_convert: list[str] | None = None, quantization_config=None):
    """"""
    Public method that replaces the linear layers of the given model with bitnet quantized layers.

    Args:
        model (`torch.nn.Module`):
            The model to convert, can be any `torch.nn.Module` instance.
        modules_to_not_convert (`list[str]`, *optional*, defaults to `None`):
            A list of nn.Linear weights to not convert. If a parameter path is in the list (e.g. `lm_head.weight`), the corresponding module will not be
            converted.
        quantization_config (`BitNetConfig`):
            The quantization config object that contains the quantization parameters.
    """"""

    has_been_replaced = False
    # we need this to correctly materialize the weights during quantization
    for module_name, module in model.named_modules():
        if not should_convert_module(module_name, modules_to_not_convert):
            continue
        with torch.device(""meta""):
            if isinstance(module, nn.Linear):
                if quantization_config and quantization_config.linear_class == ""autobitlinear"":
                    new_module = AutoBitLinear(
                        in_features=module.in_features,
                        out_features=module.out_features,
                        bias=module.bias is not None,
                        device=module.weight.device,
                        dtype=module.weight.dtype,
                        online_quant=(quantization_config.quantization_mode == ""online""),
                        use_rms_norm=quantization_config.use_rms_norm,
                        rms_norm_eps=quantization_config.rms_norm_eps,
                    )
                    if quantization_config.quantization_mode == ""offline"":
                        new_module.requires_grad_(False)
                else:
                    new_module = BitLinear(
                        in_features=module.in_features,
                        out_features=module.out_features,
                        bias=module.bias is not None,
                        device=module.weight.device,
                        dtype=module.weight.dtype,
                        use_rms_norm=quantization_config.use_rms_norm if quantization_config else False,
                        rms_norm_eps=quantization_config.rms_norm_eps if quantization_config else 1e-6,
                    )
                    new_module.requires_grad_(False)
                model.set_submodule(module_name, new_module)
                has_been_replaced = True

    if not has_been_replaced:
        logger.warning(
            ""You are loading your model using bitnet but no linear modules were found in your model.""
            "" Please double check your model architecture, or submit an issue on github if you think this is""
            "" a bug.""
        )

    return model"
huggingface__transformers__src__transformers__integrations__bitsandbytes.py__replace_with_bnb_linear,replace_with_bnb_linear,huggingface/transformers:src/transformers/integrations/bitsandbytes.py,75,True,False,False,1,False,False,False,False,False,True,False,False,False,14,0,11,ml,"def replace_with_bnb_linear(
    model: torch.nn.Module,
    modules_to_not_convert: list[str] | None = None,
    quantization_config=None,
    pre_quantized=False,
):
    """"""
    A helper function to replace all `torch.nn.Linear` modules by bnb modules from the `bitsandbytes` library.

    Args:
        model (`torch.nn.Module`):
            The model to convert, can be any `torch.nn.Module` instance.
        modules_to_not_convert (`list[str]`, defaults to `None`):
            A list of nn.Linear weights to not convert. If a parameter path is in the list (e.g. `lm_head.weight`), the corresponding module will not be
            converted.
        quantization_config (`BitsAndBytesConfig`):
            The quantization config object that contains the quantization parameters.
        pre_quantized (`book`, defaults to `False`):
            Whether the model is pre-quantized or not
    """"""
    has_been_replaced = False
    # we need this to correctly materialize the weights during quantization
    for module_name, module in model.named_modules():
        if not should_convert_module(module_name, modules_to_not_convert):
            continue
        new_module = None
        with torch.device(""meta""):
            if isinstance(module, (nn.Linear, Conv1D)):
                if isinstance(module, Conv1D):
                    in_features, out_features = module.weight.shape
                else:
                    in_features = module.in_features
                    out_features = module.out_features
                if quantization_config.quantization_method() == ""llm_int8"":
                    new_module = bnb.nn.Linear8bitLt(
                        in_features,
                        out_features,
                        module.bias is not None,
                        has_fp16_weights=quantization_config.llm_int8_has_fp16_weight,
                        threshold=quantization_config.llm_int8_threshold,
                    )
                    if pre_quantized:
                        # this is kind of an edge case when supporting both loading and quantization ...
                        # we need to set the right dtype as we cast the checkpoint with the dtype of the meta model
                        new_module.weight.data = new_module.weight.data.to(dtype=torch.int8)
                else:
                    new_module = bnb.nn.Linear4bit(
                        in_features,
                        out_features,
                        module.bias is not None,
                        quantization_config.bnb_4bit_compute_dtype,
                        compress_statistics=quantization_config.bnb_4bit_use_double_quant,
                        quant_type=quantization_config.bnb_4bit_quant_type,
                        quant_storage=quantization_config.bnb_4bit_quant_storage,
                    )
                    if pre_quantized:
                        # same here
                        new_module.weight.data = new_module.weight.data.to(
                            dtype=quantization_config.bnb_4bit_quant_storage
                        )
                if new_module is not None:
                    # Store the module class in case we need to transpose the weight later
                    new_module.source_cls = type(module)
                    # Force requires grad to False to avoid unexpected errors
                    new_module.requires_grad_(False)
                    model.set_submodule(module_name, new_module)
                    has_been_replaced = True

    if not has_been_replaced:
        logger.warning(
            ""You are loading your model using eetq but no linear modules were found in your model.""
            "" Please double check your model architecture, or submit an issue on github if you think this is""
            "" a bug.""
        )
    return model"
huggingface__transformers__src__transformers__integrations__bitsandbytes.py__dequantize_bnb_weight,dequantize_bnb_weight,huggingface/transformers:src/transformers/integrations/bitsandbytes.py,28,False,False,False,0,False,False,False,False,False,True,False,False,True,7,2,6,ml,"def dequantize_bnb_weight(weight: ""torch.nn.Parameter"", state=None):
    """"""
    Helper function to dequantize 4bit or 8bit bnb weights.

    If the weight is not a bnb quantized weight, it will be returned as is.
    """"""
    if not isinstance(weight, torch.nn.Parameter):
        raise TypeError(f""Input weight should be of type nn.Parameter, got {type(weight)} instead"")

    cls_name = weight.__class__.__name__
    if cls_name not in (""Params4bit"", ""Int8Params""):
        return weight

    if cls_name == ""Params4bit"":
        output_tensor = bnb.functional.dequantize_4bit(weight.data, weight.quant_state)
        return output_tensor

    if state.SCB is None:
        state.SCB = weight.SCB

    if hasattr(bnb.functional, ""int8_vectorwise_dequant""):
        # Use bitsandbytes API if available (requires v0.45.0+)
        dequantized = bnb.functional.int8_vectorwise_dequant(weight.data, state.SCB)
    else:
        # Multiply by (scale/127) to dequantize.
        dequantized = weight.data * state.SCB.view(-1, 1) * 7.874015718698502e-3

    return dequantized"
huggingface__transformers__src__transformers__integrations__bitsandbytes.py__dequantize_and_replace,dequantize_and_replace,huggingface/transformers:src/transformers/integrations/bitsandbytes.py,43,True,False,False,1,False,False,False,False,False,True,False,False,True,20,0,10,ml,"def dequantize_and_replace(model, quantization_config=None, dtype=None):
    """"""
    Converts a quantized model into its dequantized original version. The newly converted model will have
    some performance drop compared to the original model before quantization - use it only for specific usecases
    such as QLoRA adapters merging.

    Returns the converted model.
    """"""
    quant_method = quantization_config.quantization_method()

    target_cls = bnb.nn.Linear8bitLt if quant_method == ""llm_int8"" else bnb.nn.Linear4bit
    for module_name, module in model.named_modules():
        if isinstance(module, target_cls):
            with torch.device(""meta""):
                bias = getattr(module, ""bias"", None)
                new_module = torch.nn.Linear(module.in_features, module.out_features, bias=bias is not None)
            state = module.state if quant_method == ""llm_int8"" else None
            new_module.weight = torch.nn.Parameter(dequantize_bnb_weight(module.weight, state))
            weight = dequantize_bnb_weight(module.weight, state)
            if dtype is None:
                logger.warning_once(
                    f""The modules are dequantized in {weight.dtype}. If you want to change the dtype, please specify `dtype` in `dequantize`. ""
                )
            else:
                logger.warning_once(f""The modules are dequantized in {weight.dtype} and casted to {dtype}."")
                weight = weight.to(dtype)
            new_module.weight = torch.nn.Parameter(weight)
            if bias is not None:
                new_module.bias = bias
            if hasattr(module, ""_hf_hook""):
                old_hook = module._hf_hook
                new_hook = _create_accelerate_new_hook(old_hook)
                remove_hook_from_module(module)
                add_hook_to_module(new_module, new_hook)
            new_module.to(module.weight.device)
            model.set_submodule(module_name, new_module)
            has_been_replaced = True

    if not has_been_replaced:
        logger.warning(
            ""For some reason the model has not been properly dequantized. You might see unexpected behavior.""
        )
    return model"
huggingface__transformers__src__transformers__integrations__bitsandbytes.py__validate_bnb_backend_availability,validate_bnb_backend_availability,huggingface/transformers:src/transformers/integrations/bitsandbytes.py,18,False,False,False,0,False,False,False,False,False,True,False,False,True,7,0,3,ml,"def validate_bnb_backend_availability(raise_exception=False):
    """"""
    Validates if the available devices are supported by bitsandbytes, optionally raising an exception if not.
    """"""
    bnb_supported_devices = getattr(bnb, ""supported_torch_devices"", set())
    available_devices = set(get_available_devices())

    if not available_devices.intersection(bnb_supported_devices):
        if raise_exception:
            err_msg = (
                f""None of the available devices `available_devices = {available_devices or None}` are supported by the bitsandbytes version you have installed: `bnb_supported_devices = {bnb_supported_devices}`. ""
                ""Please check the docs to see if the backend you intend to use is available and how to install it: https://huggingface.co/docs/bitsandbytes/main/en/installation""
            )
            raise RuntimeError(err_msg)

        logger.warning(""No supported devices found for bitsandbytes"")
        return False
    return True"
