{
  "generated_at": "2026-02-22T06:04:01.353287+00:00",
  "n_functions_measured": 136,
  "n_functions_optimized": 58,
  "feature_importance_baseline": {
    "loc": 0.25194745183112294,
    "num_function_calls": 0.14275791006547814,
    "cyclomatic_complexity": 0.11421471973231594,
    "has_pandas": 0.10511503617191875,
    "num_arithmetic_ops": 0.09751163893749522,
    "num_loops": 0.0368769580348844,
    "has_torch": 0.03481328798237996,
    "has_string_ops": 0.03464560662251704,
    "has_while_loop": 0.03451477721874196,
    "has_dict_comp": 0.03003473924450402,
    "has_generator": 0.019393604773058844,
    "has_numpy": 0.014162377103537458,
    "has_list_comp": 0.013807550988139388,
    "has_for_loop": 0.01207960136340181,
    "has_nested_loops": 0.007193168867611601,
    "has_sklearn": 0.002259743540299877,
    "has_tensorflow": 0.0
  },
  "feature_importance_per_loc": {
    "loc": 0.2623071911088704,
    "num_function_calls": 0.14359313320017897,
    "cyclomatic_complexity": 0.10807659471957923,
    "has_pandas": 0.1052507130167937,
    "num_arithmetic_ops": 0.09810881185531357,
    "has_torch": 0.04290340765701007,
    "num_loops": 0.03636871266211533,
    "has_string_ops": 0.035539113993070616,
    "has_while_loop": 0.03477250439750704,
    "has_dict_comp": 0.02998800578213083,
    "has_generator": 0.020201856318275074,
    "has_numpy": 0.013749311247495804,
    "has_list_comp": 0.011705825540592902,
    "has_for_loop": 0.005167662593427526,
    "has_nested_loops": 0.003946863529866234,
    "has_sklearn": 0.002589172697775285,
    "has_tensorflow": 0.0
  },
  "feature_importance_reduction": {
    "num_function_calls": 0.26364468538052915,
    "loc": 0.21214446677792886,
    "cyclomatic_complexity": 0.16078854882902288,
    "num_arithmetic_ops": 0.10348326384770433,
    "has_string_ops": 0.05898529687880943,
    "has_torch": 0.05631861327571141,
    "has_numpy": 0.05592002681602132,
    "num_loops": 0.0159039356406783,
    "has_list_comp": 0.014712710438009681,
    "has_generator": 0.014573946234630503,
    "has_dict_comp": 0.004841277218407466,
    "has_pandas": 0.001994062524678294,
    "has_for_loop": 0.0015109359494552177,
    "has_while_loop": 0.0007414568714891679,
    "has_nested_loops": 0.0004206319552637742,
    "has_tensorflow": 0.0,
    "has_sklearn": 0.0
  },
  "cluster_profiles": [
    {
      "cluster_id": 0,
      "n_members": 45,
      "mean_emissions": 2.7616429056187024e-05,
      "mean_loc": 38.77777777777778,
      "top_features": {
        "loc": 38.77777777777778,
        "has_for_loop": 0.4444444444444444,
        "has_while_loop": 0.044444444444444446,
        "has_nested_loops": 0.17777777777777778,
        "num_loops": 0.8,
        "has_list_comp": 0.26666666666666666,
        "has_dict_comp": 0.044444444444444446,
        "has_generator": 0.2222222222222222,
        "has_numpy": 0.28888888888888886,
        "has_pandas": 0.0,
        "has_torch": 0.5555555555555556,
        "has_tensorflow": 0.0,
        "has_sklearn": 0.022222222222222223,
        "has_string_ops": 0.6444444444444445,
        "num_function_calls": 12.022222222222222,
        "num_arithmetic_ops": 1.9777777777777779,
        "cyclomatic_complexity": 7.288888888888889
      }
    },
    {
      "cluster_id": 1,
      "n_members": 19,
      "mean_emissions": 1.2621296246061262e-05,
      "mean_loc": 65.10526315789474,
      "top_features": {
        "loc": 65.10526315789474,
        "has_for_loop": 0.5263157894736842,
        "has_while_loop": 0.05263157894736842,
        "has_nested_loops": 0.15789473684210525,
        "num_loops": 1.105263157894737,
        "has_list_comp": 0.3157894736842105,
        "has_dict_comp": 0.15789473684210525,
        "has_generator": 0.10526315789473684,
        "has_numpy": 0.2631578947368421,
        "has_pandas": 0.0,
        "has_torch": 0.47368421052631576,
        "has_tensorflow": 0.0,
        "has_sklearn": 0.0,
        "has_string_ops": 0.7368421052631579,
        "num_function_calls": 17.63157894736842,
        "num_arithmetic_ops": 4.368421052631579,
        "cyclomatic_complexity": 10.421052631578947
      }
    },
    {
      "cluster_id": 2,
      "n_members": 64,
      "mean_emissions": 2.695108823761132e-05,
      "mean_loc": 18.140625,
      "top_features": {
        "loc": 18.140625,
        "has_for_loop": 0.25,
        "has_while_loop": 0.015625,
        "has_nested_loops": 0.046875,
        "num_loops": 0.40625,
        "has_list_comp": 0.125,
        "has_dict_comp": 0.015625,
        "has_generator": 0.125,
        "has_numpy": 0.265625,
        "has_pandas": 0.03125,
        "has_torch": 0.453125,
        "has_tensorflow": 0.0,
        "has_sklearn": 0.0,
        "has_string_ops": 0.515625,
        "num_function_calls": 5.96875,
        "num_arithmetic_ops": 1.0625,
        "cyclomatic_complexity": 3.78125
      }
    },
    {
      "cluster_id": 3,
      "n_members": 8,
      "mean_emissions": 1.797477195090938e-05,
      "mean_loc": 101.875,
      "top_features": {
        "loc": 101.875,
        "has_for_loop": 0.5,
        "has_while_loop": 0.0,
        "has_nested_loops": 0.25,
        "num_loops": 1.375,
        "has_list_comp": 0.75,
        "has_dict_comp": 0.0,
        "has_generator": 0.375,
        "has_numpy": 0.125,
        "has_pandas": 0.0,
        "has_torch": 0.375,
        "has_tensorflow": 0.0,
        "has_sklearn": 0.0,
        "has_string_ops": 0.875,
        "num_function_calls": 28.25,
        "num_arithmetic_ops": 3.625,
        "cyclomatic_complexity": 15.125
      }
    }
  ],
  "stats_by_category": {
    "data_processing": {
      "mean": 6.921980043369649e-06,
      "median": 6.052940365698305e-10,
      "p90": 2.2982165484483956e-05,
      "n": 7
    },
    "ml": {
      "mean": 9.110785405949647e-06,
      "median": 3.677590225619496e-10,
      "p90": 1.1367731229943453e-09,
      "n": 93
    },
    "other": {
      "mean": 6.820707933225206e-05,
      "median": 4.0713144074179585e-10,
      "p90": 6.113055102659098e-05,
      "n": 36
    }
  },
  "top20_results": [
    {
      "function_id": "huggingface__transformers__.circleci__create_circleci_config.py__create_circleci_config",
      "function_name": "create_circleci_config",
      "source_file": "huggingface/transformers:.circleci/create_circleci_config.py",
      "baseline_emissions_kg": 0.0011052998822114,
      "optimized_emissions_kg": 3.1027732374495816e-10,
      "reduction_pct": 99.9999719282225,
      "original_source": "def create_circleci_config(folder=None):\n    if folder is None:\n        folder = os.getcwd()\n    os.environ[\"test_preparation_dir\"] = folder\n    jobs = [k for k in ALL_TESTS if os.path.isfile(os.path.join(\"test_preparation\" , f\"{k.job_name}_test_list.txt\") )]\n    print(\"The following jobs will be run \", jobs)\n\n    if len(jobs) == 0:\n        jobs = [EmptyJob()]\n    else:\n        print(\"Full list of job name inputs\", {j.job_name + \"_test_list\":{\"type\":\"string\", \"default\":''} for j in jobs})\n        # Add a job waiting all the test jobs and aggregate their test summary files at the end\n        collection_job = EmptyJob()\n        collection_job.job_name = \"collection_job\"\n        jobs = [collection_job] + jobs\n\n    config = {\n        \"version\": \"2.1\",\n        \"parameters\": {\n            # Only used to accept the parameters from the trigger\n            \"nightly\": {\"type\": \"boolean\", \"default\": False},\n            # Only used to accept the parameters from GitHub Actions trigger\n            \"GHA_Actor\": {\"type\": \"string\", \"default\": \"\"},\n            \"GHA_Action\": {\"type\": \"string\", \"default\": \"\"},\n            \"GHA_Event\": {\"type\": \"string\", \"default\": \"\"},\n            \"GHA_Meta\": {\"type\": \"string\", \"default\": \"\"},\n            \"tests_to_run\": {\"type\": \"string\", \"default\": \"\"},\n            **{j.job_name + \"_test_list\":{\"type\":\"string\", \"default\":''} for j in jobs},\n            **{j.job_name + \"_parallelism\":{\"type\":\"integer\", \"default\":1} for j in jobs},\n        },\n        \"jobs\": {j.job_name: j.to_dict() for j in jobs}\n    }\n    if \"CIRCLE_TOKEN\" in os.environ:\n        # For private forked repo. (e.g. new model addition)\n        config[\"workflows\"] = {\"version\": 2, \"run_tests\": {\"jobs\": [{j.job_name: {\"context\": [\"TRANSFORMERS_CONTEXT\"]}} for j in jobs]}}\n    else:\n        # For public repo. (e.g. `transformers`)\n        config[\"workflows\"] = {\"version\": 2, \"run_tests\": {\"jobs\": [j.job_name for j in jobs]}}\n    with open(os.path.join(folder, \"generated_config.yml\"), \"w\", encoding=\"utf-8\") as f:\n        f.write(yaml.dump(config, sort_keys=False, default_flow_style=False).replace(\"' << pipeline\", \" << pipeline\").replace(\">> '\", \" >>\"))",
      "optimized_source": "def create_circleci_config(folder=None):\n    if folder is None:\n        folder = os.getcwd()\n    os.environ[\"test_preparation_dir\"] = folder\n\n    test_prep_dir = \"test_preparation\"\n    jobs = [k for k in ALL_TESTS if os.path.isfile(os.path.join(test_prep_dir, f\"{k.job_name}_test_list.txt\"))]\n    print(\"The following jobs will be run \", jobs)\n\n    if len(jobs) == 0:\n        jobs = [EmptyJob()]\n    else:\n        print(\"Full list of job name inputs\", {j.job_name + \"_test_list\": {\"type\": \"string\", \"default\": ''} for j in jobs})\n        collection_job = EmptyJob()\n        collection_job.job_name = \"collection_job\"\n        jobs = [collection_job] + jobs\n\n    str_default = {\"type\": \"string\", \"default\": \"\"}\n    int_default = {\"type\": \"integer\", \"default\": 1}\n\n    parameters = {\n        \"nightly\": {\"type\": \"boolean\", \"default\": False},\n        \"GHA_Actor\": str_default,\n        \"GHA_Action\": str_default,\n        \"GHA_Event\": str_default,\n        \"GHA_Meta\": str_default,\n        \"tests_to_run\": str_default,\n    }\n    for j in jobs:\n        jn = j.job_name\n        parameters[jn + \"_test_list\"] = str_default\n        parameters[jn + \"_parallelism\"] = int_default\n\n    config = {\n        \"version\": \"2.1\",\n        \"parameters\": parameters,\n        \"jobs\": {j.job_name: j.to_dict() for j in jobs},\n    }\n\n    has_token = \"CIRCLE_TOKEN\" in os.environ\n    if has_token:\n        workflow_jobs = [{j.job_name: {\"context\": [\"TRANSFORMERS_CONTEXT\"]}} for j in jobs]\n    else:\n        workflow_jobs = [j.job_name for j in jobs]\n    config[\"workflows\"] = {\"version\": 2, \"run_tests\": {\"jobs\": workflow_jobs}}\n\n    yaml_str = yaml.dump(config, sort_keys=False, default_flow_style=False)\n    yaml_str = yaml_str.replace(\"' << pipeline\", \" << pipeline\").replace(\">> '\", \" >>\")\n    with open(os.path.join(folder, \"generated_config.yml\"), \"w\", encoding=\"utf-8\") as f:\n        f.write(yaml_str)"
    },
    {
      "function_id": "huggingface__transformers__.circleci__parse_test_outputs.py__main",
      "function_name": "main",
      "source_file": "huggingface/transformers:.circleci/parse_test_outputs.py",
      "baseline_emissions_kg": 0.0010840679888273,
      "optimized_emissions_kg": 4.3323730239353614e-10,
      "reduction_pct": 99.99996003596576,
      "original_source": "def main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--file\", help=\"file to parse\")\n    parser.add_argument(\"--skip\", action=\"store_true\", help=\"show skipped reasons\")\n    parser.add_argument(\"--fail\", action=\"store_true\", help=\"show failed tests\")\n    parser.add_argument(\"--errors\", action=\"store_true\", help=\"show failed tests\")\n    args = parser.parse_args()\n\n    if args.skip:\n        parse_pytest_output(args.file)\n\n    if args.fail:\n        parse_pytest_failure_output(args.file)\n\n    if args.errors:\n        parse_pytest_errors_output(args.file)",
      "optimized_source": "def main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--file\", help=\"file to parse\")\n    parser.add_argument(\"--skip\", action=\"store_true\", help=\"show skipped reasons\")\n    parser.add_argument(\"--fail\", action=\"store_true\", help=\"show failed tests\")\n    parser.add_argument(\"--errors\", action=\"store_true\", help=\"show failed tests\")\n    args = parser.parse_args()\n\n    if not (args.skip or args.fail or args.errors):\n        return\n\n    file = args.file\n\n    if args.skip:\n        parse_pytest_output(file)\n\n    if args.fail:\n        parse_pytest_failure_output(file)\n\n    if args.errors:\n        parse_pytest_errors_output(file)"
    },
    {
      "function_id": "huggingface__transformers__src__transformers__hf_argparser.py__string_to_bool",
      "function_name": "string_to_bool",
      "source_file": "huggingface/transformers:src/transformers/hf_argparser.py",
      "baseline_emissions_kg": 0.0001086780952323,
      "optimized_emissions_kg": 3.125647096778266e-10,
      "reduction_pct": 99.99971239401188,
      "original_source": "def string_to_bool(v):\n    if isinstance(v, bool):\n        return v\n    if v.lower() in (\"yes\", \"true\", \"t\", \"y\", \"1\"):\n        return True\n    elif v.lower() in (\"no\", \"false\", \"f\", \"n\", \"0\"):\n        return False\n    else:\n        raise ArgumentTypeError(\n            f\"Truthy value expected: got {v} but expected one of yes/no, true/false, t/f, y/n, 1/0 (case insensitive).\"\n        )",
      "optimized_source": "def string_to_bool(v):\n    if isinstance(v, bool):\n        return v\n    low = v.lower()\n    if low in (\"yes\", \"true\", \"t\", \"y\", \"1\"):\n        return True\n    if low in (\"no\", \"false\", \"f\", \"n\", \"0\"):\n        return False\n    raise ArgumentTypeError(\n        f\"Truthy value expected: got {v} but expected one of yes/no, true/false, t/f, y/n, 1/0 (case insensitive).\"\n    )"
    },
    {
      "function_id": "huggingface__transformers__benchmark_v2__framework__benchmark_runner.py__flush_memory",
      "function_name": "flush_memory",
      "source_file": "huggingface/transformers:benchmark_v2/framework/benchmark_runner.py",
      "baseline_emissions_kg": 6.427150001672824e-05,
      "optimized_emissions_kg": 2.7140681241289707e-10,
      "reduction_pct": 99.99957771825405,
      "original_source": "def flush_memory(flush_compile: bool = True) -> None:\n    \"\"\"Flush GPU memory and run garbage collection. If the flush_compile flag is set, we also clear the everything\n    related to compile cache.\"\"\"\n    gc.collect()\n    # If needed, flush everything related to torch.compile\n    if flush_compile:\n        # Dynamo resets\n        torch._dynamo.reset()\n        torch._dynamo.reset_code_caches()\n        if hasattr(torch._inductor, \"codecache\"):\n            # Clear FX graph cache\n            if hasattr(torch._inductor.codecache, \"FxGraphCache\"):\n                torch._inductor.codecache.FxGraphCache.clear()\n            # Clear PyCodeCache\n            if hasattr(torch._inductor.codecache, \"PyCodeCache\"):\n                torch._inductor.codecache.PyCodeCache.cache_clear()\n            # Clear TritonFuture cache (for async compilation)\n            if hasattr(torch._inductor.codecache, \"TritonFuture\"):\n                if hasattr(torch._inductor.codecache.TritonFuture, \"_compile_cache\"):\n                    torch._inductor.codecache.TritonFuture._compile_cache.clear()\n    # Clear device cache\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.synchronize()\n    elif is_torch_xpu_available():\n        torch.xpu.empty_cache()\n        torch.xpu.synchronize()\n    gc.collect()",
      "optimized_source": "def flush_memory(flush_compile: bool = True) -> None:\n    \"\"\"Flush GPU memory and run garbage collection. If the flush_compile flag is set, we also clear the everything\n    related to compile cache.\"\"\"\n    gc.collect()\n    if flush_compile:\n        torch._dynamo.reset()\n        torch._dynamo.reset_code_caches()\n        codecache = getattr(torch._inductor, \"codecache\", None)\n        if codecache is not None:\n            fx_graph_cache = getattr(codecache, \"FxGraphCache\", None)\n            if fx_graph_cache is not None:\n                fx_graph_cache.clear()\n            py_code_cache = getattr(codecache, \"PyCodeCache\", None)\n            if py_code_cache is not None:\n                py_code_cache.cache_clear()\n            triton_future = getattr(codecache, \"TritonFuture\", None)\n            if triton_future is not None:\n                compile_cache = getattr(triton_future, \"_compile_cache\", None)\n                if compile_cache is not None:\n                    compile_cache.clear()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.synchronize()\n    elif is_torch_xpu_available():\n        torch.xpu.empty_cache()\n        torch.xpu.synchronize()\n    gc.collect()"
    },
    {
      "function_id": "huggingface__transformers__src__transformers__cli__add_new_model_like.py__create_test_files",
      "function_name": "create_test_files",
      "source_file": "huggingface/transformers:src/transformers/cli/add_new_model_like.py",
      "baseline_emissions_kg": 7.314099908550083e-05,
      "optimized_emissions_kg": 3.267299779865425e-10,
      "reduction_pct": 99.99955328751032,
      "original_source": "def create_test_files(\n    repo_path: Path, old_model_infos: ModelInfos, new_lowercase_name, filenames_to_add: list[tuple[str, bool]]\n):\n    \"\"\"\n    Create the test files for the new model. It basically copies over the old test files and adjust the class names.\n\n    Args:\n        old_model_infos (`ModelInfos`):\n            The structure containing the class information of the old model.\n        new_lowercase_name (`str`):\n            The new lowercase model name.\n        filenames_to_add (`list[tuple[str, bool]]`):\n            A list of tuples of all potential filenames to add for a new model, along a boolean flag describing if we\n            should add this file or not. For example, [(`modeling_xxx.px`, True), (`configuration_xxx.py`, True), (`tokenization_xxx.py`, False),...]\n    \"\"\"\n    new_cased_name = \"\".join(x.title() for x in new_lowercase_name.replace(\"-\", \"_\").split(\"_\"))\n    old_lowercase_name = old_model_infos.lowercase_name\n    old_cased_name = old_model_infos.camelcase_name\n    filenames_to_add = [\n        (\"test_\" + filename.replace(old_lowercase_name, new_lowercase_name), to_add)\n        for filename, to_add in filenames_to_add[1:]\n    ]\n    # fast tokenizer/image processor have the same test files as normal ones\n    corrected_filenames_to_add = []\n    for file, to_add in filenames_to_add:\n        if re.search(rf\"test_(?:tokenization)|(?:image_processing)_{new_lowercase_name}_fast.py\", file):\n            previous_file, previous_to_add = corrected_filenames_to_add[-1]\n            corrected_filenames_to_add[-1] = (previous_file, previous_to_add or to_add)\n        else:\n            corrected_filenames_to_add.append((file, to_add))\n\n    test_files = {}\n    for new_file, to_add in corrected_filenames_to_add:\n        if to_add:\n            original_test_file = new_file.replace(new_lowercase_name, old_lowercase_name)\n            original_test_path = repo_path / \"tests\" / \"models\" / old_lowercase_name / original_test_file\n            # Sometimes, tests may not exist\n            if not original_test_path.is_file():\n                continue\n            with open(original_test_path, \"r\") as f:\n                test_code = f.read()\n            # Remove old copyright and add new one\n            test_lines = test_code.split(\"\\n\")\n            idx = 0\n            while test_lines[idx].startswith(\"#\"):\n                idx += 1\n            test_code = COPYRIGHT + \"\\n\".join(test_lines[idx:])\n            test_files[new_file] = test_code.replace(old_cased_name, new_cased_name)\n\n    return test_files",
      "optimized_source": "def create_test_files(\n    repo_path, old_model_infos, new_lowercase_name, filenames_to_add: list[tuple[str, bool]]\n):\n    \"\"\"\n    Create the test files for the new model. It basically copies over the old test files and adjust the class names.\n\n    Args:\n        old_model_infos (`ModelInfos`):\n            The structure containing the class information of the old model.\n        new_lowercase_name (`str`):\n            The new lowercase model name.\n        filenames_to_add (`list[tuple[str, bool]]`):\n            A list of tuples of all potential filenames to add for a new model, along a boolean flag describing if we\n            should add this file or not. For example, [(`modeling_xxx.px`, True), (`configuration_xxx.py`, True), (`tokenization_xxx.py`, False),...]\n    \"\"\"\n    new_cased_name = \"\".join(x.title() for x in new_lowercase_name.replace(\"-\", \"_\").split(\"_\"))\n    old_lowercase_name = old_model_infos.lowercase_name\n    old_cased_name = old_model_infos.camelcase_name\n\n    fast_suffix = f\"_{new_lowercase_name}_fast.py\"\n    corrected_filenames_to_add = []\n    for filename, to_add in filenames_to_add[1:]:\n        new_file = \"test_\" + filename.replace(old_lowercase_name, new_lowercase_name)\n        if new_file.endswith(fast_suffix):\n            rest = new_file[5:]  # after \"test_\"\n            if rest.startswith(\"tokenization_\") or rest.startswith(\"image_processing_\"):\n                if corrected_filenames_to_add:\n                    prev_file, prev_to_add = corrected_filenames_to_add[-1]\n                    corrected_filenames_to_add[-1] = (prev_file, prev_to_add or to_add)\n                    continue\n        corrected_filenames_to_add.append((new_file, to_add))\n\n    test_files = {}\n    tests_dir = repo_path / \"tests\" / \"models\" / old_lowercase_name\n\n    for new_file, to_add in corrected_filenames_to_add:\n        if not to_add:\n            continue\n        original_test_file = new_file.replace(new_lowercase_name, old_lowercase_name)\n        original_test_path = tests_dir / original_test_file\n        if not original_test_path.is_file():\n            continue\n        with open(original_test_path, \"r\") as f:\n            test_code = f.read()\n        # Remove old copyright: skip leading lines starting with '#'\n        idx = 0\n        lines = test_code.split(\"\\n\")\n        n = len(lines)\n        while idx < n and lines[idx].startswith(\"#\"):\n            idx += 1\n        test_code = COPYRIGHT + \"\\n\".join(lines[idx:])\n        test_files[new_file] = test_code.replace(old_cased_name, new_cased_name)\n\n    return test_files"
    },
    {
      "function_id": "huggingface__transformers__src__transformers__image_transforms.py__center_crop",
      "function_name": "center_crop",
      "source_file": "huggingface/transformers:src/transformers/image_transforms.py",
      "baseline_emissions_kg": 6.502934888472358e-05,
      "optimized_emissions_kg": 2.9852324859821235e-10,
      "reduction_pct": 99.99954094073873,
      "original_source": "def center_crop(\n    image: np.ndarray,\n    size: tuple[int, int],\n    data_format: str | ChannelDimension | None = None,\n    input_data_format: str | ChannelDimension | None = None,\n) -> np.ndarray:\n    \"\"\"\n    Crops the `image` to the specified `size` using a center crop. Note that if the image is too small to be cropped to\n    the size given, it will be padded (so the returned result will always be of size `size`).\n\n    Args:\n        image (`np.ndarray`):\n            The image to crop.\n        size (`tuple[int, int]`):\n            The target size for the cropped image.\n        data_format (`str` or `ChannelDimension`, *optional*):\n            The channel dimension format for the output image. Can be one of:\n                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n            If unset, will use the inferred format of the input image.\n        input_data_format (`str` or `ChannelDimension`, *optional*):\n            The channel dimension format for the input image. Can be one of:\n                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n            If unset, will use the inferred format of the input image.\n    Returns:\n        `np.ndarray`: The cropped image.\n    \"\"\"\n    requires_backends(center_crop, [\"vision\"])\n\n    if not isinstance(image, np.ndarray):\n        raise TypeError(f\"Input image must be of type np.ndarray, got {type(image)}\")\n\n    if not isinstance(size, Iterable) or len(size) != 2:\n        raise ValueError(\"size must have 2 elements representing the height and width of the output image\")\n\n    if input_data_format is None:\n        input_data_format = infer_channel_dimension_format(image)\n    output_data_format = data_format if data_format is not None else input_data_format\n\n    # We perform the crop in (C, H, W) format and then convert to the output format\n    image = to_channel_dimension_format(image, ChannelDimension.FIRST, input_data_format)\n\n    orig_height, orig_width = get_image_size(image, ChannelDimension.FIRST)\n    crop_height, crop_width = size\n    crop_height, crop_width = int(crop_height), int(crop_width)\n\n    # In case size is odd, (image_shape[0] + size[0]) // 2 won't give the proper result.\n    top = (orig_height - crop_height) // 2\n    bottom = top + crop_height\n    # In case size is odd, (image_shape[1] + size[1]) // 2 won't give the proper result.\n    left = (orig_width - crop_width) // 2\n    right = left + crop_width\n\n    # Check if cropped area is within image boundaries\n    if top >= 0 and bottom <= orig_height and left >= 0 and right <= orig_width:\n        image = image[..., top:bottom, left:right]\n        image = to_channel_dimension_format(image, output_data_format, ChannelDimension.FIRST)\n        return image\n\n    # Otherwise, we may need to pad if the image is too small. Oh joy...\n    new_height = max(crop_height, orig_height)\n    new_width = max(crop_width, orig_width)\n    new_shape = image.shape[:-2] + (new_height, new_width)\n    new_image = np.zeros_like(image, shape=new_shape)\n\n    # If the image is too small, pad it with zeros\n    top_pad = ceil((new_height - orig_height) / 2)\n    bottom_pad = top_pad + orig_height\n    left_pad = ceil((new_width - orig_width) / 2)\n    right_pad = left_pad + orig_width\n    new_image[..., top_pad:bottom_pad, left_pad:right_pad] = image\n\n    top += top_pad\n    bottom += top_pad\n    left += left_pad\n    right += left_pad\n\n    new_image = new_image[..., max(0, top) : min(new_height, bottom), max(0, left) : min(new_width, right)]\n    new_image = to_channel_dimension_format(new_image, output_data_format, ChannelDimension.FIRST)\n\n    return new_image",
      "optimized_source": "def center_crop(\n    image,\n    size,\n    data_format=None,\n    input_data_format=None,\n):\n    import numpy as np\n    requires_backends(center_crop, [\"vision\"])\n\n    if not isinstance(image, np.ndarray):\n        raise TypeError(f\"Input image must be of type np.ndarray, got {type(image)}\")\n\n    if not isinstance(size, Iterable) or len(size) != 2:\n        raise ValueError(\"size must have 2 elements representing the height and width of the output image\")\n\n    if input_data_format is None:\n        input_data_format = infer_channel_dimension_format(image)\n    output_data_format = data_format if data_format is not None else input_data_format\n\n    orig_height, orig_width = get_image_size(image, input_data_format)\n    crop_height, crop_width = int(size[0]), int(size[1])\n\n    top = (orig_height - crop_height) // 2\n    bottom = top + crop_height\n    left = (orig_width - crop_width) // 2\n    right = left + crop_width\n\n    is_channels_first = (input_data_format == ChannelDimension.FIRST or input_data_format == \"channels_first\")\n\n    # Fast path: crop fits entirely within image\n    if top >= 0 and bottom <= orig_height and left >= 0 and right <= orig_width:\n        if is_channels_first:\n            cropped = image[..., top:bottom, left:right]\n        else:\n            cropped = image[..., top:bottom, left:right, :]\n        if output_data_format != input_data_format:\n            cropped = to_channel_dimension_format(cropped, output_data_format, input_data_format)\n        return cropped\n\n    # Slow path: need padding\n    new_height = max(crop_height, orig_height)\n    new_width = max(crop_width, orig_width)\n\n    if is_channels_first:\n        new_shape = image.shape[:-2] + (new_height, new_width)\n    else:\n        new_shape = image.shape[:-3] + (new_height, new_width) + (image.shape[-1],)\n\n    new_image = np.zeros_like(image, shape=new_shape)\n\n    top_pad = ceil((new_height - orig_height) / 2)\n    bottom_pad = top_pad + orig_height\n    left_pad = ceil((new_width - orig_width) / 2)\n    right_pad = left_pad + orig_width\n\n    if is_channels_first:\n        new_image[..., top_pad:bottom_pad, left_pad:right_pad] = image\n    else:\n        new_image[..., top_pad:bottom_pad, left_pad:right_pad, :] = image\n\n    top += top_pad\n    bottom += top_pad\n    left += left_pad\n    right += left_pad\n\n    st = max(0, top)\n    sb = min(new_height, bottom)\n    sl = max(0, left)\n    sr = min(new_width, right)\n\n    if is_channels_first:\n        new_image = new_image[..., st:sb, sl:sr]\n    else:\n        new_image = new_image[..., st:sb, sl:sr, :]\n\n    if output_data_format != input_data_format:\n        new_image = to_channel_dimension_format(new_image, output_data_format, input_data_format)\n\n    return new_image"
    },
    {
      "function_id": "huggingface__transformers__src__transformers__dynamic_module_utils.py__custom_object_save",
      "function_name": "custom_object_save",
      "source_file": "huggingface/transformers:src/transformers/dynamic_module_utils.py",
      "baseline_emissions_kg": 0.0001084671968678,
      "optimized_emissions_kg": 5.061661086132512e-10,
      "reduction_pct": 99.9995333463727,
      "original_source": "def custom_object_save(obj: Any, folder: str | os.PathLike, config: dict | None = None) -> list[str]:\n    \"\"\"\n    Save the modeling files corresponding to a custom model/configuration/tokenizer etc. in a given folder. Optionally\n    adds the proper fields in a config.\n\n    Args:\n        obj (`Any`): The object for which to save the module files.\n        folder (`str` or `os.PathLike`): The folder where to save.\n        config (`PreTrainedConfig` or dictionary, `optional`):\n            A config in which to register the auto_map corresponding to this custom object.\n\n    Returns:\n        `list[str]`: The list of files saved.\n    \"\"\"\n    if obj.__module__ == \"__main__\":\n        logger.warning(\n            f\"We can't save the code defining {obj} in {folder} as it's been defined in __main__. You should put \"\n            \"this code in a separate module so we can include it in the saved folder and make it easier to share via \"\n            \"the Hub.\"\n        )\n        return\n\n    def _set_auto_map_in_config(_config):\n        module_name = obj.__class__.__module__\n        last_module = module_name.split(\".\")[-1]\n        full_name = f\"{last_module}.{obj.__class__.__name__}\"\n        # Special handling for tokenizers\n        if \"Tokenizer\" in full_name:\n            slow_tokenizer_class = None\n            fast_tokenizer_class = None\n            if obj.__class__.__name__.endswith(\"Fast\"):\n                # Fast tokenizer: we have the fast tokenizer class and we may have the slow one has an attribute.\n                fast_tokenizer_class = f\"{last_module}.{obj.__class__.__name__}\"\n                if getattr(obj, \"slow_tokenizer_class\", None) is not None:\n                    slow_tokenizer = getattr(obj, \"slow_tokenizer_class\")\n                    slow_tok_module_name = slow_tokenizer.__module__\n                    last_slow_tok_module = slow_tok_module_name.split(\".\")[-1]\n                    slow_tokenizer_class = f\"{last_slow_tok_module}.{slow_tokenizer.__name__}\"\n            else:\n                # Slow tokenizer: no way to have the fast class\n                slow_tokenizer_class = f\"{last_module}.{obj.__class__.__name__}\"\n\n            full_name = (slow_tokenizer_class, fast_tokenizer_class)\n\n        if isinstance(_config, dict):\n            auto_map = _config.get(\"auto_map\", {})\n            auto_map[obj._auto_class] = full_name\n            _config[\"auto_map\"] = auto_map\n        elif getattr(_config, \"auto_map\", None) is not None:\n            _config.auto_map[obj._auto_class] = full_name\n        else:\n            _config.auto_map = {obj._auto_class: full_name}\n\n    # Add object class to the config auto_map\n    if isinstance(config, (list, tuple)):\n        for cfg in config:\n            _set_auto_map_in_config(cfg)\n    elif config is not None:\n        _set_auto_map_in_config(config)\n\n    result = []\n    # Copy module file to the output folder.\n    object_file = sys.modules[obj.__module__].__file__\n    dest_file = Path(folder) / (Path(object_file).name)\n    shutil.copy(object_file, dest_file)\n    result.append(dest_file)\n\n    # Gather all relative imports recursively and make sure they are copied as well.\n    for needed_file in get_relative_import_files(object_file):\n        dest_file = Path(folder) / (Path(needed_file).name)\n        shutil.copy(needed_file, dest_file)\n        result.append(dest_file)\n\n    return result",
      "optimized_source": "def custom_object_save(obj: Any, folder: str | os.PathLike, config: dict | None = None) -> list[str]:\n    \"\"\"\n    Save the modeling files corresponding to a custom model/configuration/tokenizer etc. in a given folder. Optionally\n    adds the proper fields in a config.\n\n    Args:\n        obj (`Any`): The object for which to save the module files.\n        folder (`str` or `os.PathLike`): The folder where to save.\n        config (`PreTrainedConfig` or dictionary, `optional`):\n            A config in which to register the auto_map corresponding to this custom object.\n\n    Returns:\n        `list[str]`: The list of files saved.\n    \"\"\"\n    if obj.__module__ == \"__main__\":\n        logger.warning(\n            f\"We can't save the code defining {obj} in {folder} as it's been defined in __main__. You should put \"\n            \"this code in a separate module so we can include it in the saved folder and make it easier to share via \"\n            \"the Hub.\"\n        )\n        return\n\n    def _set_auto_map_in_config(_config):\n        module_name = obj.__class__.__module__\n        last_module = module_name.split(\".\")[-1]\n        full_name = f\"{last_module}.{obj.__class__.__name__}\"\n        # Special handling for tokenizers\n        if \"Tokenizer\" in full_name:\n            slow_tokenizer_class = None\n            fast_tokenizer_class = None\n            if obj.__class__.__name__.endswith(\"Fast\"):\n                # Fast tokenizer: we have the fast tokenizer class and we may have the slow one has an attribute.\n                fast_tokenizer_class = f\"{last_module}.{obj.__class__.__name__}\"\n                if getattr(obj, \"slow_tokenizer_class\", None) is not None:\n                    slow_tokenizer = getattr(obj, \"slow_tokenizer_class\")\n                    slow_tok_module_name = slow_tokenizer.__module__\n                    last_slow_tok_module = slow_tok_module_name.split(\".\")[-1]\n                    slow_tokenizer_class = f\"{last_slow_tok_module}.{slow_tokenizer.__name__}\"\n            else:\n                # Slow tokenizer: no way to have the fast class\n                slow_tokenizer_class = f\"{last_module}.{obj.__class__.__name__}\"\n\n            full_name = (slow_tokenizer_class, fast_tokenizer_class)\n\n        if isinstance(_config, dict):\n            auto_map = _config.get(\"auto_map\", {})\n            auto_map[obj._auto_class] = full_name\n            _config[\"auto_map\"] = auto_map\n        elif getattr(_config, \"auto_map\", None) is not None:\n            _config.auto_map[obj._auto_class] = full_name\n        else:\n            _config.auto_map = {obj._auto_class: full_name}\n\n    # Add object class to the config auto_map\n    if isinstance(config, (list, tuple)):\n        for cfg in config:\n            _set_auto_map_in_config(cfg)\n    elif config is not None:\n        _set_auto_map_in_config(config)\n\n    result = []\n    # Copy module file to the output folder.\n    object_file = sys.modules[obj.__module__].__file__\n    dest_file = Path(folder) / (Path(object_file).name)\n    shutil.copy(object_file, dest_file)\n    result.append(dest_file)\n\n    # Gather all relative imports recursively and make sure they are copied as well.\n    for needed_file in get_relative_import_files(object_file):\n        dest_file = Path(folder) / (Path(needed_file).name)\n        shutil.copy(needed_file, dest_file)\n        result.append(dest_file)\n\n    return result"
    },
    {
      "function_id": "huggingface__transformers__benchmark__benchmark.py__summarize",
      "function_name": "summarize",
      "source_file": "huggingface/transformers:benchmark/benchmark.py",
      "baseline_emissions_kg": 0.0001437952221965,
      "optimized_emissions_kg": 1.2849819105147612e-09,
      "reduction_pct": 99.9991063806635,
      "original_source": "def summarize(run_dir, metrics, expand_metrics=False):\n    \"\"\"Produce a summary for each optimum-benchmark launched job's output directory found in `run_dir`.\n\n    Each summary's format is as follows (for `expand_metrics=False`):\n    ```\n    {\n        \"model\": \"google/gemma-2b\",\n        \"commit\": \"3cd6ed22e4d49219f300f5055e71e3929aba20d7\",\n        \"config\": \"benchmark.input_shapes.batch_size=1,benchmark.input_shapes.sequence_length=5\",\n        \"metrics\": {\n            \"decode.latency.mean\": 1.624666809082031,\n            \"per_token.latency.mean\": 0.012843788806628804,\n            \"per_token.throughput.value\": 77.85864553330948\n        }\n    }\n    ```\n    \"\"\"\n    reports = glob.glob(os.path.join(run_dir, \"**/benchmark_report.json\"), recursive=True)\n    report_dirs = [str(Path(report).parent) for report in reports]\n\n    summaries = []\n    for report_dir in report_dirs:\n        commit = re.search(r\"/commit=([^/]+)\", report_dir).groups()[0]\n\n        if not os.path.isfile(os.path.join(report_dir, \"benchmark.json\")):\n            continue\n        benchmark = Benchmark.from_json(os.path.join(report_dir, \"benchmark.json\"))\n        report = benchmark.report\n\n        model = benchmark.config.backend[\"model\"]\n\n        # This looks like `benchmark.input_shapes.batch_size=1,benchmark.input_shapes.sequence_length=5`.\n        # (we rely on the usage of hydra's `${hydra.job.override_dirname}`.)\n        benchmark_name = re.sub(f\"backend.model={model},*\", \"\", report_dir)\n        benchmark_name = str(Path(benchmark_name).parts[-1])\n        if benchmark_name.startswith(\"commit=\"):\n            benchmark_name = benchmark.config.name\n\n        metrics_values = {}\n        # post-processing of report: show a few selected/important metric\n        for metric in metrics:\n            keys = metric.split(\".\")\n            value = report.to_dict()\n            current = metrics_values\n            for key in keys:\n                # Avoid KeyError when a user's specified metric has typo.\n                # TODO: Give warnings.\n                if key not in value:\n                    continue\n                value = value[key]\n\n                if expand_metrics:\n                    if isinstance(value, dict):\n                        if key not in current:\n                            current[key] = {}\n                            current = current[key]\n                    else:\n                        current[key] = value\n\n            if not expand_metrics:\n                metrics_values[metric] = value\n\n        # show some config information\n        print(f\"model: {model}\")\n        print(f\"commit: {commit}\")\n        print(f\"config: {benchmark_name}\")\n        if len(metrics_values) > 0:\n            print(\"metrics:\")\n            if expand_metrics:\n                print(metrics_values)\n            else:\n                for metric, value in metrics_values.items():\n                    print(f\"  - {metric}: {value}\")\n        print(\"-\" * 80)\n\n        summary = {\n            \"model\": model,\n            \"commit\": commit,\n            \"config\": benchmark_name,\n            \"metrics\": metrics_values,\n        }\n        summaries.append(summary)\n\n        with open(os.path.join(report_dir, \"summary.json\"), \"w\") as fp:\n            json.dump(summary, fp, indent=4)\n\n    return summaries",
      "optimized_source": "def summarize(run_dir, metrics, expand_metrics=False):\n    \"\"\"Produce a summary for each optimum-benchmark launched job's output directory found in `run_dir`.\n\n    Each summary's format is as follows (for `expand_metrics=False`):\n    ```\n    {\n        \"model\": \"google/gemma-2b\",\n        \"commit\": \"3cd6ed22e4d49219f300f5055e71e3929aba20d7\",\n        \"config\": \"benchmark.input_shapes.batch_size=1,benchmark.input_shapes.sequence_length=5\",\n        \"metrics\": {\n            \"decode.latency.mean\": 1.624666809082031,\n            \"per_token.latency.mean\": 0.012843788806628804,\n            \"per_token.throughput.value\": 77.85864553330948\n        }\n    }\n    ```\n    \"\"\"\n    reports = glob.glob(os.path.join(run_dir, \"**/benchmark_report.json\"), recursive=True)\n    report_dirs = [str(Path(report).parent) for report in reports]\n\n    summaries = []\n    for report_dir in report_dirs:\n        commit = re.search(r\"/commit=([^/]+)\", report_dir).groups()[0]\n\n        if not os.path.isfile(os.path.join(report_dir, \"benchmark.json\")):\n            continue\n        benchmark = Benchmark.from_json(os.path.join(report_dir, \"benchmark.json\"))\n        report = benchmark.report\n\n        model = benchmark.config.backend[\"model\"]\n\n        # This looks like `benchmark.input_shapes.batch_size=1,benchmark.input_shapes.sequence_length=5`.\n        # (we rely on the usage of hydra's `${hydra.job.override_dirname}`.)\n        benchmark_name = re.sub(f\"backend.model={model},*\", \"\", report_dir)\n        benchmark_name = str(Path(benchmark_name).parts[-1])\n        if benchmark_name.startswith(\"commit=\"):\n            benchmark_name = benchmark.config.name\n\n        metrics_values = {}\n        # post-processing of report: show a few selected/important metric\n        for metric in metrics:\n            keys = metric.split(\".\")\n            value = report.to_dict()\n            current = metrics_values\n            for key in keys:\n                # Avoid KeyError when a user's specified metric has typo.\n                # TODO: Give warnings.\n                if key not in value:\n                    continue\n                value = value[key]\n\n                if expand_metrics:\n                    if isinstance(value, dict):\n                        if key not in current:\n                            current[key] = {}\n                            current = current[key]\n                    else:\n                        current[key] = value\n\n            if not expand_metrics:\n                metrics_values[metric] = value\n\n        # show some config information\n        print(f\"model: {model}\")\n        print(f\"commit: {commit}\")\n        print(f\"config: {benchmark_name}\")\n        if len(metrics_values) > 0:\n            print(\"metrics:\")\n            if expand_metrics:\n                print(metrics_values)\n            else:\n                for metric, value in metrics_values.items():\n                    print(f\"  - {metric}: {value}\")\n        print(\"-\" * 80)\n\n        summary = {\n            \"model\": model,\n            \"commit\": commit,\n            \"config\": benchmark_name,\n            \"metrics\": metrics_values,\n        }\n        summaries.append(summary)\n\n        with open(os.path.join(report_dir, \"summary.json\"), \"w\") as fp:\n            json.dump(summary, fp, indent=4)\n\n    return summaries"
    },
    {
      "function_id": "huggingface__transformers__benchmark__benchmarks_entrypoint.py__create_global_metrics_recorder",
      "function_name": "create_global_metrics_recorder",
      "source_file": "huggingface/transformers:benchmark/benchmarks_entrypoint.py",
      "baseline_emissions_kg": 3.043810981108116e-05,
      "optimized_emissions_kg": 2.8271495255467016e-10,
      "reduction_pct": 99.9990711809823,
      "original_source": "def create_global_metrics_recorder(\n    repository: str, branch: str, commit_id: str, commit_msg: str, generate_csv: bool = False\n) -> MetricsRecorder:\n    \"\"\"\n    Create a global metrics recorder that will be used across all benchmarks.\n    \"\"\"\n    connection = create_database_connection()\n    recorder = MetricsRecorder(connection, logger, repository, branch, commit_id, commit_msg, generate_csv)\n\n    # Log the storage mode\n    storage_modes = []\n    if connection is not None:\n        storage_modes.append(\"database\")\n    if generate_csv:\n        storage_modes.append(\"CSV\")\n\n    if not storage_modes:\n        logger.warning(\"Running benchmarks with NO data storage (no database connection, CSV disabled)\")\n        logger.warning(\"Use --csv flag to enable CSV output when database is unavailable\")\n    else:\n        logger.info(f\"Running benchmarks with: {' + '.join(storage_modes)} storage\")\n\n    return recorder",
      "optimized_source": "def create_global_metrics_recorder(\n    repository: str, branch: str, commit_id: str, commit_msg: str, generate_csv: bool = False\n):\n    \"\"\"\n    Create a global metrics recorder that will be used across all benchmarks.\n    \"\"\"\n    connection = create_database_connection()\n    recorder = MetricsRecorder(connection, logger, repository, branch, commit_id, commit_msg, generate_csv)\n\n    has_db = connection is not None\n    if has_db:\n        if generate_csv:\n            logger.info(\"Running benchmarks with: database + CSV storage\")\n        else:\n            logger.info(\"Running benchmarks with: database storage\")\n    elif generate_csv:\n        logger.info(\"Running benchmarks with: CSV storage\")\n    else:\n        logger.warning(\"Running benchmarks with NO data storage (no database connection, CSV disabled)\")\n        logger.warning(\"Use --csv flag to enable CSV output when database is unavailable\")\n\n    return recorder"
    },
    {
      "function_id": "huggingface__transformers__benchmark__benchmarks_entrypoint.py__create_database_connection",
      "function_name": "create_database_connection",
      "source_file": "huggingface/transformers:benchmark/benchmarks_entrypoint.py",
      "baseline_emissions_kg": 1.8011535933419145e-05,
      "optimized_emissions_kg": 2.827369741338771e-10,
      "reduction_pct": 99.99843024506528,
      "original_source": "def create_database_connection():\n    \"\"\"\n    Try to create a database connection. Returns None if connection fails.\n    \"\"\"\n    if not PSYCOPG2_AVAILABLE:\n        logger.warning(\"psycopg2 not available - running in CSV-only mode\")\n        return None\n\n    try:\n        import psycopg2\n\n        conn = psycopg2.connect(\"dbname=metrics\")\n        logger.info(\"Successfully connected to database\")\n        return conn\n    except Exception as e:\n        logger.warning(f\"Failed to connect to database: {e}. Running in CSV-only mode\")\n        return None",
      "optimized_source": "def create_database_connection():\n    \"\"\"\n    Try to create a database connection. Returns None if connection fails.\n    \"\"\"\n    if not PSYCOPG2_AVAILABLE:\n        logger.warning(\"psycopg2 not available - running in CSV-only mode\")\n        return None\n\n    try:\n        conn = psycopg2.connect(\"dbname=metrics\")\n        logger.info(\"Successfully connected to database\")\n        return conn\n    except Exception as e:\n        logger.warning(\"Failed to connect to database: %s. Running in CSV-only mode\", e)\n        return None"
    },
    {
      "function_id": "huggingface__transformers__src__transformers__image_transforms.py__get_resize_output_image_size",
      "function_name": "get_resize_output_image_size",
      "source_file": "huggingface/transformers:src/transformers/image_transforms.py",
      "baseline_emissions_kg": 6.630019531417081e-05,
      "optimized_emissions_kg": 1.6122019298316447e-09,
      "reduction_pct": 99.9975683300446,
      "original_source": "def get_resize_output_image_size(\n    input_image: np.ndarray,\n    size: int | tuple[int, int] | list[int] | tuple[int, ...],\n    default_to_square: bool = True,\n    max_size: int | None = None,\n    input_data_format: str | ChannelDimension | None = None,\n) -> tuple:\n    \"\"\"\n    Find the target (height, width) dimension of the output image after resizing given the input image and the desired\n    size.\n\n    Args:\n        input_image (`np.ndarray`):\n            The image to resize.\n        size (`int` or `tuple[int, int]` or list[int] or `tuple[int]`):\n            The size to use for resizing the image. If `size` is a sequence like (h, w), output size will be matched to\n            this.\n\n            If `size` is an int and `default_to_square` is `True`, then image will be resized to (size, size). If\n            `size` is an int and `default_to_square` is `False`, then smaller edge of the image will be matched to this\n            number. i.e, if height > width, then image will be rescaled to (size * height / width, size).\n        default_to_square (`bool`, *optional*, defaults to `True`):\n            How to convert `size` when it is a single int. If set to `True`, the `size` will be converted to a square\n            (`size`,`size`). If set to `False`, will replicate\n            [`torchvision.transforms.Resize`](https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.Resize)\n            with support for resizing only the smallest edge and providing an optional `max_size`.\n        max_size (`int`, *optional*):\n            The maximum allowed for the longer edge of the resized image: if the longer edge of the image is greater\n            than `max_size` after being resized according to `size`, then the image is resized again so that the longer\n            edge is equal to `max_size`. As a result, `size` might be overruled, i.e the smaller edge may be shorter\n            than `size`. Only used if `default_to_square` is `False`.\n        input_data_format (`ChannelDimension`, *optional*):\n            The channel dimension format of the input image. If unset, will use the inferred format from the input.\n\n    Returns:\n        `tuple`: The target (height, width) dimension of the output image after resizing.\n    \"\"\"\n    if isinstance(size, (tuple, list)):\n        if len(size) == 2:\n            return tuple(size)\n        elif len(size) == 1:\n            # Perform same logic as if size was an int\n            size = size[0]\n        else:\n            raise ValueError(\"size must have 1 or 2 elements if it is a list or tuple\")\n\n    if default_to_square:\n        return (size, size)\n\n    height, width = get_image_size(input_image, input_data_format)\n    short, long = (width, height) if width <= height else (height, width)\n    requested_new_short = size\n\n    new_short, new_long = requested_new_short, int(requested_new_short * long / short)\n\n    if max_size is not None:\n        if max_size <= requested_new_short:\n            raise ValueError(\n                f\"max_size = {max_size} must be strictly greater than the requested \"\n                f\"size for the smaller edge size = {size}\"\n            )\n        if new_long > max_size:\n            new_short, new_long = int(max_size * new_short / new_long), max_size\n\n    return (new_long, new_short) if width <= height else (new_short, new_long)",
      "optimized_source": "def get_resize_output_image_size(\n    input_image,\n    size,\n    default_to_square: bool = True,\n    max_size=None,\n    input_data_format=None,\n) -> tuple:\n    if isinstance(size, (tuple, list)):\n        if len(size) == 2:\n            return (size[0], size[1])\n        elif len(size) == 1:\n            size = size[0]\n        else:\n            raise ValueError(\"size must have 1 or 2 elements if it is a list or tuple\")\n\n    if default_to_square:\n        return (size, size)\n\n    height, width = get_image_size(input_image, input_data_format)\n\n    if width <= height:\n        short, long = width, height\n    else:\n        short, long = height, width\n\n    new_short = size\n    new_long = (size * long + (short >> 1)) // short\n\n    if max_size is not None:\n        if max_size <= size:\n            raise ValueError(\n                f\"max_size = {max_size} must be strictly greater than the requested \"\n                f\"size for the smaller edge size = {size}\"\n            )\n        if new_long > max_size:\n            new_short = (max_size * new_short) // new_long\n            new_long = max_size\n\n    if width <= height:\n        return (new_long, new_short)\n    return (new_short, new_long)"
    },
    {
      "function_id": "huggingface__transformers__src__transformers__data__metrics__squad_metrics.py__normalize_answer",
      "function_name": "normalize_answer",
      "source_file": "huggingface/transformers:src/transformers/data/metrics/squad_metrics.py",
      "baseline_emissions_kg": 1.3583006820881964e-05,
      "optimized_emissions_kg": 2.77304197641206e-10,
      "reduction_pct": 95.91937728030764,
      "original_source": "def normalize_answer(s):\n    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n\n    def remove_articles(text):\n        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n        return re.sub(regex, \" \", text)\n\n    def white_space_fix(text):\n        return \" \".join(text.split())\n\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return \"\".join(ch for ch in text if ch not in exclude)\n\n    def lower(text):\n        return text.lower()\n\n    return white_space_fix(remove_articles(remove_punc(lower(s))))",
      "optimized_source": "import string\n\n_TRANSLATE_TABLE = str.maketrans(\"\", \"\", string.punctuation)\n_ARTICLES = frozenset((\"a\", \"an\", \"the\"))\n\ndef normalize_answer(s):\n    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n    tokens = s.lower().translate(_TRANSLATE_TABLE).split()\n    result = [w for w in tokens if w not in _ARTICLES]\n    return \" \".join(result)"
    },
    {
      "function_id": "huggingface__transformers__src__transformers__data__metrics__squad_metrics.py__normalize_answer",
      "function_name": "normalize_answer",
      "source_file": "huggingface/transformers:src/transformers/data/metrics/squad_metrics.py",
      "baseline_emissions_kg": 6.795634310983587e-09,
      "optimized_emissions_kg": 2.77304197641206e-10,
      "reduction_pct": 95.91937728030764,
      "original_source": "def normalize_answer(s):\n    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n\n    def remove_articles(text):\n        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n        return re.sub(regex, \" \", text)\n\n    def white_space_fix(text):\n        return \" \".join(text.split())\n\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return \"\".join(ch for ch in text if ch not in exclude)\n\n    def lower(text):\n        return text.lower()\n\n    return white_space_fix(remove_articles(remove_punc(lower(s))))",
      "optimized_source": "import string\n\n_TRANSLATE_TABLE = str.maketrans(\"\", \"\", string.punctuation)\n_ARTICLES = frozenset((\"a\", \"an\", \"the\"))\n\ndef normalize_answer(s):\n    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n    tokens = s.lower().translate(_TRANSLATE_TABLE).split()\n    result = [w for w in tokens if w not in _ARTICLES]\n    return \" \".join(result)"
    },
    {
      "function_id": "huggingface__transformers__src__transformers__image_transforms.py__rescale",
      "function_name": "rescale",
      "source_file": "huggingface/transformers:src/transformers/image_transforms.py",
      "baseline_emissions_kg": 6.578019481396769e-09,
      "optimized_emissions_kg": 3.2704878949734846e-10,
      "reduction_pct": 95.02815717675703,
      "original_source": "def rescale(\n    image: np.ndarray,\n    scale: float,\n    data_format: ChannelDimension | None = None,\n    dtype: np.dtype = np.float32,\n    input_data_format: str | ChannelDimension | None = None,\n) -> np.ndarray:\n    \"\"\"\n    Rescales `image` by `scale`.\n\n    Args:\n        image (`np.ndarray`):\n            The image to rescale.\n        scale (`float`):\n            The scale to use for rescaling the image.\n        data_format (`ChannelDimension`, *optional*):\n            The channel dimension format of the image. If not provided, it will be the same as the input image.\n        dtype (`np.dtype`, *optional*, defaults to `np.float32`):\n            The dtype of the output image. Defaults to `np.float32`. Used for backwards compatibility with feature\n            extractors.\n        input_data_format (`ChannelDimension`, *optional*):\n            The channel dimension format of the input image. If not provided, it will be inferred from the input image.\n\n    Returns:\n        `np.ndarray`: The rescaled image.\n    \"\"\"\n    if not isinstance(image, np.ndarray):\n        raise TypeError(f\"Input image must be of type np.ndarray, got {type(image)}\")\n\n    rescaled_image = image.astype(np.float64) * scale  # Numpy type promotion has changed, so always upcast first\n    if data_format is not None:\n        rescaled_image = to_channel_dimension_format(rescaled_image, data_format, input_data_format)\n\n    rescaled_image = rescaled_image.astype(dtype)  # Finally downcast to the desired dtype at the end\n\n    return rescaled_image",
      "optimized_source": "def rescale(\n    image: np.ndarray,\n    scale: float,\n    data_format: ChannelDimension | None = None,\n    dtype: np.dtype = np.float32,\n    input_data_format: str | ChannelDimension | None = None,\n) -> np.ndarray:\n    if not isinstance(image, np.ndarray):\n        raise TypeError(f\"Input image must be of type np.ndarray, got {type(image)}\")\n\n    # If the target dtype is float32 or lower precision, we can skip the float64 upcast\n    # and multiply directly in float32, saving memory and compute.\n    # Only use float64 when the target dtype actually requires it.\n    if np.dtype(dtype).itemsize <= 4:\n        rescaled_image = image.astype(np.float32, copy=False)\n        rescaled_image = np.multiply(rescaled_image, np.float32(scale), out=rescaled_image)\n    else:\n        rescaled_image = image.astype(np.float64, copy=False)\n        rescaled_image = np.multiply(rescaled_image, scale, out=rescaled_image)\n\n    if data_format is not None:\n        rescaled_image = to_channel_dimension_format(rescaled_image, data_format, input_data_format)\n\n    if rescaled_image.dtype != dtype:\n        rescaled_image = rescaled_image.astype(dtype, copy=False)\n\n    return rescaled_image"
    },
    {
      "function_id": "huggingface__transformers__.github__scripts__assign_reviewers.py__get_file_owners",
      "function_name": "get_file_owners",
      "source_file": "huggingface/transformers:.github/scripts/assign_reviewers.py",
      "baseline_emissions_kg": 3.565132941339107e-09,
      "optimized_emissions_kg": 3.0684874783619307e-10,
      "reduction_pct": 91.39306295487155,
      "original_source": "def get_file_owners(file_path, codeowners_lines):\n    # Process lines in reverse (last matching pattern takes precedence)\n    for line in reversed(codeowners_lines):\n        # Skip comments and empty lines, strip inline comments\n        line = line.split('#')[0].strip()\n        if not line:\n            continue\n\n        # Split into pattern and owners\n        parts = line.split()\n        pattern = parts[0]\n        # Can be empty, e.g. for dummy files with explicitly no owner!\n        owners = [owner.removeprefix(\"@\") for owner in parts[1:]]\n\n        # Check if file matches pattern\n        file_regex = pattern_to_regex(pattern)\n        if re.search(file_regex, file_path) is not None:\n            return owners  # Remember, can still be empty!\n    return []  # Should never happen, but just in case",
      "optimized_source": "def get_file_owners(file_path, codeowners_lines):\n    # Process lines in reverse (last matching pattern takes precedence)\n    for line in reversed(codeowners_lines):\n        # Skip comments and empty lines, strip inline comments\n        hash_idx = line.find('#')\n        if hash_idx != -1:\n            line = line[:hash_idx]\n        stripped = line.strip()\n        if not stripped:\n            continue\n\n        # Split into pattern and owners\n        parts = stripped.split()\n        pattern = parts[0]\n\n        # Check if file matches pattern\n        file_regex = pattern_to_regex(pattern)\n        if re.search(file_regex, file_path) is not None:\n            # Can be empty, e.g. for dummy files with explicitly no owner!\n            owners = parts[1:]\n            for i in range(len(owners)):\n                o = owners[i]\n                if o.startswith('@'):\n                    owners[i] = o[1:]\n            return owners\n    return []  # Should never happen, but just in case"
    },
    {
      "function_id": "huggingface__transformers__.github__scripts__assign_reviewers.py__pr_author_is_in_hf",
      "function_name": "pr_author_is_in_hf",
      "source_file": "huggingface/transformers:.github/scripts/assign_reviewers.py",
      "baseline_emissions_kg": 3.3385601573390886e-09,
      "optimized_emissions_kg": 3.6427621782349887e-10,
      "reduction_pct": 89.08882270631794,
      "original_source": "def pr_author_is_in_hf(pr_author, codeowners_lines):\n    # Check if the PR author is in the codeowners file\n    for line in codeowners_lines:\n        line = line.split('#')[0].strip()\n        if not line:\n            continue\n\n        # Split into pattern and owners\n        parts = line.split()\n        owners = [owner.removeprefix(\"@\") for owner in parts[1:]]\n\n        if pr_author in owners:\n            return True\n    return False",
      "optimized_source": "def pr_author_is_in_hf(pr_author, codeowners_lines):\n    at_author = \"@\" + pr_author\n    for line in codeowners_lines:\n        hash_pos = line.find('#')\n        if hash_pos != -1:\n            line = line[:hash_pos]\n        \n        parts = line.split()\n        for i in range(1, len(parts)):\n            owner = parts[i]\n            if owner == at_author or owner == pr_author:\n                return True\n    return False"
    },
    {
      "function_id": "huggingface__transformers__benchmark_v2__framework__data_classes.py__add_unit_to_duration",
      "function_name": "add_unit_to_duration",
      "source_file": "huggingface/transformers:benchmark_v2/framework/data_classes.py",
      "baseline_emissions_kg": 2.5797493217542067e-09,
      "optimized_emissions_kg": 2.9960845686146063e-10,
      "reduction_pct": 88.38614068681179,
      "original_source": "def add_unit_to_duration(stats: dict[str, float]) -> dict[str, str]:\n    for key in list(stats.keys()):\n        value = stats[key]\n        if value > 3600:\n            stats[key] = f\"{(value / 3600):.2f}hr\"\n        elif value > 60:\n            stats[key] = f\"{(value / 60):.2f}min\"\n        elif value > 1:\n            stats[key] = f\"{value:.2f}s\"\n        elif value > 1e-3:\n            stats[key] = f\"{(value * 1e3):.2f}ms\"\n        elif value > 1e-6:\n            stats[key] = f\"{(value * 1e6):.2f}us\"\n        else:\n            stats[key] = f\"{(value * 1e9):.2f}ns\"\n    return stats",
      "optimized_source": "def add_unit_to_duration(stats: dict[str, float]) -> dict[str, str]:\n    for key, value in stats.items():\n        if value > 3600:\n            stats[key] = f\"{value / 3600:.2f}hr\"\n        elif value > 60:\n            stats[key] = f\"{value / 60:.2f}min\"\n        elif value > 1:\n            stats[key] = f\"{value:.2f}s\"\n        elif value > 1e-3:\n            stats[key] = f\"{value * 1e3:.2f}ms\"\n        elif value > 1e-6:\n            stats[key] = f\"{value * 1e6:.2f}us\"\n        else:\n            stats[key] = f\"{value * 1e9:.2f}ns\"\n    return stats"
    },
    {
      "function_id": "huggingface__transformers__.github__scripts__assign_reviewers.py__pattern_to_regex",
      "function_name": "pattern_to_regex",
      "source_file": "huggingface/transformers:.github/scripts/assign_reviewers.py",
      "baseline_emissions_kg": 2.4578152178597524e-09,
      "optimized_emissions_kg": 3.3214073290000667e-10,
      "reduction_pct": 86.48634240334663,
      "original_source": "def pattern_to_regex(pattern):\n    if pattern.startswith(\"/\"):\n        start_anchor = True\n        pattern = re.escape(pattern[1:])\n    else:\n        start_anchor = False\n        pattern = re.escape(pattern)\n    # Replace `*` with \"any number of non-slash characters\"\n    pattern = pattern.replace(r\"\\*\", \"[^/]*\")\n    if start_anchor:\n        pattern = r\"^\\/?\" + pattern  # Allow an optional leading slash after the start of the string\n    return pattern",
      "optimized_source": "def pattern_to_regex(pattern):\n    if pattern.startswith(\"/\"):\n        start_anchor = True\n        pattern = re.escape(pattern[1:])\n    else:\n        start_anchor = False\n        pattern = re.escape(pattern)\n    \n    if r\"\\*\" in pattern:\n        pattern = pattern.replace(r\"\\*\", \"[^/]*\")\n    \n    if start_anchor:\n        return r\"^\\/?\" + pattern\n    return pattern"
    },
    {
      "function_id": "huggingface__transformers__src__transformers__hf_argparser.py__make_choice_type_function",
      "function_name": "make_choice_type_function",
      "source_file": "huggingface/transformers:src/transformers/hf_argparser.py",
      "baseline_emissions_kg": 1.5451348984701327e-09,
      "optimized_emissions_kg": 2.927377636282472e-10,
      "reduction_pct": 81.05422614439087,
      "original_source": "def make_choice_type_function(choices: list) -> Callable[[str], Any]:\n    \"\"\"\n    Creates a mapping function from each choices string representation to the actual value. Used to support multiple\n    value types for a single argument.\n\n    Args:\n        choices (list): List of choices.\n\n    Returns:\n        Callable[[str], Any]: Mapping function from string representation to actual value for each choice.\n    \"\"\"\n    str_to_choice = {str(choice): choice for choice in choices}\n    return lambda arg: str_to_choice.get(arg, arg)",
      "optimized_source": "def make_choice_type_function(choices: list) -> Callable[[str], Any]:\n    str_to_choice = {str(choice): choice for choice in choices}\n    return str_to_choice.get"
    },
    {
      "function_id": "huggingface__transformers__src__transformers__cli__chat.py__load_generation_config",
      "function_name": "load_generation_config",
      "source_file": "huggingface/transformers:src/transformers/cli/chat.py",
      "baseline_emissions_kg": 1.317760056024417e-09,
      "optimized_emissions_kg": 3.0517518677515915e-10,
      "reduction_pct": 76.84136915669991,
      "original_source": "def load_generation_config(generation_config: str | None) -> GenerationConfig:\n    if generation_config is None:\n        return GenerationConfig()\n\n    if \".json\" in generation_config:  # is a local file\n        dirname = os.path.dirname(generation_config)\n        filename = os.path.basename(generation_config)\n        return GenerationConfig.from_pretrained(dirname, filename)\n    else:\n        return GenerationConfig.from_pretrained(generation_config)",
      "optimized_source": "import os\nfrom transformers import GenerationConfig\n\ndef load_generation_config(generation_config: str | None) -> GenerationConfig:\n    if generation_config is None:\n        return GenerationConfig()\n\n    if generation_config.endswith(\".json\"):\n        dirname = os.path.dirname(generation_config)\n        filename = os.path.basename(generation_config)\n        return GenerationConfig.from_pretrained(dirname, filename)\n    else:\n        return GenerationConfig.from_pretrained(generation_config)"
    },
    {
      "function_id": "huggingface__transformers__src__transformers__image_utils.py__make_flat_list_of_images",
      "function_name": "make_flat_list_of_images",
      "source_file": "huggingface/transformers:src/transformers/image_utils.py",
      "baseline_emissions_kg": 1.389578252969659e-09,
      "optimized_emissions_kg": 3.5806345578748715e-10,
      "reduction_pct": 74.23222081791565,
      "original_source": "def make_flat_list_of_images(\n    images: list[ImageInput] | ImageInput,\n    expected_ndims: int = 3,\n) -> ImageInput:\n    \"\"\"\n    Ensure that the output is a flat list of images. If the input is a single image, it is converted to a list of length 1.\n    If the input is a nested list of images, it is converted to a flat list of images.\n    Args:\n        images (`Union[list[ImageInput], ImageInput]`):\n            The input image.\n        expected_ndims (`int`, *optional*, defaults to 3):\n            The expected number of dimensions for a single input image.\n    Returns:\n        list: A list of images or a 4d array of images.\n    \"\"\"\n    # If the input is a nested list of images, we flatten it\n    if (\n        isinstance(images, (list, tuple))\n        and all(isinstance(images_i, (list, tuple)) for images_i in images)\n        and all(is_valid_list_of_images(images_i) or not images_i for images_i in images)\n    ):\n        return [img for img_list in images for img in img_list]\n\n    if isinstance(images, (list, tuple)) and is_valid_list_of_images(images):\n        if is_pil_image(images[0]) or images[0].ndim == expected_ndims:\n            return images\n        if images[0].ndim == expected_ndims + 1:\n            return [img for img_list in images for img in img_list]\n\n    if is_valid_image(images):\n        if is_pil_image(images) or images.ndim == expected_ndims:\n            return [images]\n        if images.ndim == expected_ndims + 1:\n            return list(images)\n\n    raise ValueError(f\"Could not make a flat list of images from {images}\")",
      "optimized_source": "def make_flat_list_of_images(\n    images: list[ImageInput] | ImageInput,\n    expected_ndims: int = 3,\n) -> ImageInput:\n    \"\"\"\n    Ensure that the output is a flat list of images. If the input is a single image, it is converted to a list of length 1.\n    If the input is a nested list of images, it is converted to a flat list of images.\n    Args:\n        images (`Union[list[ImageInput], ImageInput]`):\n            The input image.\n        expected_ndims (`int`, *optional*, defaults to 3):\n            The expected number of dimensions for a single input image.\n    Returns:\n        list: A list of images or a 4d array of images.\n    \"\"\"\n    # If the input is a nested list of images, we flatten it\n    if (\n        isinstance(images, (list, tuple))\n        and all(isinstance(images_i, (list, tuple)) for images_i in images)\n        and all(is_valid_list_of_images(images_i) or not images_i for images_i in images)\n    ):\n        return [img for img_list in images for img in img_list]\n\n    if isinstance(images, (list, tuple)) and is_valid_list_of_images(images):\n        if is_pil_image(images[0]) or images[0].ndim == expected_ndims:\n            return images\n        if images[0].ndim == expected_ndims + 1:\n            return [img for img_list in images for img in img_list]\n\n    if is_valid_image(images):\n        if is_pil_image(images) or images.ndim == expected_ndims:\n            return [images]\n        if images.ndim == expected_ndims + 1:\n            return list(images)\n\n    raise ValueError(f\"Could not make a flat list of images from {images}\")"
    },
    {
      "function_id": "huggingface__transformers__src__transformers__cli__add_fast_image_processor.py__add_fast_image_processor_to_tests",
      "function_name": "add_fast_image_processor_to_tests",
      "source_file": "huggingface/transformers:src/transformers/cli/add_fast_image_processor.py",
      "baseline_emissions_kg": 1.724072515498847e-09,
      "optimized_emissions_kg": 4.691539690567881e-10,
      "reduction_pct": 72.7880373453409,
      "original_source": "def add_fast_image_processor_to_tests(fast_image_processor_name: str, model_name: str):\n    \"\"\"\n    Add the fast image processor to the image processing tests.\n    \"\"\"\n    tests_path = REPO_PATH / \"tests\" / \"models\" / model_name\n    test_file = tests_path / f\"test_image_processing_{model_name}.py\"\n    if not os.path.exists(test_file):\n        logger.warning(f\"No test file found for {model_name}. Skipping.\")\n        return\n\n    with open(test_file, \"r\", encoding=\"utf-8\") as f:\n        content = f.read()\n\n    # add is_torchvision_available import to the imports\n    # Regex to match import statements from transformers.utils\n    pattern = r\"\"\"\n        from\\s+transformers\\.utils\\s+import\\s+\n        (?:                                   # Non-capturing group for either:\n            ([\\w, ]+)                         # 1. Single-line imports (e.g., 'a, b')\n            |                                 # OR\n            \\((.*?)\\)                         # 2. Multi-line imports (e.g., '(a, ... b)')\n        )\n    \"\"\"\n    regex = re.compile(pattern, re.VERBOSE | re.DOTALL)\n\n    def replacement_function(match):\n        # Extract existing imports\n        existing_imports = (match.group(1) or match.group(2)).split(\",\")\n        existing_imports = existing_imports[:-1] if existing_imports[-1] == \"\\n\" else existing_imports\n        existing_imports = [imp.strip() for imp in existing_imports]\n\n        # Add the new import if not already present\n        if \"is_torchvision_available\" not in existing_imports:\n            existing_imports.append(\"is_torchvision_available\")\n            existing_imports.sort()\n\n        # Rebuild the import statement\n        if match.group(1):  # Single-line import\n            updated_imports = \", \".join(existing_imports)\n        else:  # Multi-line import\n            updated_imports = \"(\\n    \" + \",\\n    \".join(existing_imports) + \",\\n)\"\n\n        return f\"from transformers.utils import {updated_imports}\"\n\n    # Replace all matches in the file content\n    updated_content = regex.sub(replacement_function, content)\n\n    # add the fast image processor to the imports\n    base_import_string = f\"    from transformers import {fast_image_processor_name[:-4]}\"\n    fast_import_string = (\n        f\"    if is_torchvision_available():\\n        from transformers import {fast_image_processor_name}\"\n    )\n    if fast_import_string not in updated_content:\n        updated_content = updated_content.replace(base_import_string, base_import_string + \"\\n\\n\" + fast_import_string)\n\n    # get line starting with \"    image_processing_class = \" and add a line after it starting with \"    fast_image_processing_class = \"\n    image_processing_class_line = re.search(r\"    image_processing_class = .*\", updated_content)\n    if not image_processing_class_line:\n        logger.warning(f\"Couldn't find the 'image_processing_class' line in {test_file}. Skipping.\")\n        return\n\n    fast_image_processing_class_line = (\n        f\"    fast_image_processing_class = {fast_image_processor_name} if is_torchvision_available() else None\"\n    )\n    if \"    fast_image_processing_class = \" not in updated_content:\n        updated_content = updated_content.replace(\n            image_processing_class_line.group(0),\n            image_processing_class_line.group(0) + \"\\n\" + fast_image_processing_class_line,\n        )\n\n    # write the updated content\n    with open(test_file, \"w\", encoding=\"utf-8\") as f:\n        f.write(updated_content)",
      "optimized_source": "def add_fast_image_processor_to_tests(fast_image_processor_name: str, model_name: str):\n    \"\"\"\n    Add the fast image processor to the image processing tests.\n    \"\"\"\n    tests_path = REPO_PATH / \"tests\" / \"models\" / model_name\n    test_file = tests_path / f\"test_image_processing_{model_name}.py\"\n    if not os.path.exists(test_file):\n        logger.warning(f\"No test file found for {model_name}. Skipping.\")\n        return\n\n    with open(test_file, \"r\", encoding=\"utf-8\") as f:\n        content = f.read()\n\n    # add is_torchvision_available import to the imports\n    pattern = re.compile(\n        r\"from\\s+transformers\\.utils\\s+import\\s+(?:([\\w, ]+)|\\((.*?)\\))\",\n        re.DOTALL,\n    )\n\n    def replacement_function(match):\n        raw = match.group(1) or match.group(2)\n        existing_imports = [imp.strip() for imp in raw.split(\",\")]\n        if existing_imports and existing_imports[-1] in (\"\", \"\\n\"):\n            existing_imports.pop()\n        existing_imports = [imp for imp in existing_imports if imp]\n\n        if \"is_torchvision_available\" not in existing_imports:\n            existing_imports.append(\"is_torchvision_available\")\n            existing_imports.sort()\n\n        if match.group(1):  # Single-line import\n            updated_imports = \", \".join(existing_imports)\n        else:  # Multi-line import\n            updated_imports = \"(\\n    \" + \",\\n    \".join(existing_imports) + \",\\n)\"\n\n        return f\"from transformers.utils import {updated_imports}\"\n\n    updated_content = pattern.sub(replacement_function, content)\n\n    # add the fast image processor to the imports\n    base_import_string = f\"    from transformers import {fast_image_processor_name[:-4]}\"\n    fast_import_string = (\n        f\"    if is_torchvision_available():\\n        from transformers import {fast_image_processor_name}\"\n    )\n    if fast_import_string not in updated_content:\n        updated_content = updated_content.replace(base_import_string, base_import_string + \"\\n\\n\" + fast_import_string)\n\n    # get line starting with \"    image_processing_class = \" and add a line after it\n    image_processing_class_line = re.search(r\"    image_processing_class = .*\", updated_content)\n    if not image_processing_class_line:\n        logger.warning(f\"Couldn't find the 'image_processing_class' line in {test_file}. Skipping.\")\n        return\n\n    fast_image_processing_class_line = (\n        f\"    fast_image_processing_class = {fast_image_processor_name} if is_torchvision_available() else None\"\n    )\n    if \"    fast_image_processing_class = \" not in updated_content:\n        updated_content = updated_content.replace(\n            image_processing_class_line.group(0),\n            image_processing_class_line.group(0) + \"\\n\" + fast_image_processing_class_line,\n        )\n\n    with open(test_file, \"w\", encoding=\"utf-8\") as f:\n        f.write(updated_content)"
    },
    {
      "function_id": "huggingface__transformers__src__transformers__image_transforms.py__id_to_rgb",
      "function_name": "id_to_rgb",
      "source_file": "huggingface/transformers:src/transformers/image_transforms.py",
      "baseline_emissions_kg": 1.1599533404689282e-09,
      "optimized_emissions_kg": 3.4729113722569315e-10,
      "reduction_pct": 70.05990455742852,
      "original_source": "def id_to_rgb(id_map):\n    \"\"\"\n    Converts unique ID to RGB color.\n    \"\"\"\n    if isinstance(id_map, np.ndarray):\n        id_map_copy = id_map.copy()\n        rgb_shape = tuple(list(id_map.shape) + [3])\n        rgb_map = np.zeros(rgb_shape, dtype=np.uint8)\n        for i in range(3):\n            rgb_map[..., i] = id_map_copy % 256\n            id_map_copy //= 256\n        return rgb_map\n    color = []\n    for _ in range(3):\n        color.append(id_map % 256)\n        id_map //= 256\n    return color",
      "optimized_source": "def id_to_rgb(id_map):\n    \"\"\"\n    Converts unique ID to RGB color.\n    \"\"\"\n    if isinstance(id_map, np.ndarray):\n        id_map_copy = id_map.copy()\n        rgb_shape = tuple(list(id_map.shape) + [3])\n        rgb_map = np.zeros(rgb_shape, dtype=np.uint8)\n        for i in range(3):\n            rgb_map[..., i] = id_map_copy % 256\n            id_map_copy //= 256\n        return rgb_map\n    color = []\n    for _ in range(3):\n        color.append(id_map % 256)\n        id_map //= 256\n    return color"
    },
    {
      "function_id": "huggingface__transformers__src__transformers__cli__add_new_model_like.py__find_modular_structure",
      "function_name": "find_modular_structure",
      "source_file": "huggingface/transformers:src/transformers/cli/add_new_model_like.py",
      "baseline_emissions_kg": 1.044052253096015e-09,
      "optimized_emissions_kg": 3.3398555536405165e-10,
      "reduction_pct": 68.0106475156146,
      "original_source": "def find_modular_structure(\n    module_name: str, old_model_infos: ModelInfos, new_cased_name: str\n) -> tuple[str, str, list]:\n    \"\"\"\n    Extract the modular structure that will be needed to copy a file `module_name` using modular.\n\n    Args:\n        module_name (`str`):\n            The full path to the python module to copy with modular.\n        old_model_infos (`ModelInfos`):\n            The structure containing the class information of the old model.\n        new_cased_name (`str`):\n            The new cased model name.\n    \"\"\"\n    all_classes, public_classes = find_all_classes_from_file(module_name)\n    import_location = \".\".join(module_name.parts[-2:]).replace(\".py\", \"\")\n    old_cased_name = old_model_infos.camelcase_name\n    imports = f\"from ..{import_location} import {', '.join(class_ for class_ in all_classes)}\"\n    modular_classes = \"\\n\\n\".join(\n        f\"class {class_.replace(old_cased_name, new_cased_name)}({class_}):\\n    pass\" for class_ in all_classes\n    )\n    public_classes = [class_.replace(old_cased_name, new_cased_name) for class_ in public_classes]\n    return imports, modular_classes, public_classes",
      "optimized_source": "def find_modular_structure(\n    module_name: str, old_model_infos: ModelInfos, new_cased_name: str\n) -> tuple[str, str, list]:\n    \"\"\"\n    Extract the modular structure that will be needed to copy a file `module_name` using modular.\n\n    Args:\n        module_name (`str`):\n            The full path to the python module to copy with modular.\n        old_model_infos (`ModelInfos`):\n            The structure containing the class information of the old model.\n        new_cased_name (`str`):\n            The new cased model name.\n    \"\"\"\n    all_classes, public_classes = find_all_classes_from_file(module_name)\n    import_location = \".\".join(module_name.parts[-2:]).replace(\".py\", \"\")\n    old_cased_name = old_model_infos.camelcase_name\n    imports = f\"from ..{import_location} import {', '.join(all_classes)}\"\n    modular_parts = []\n    for class_ in all_classes:\n        new_class = class_.replace(old_cased_name, new_cased_name)\n        modular_parts.append(f\"class {new_class}({class_}):\\n    pass\")\n    modular_classes = \"\\n\\n\".join(modular_parts)\n    public_classes = [class_.replace(old_cased_name, new_cased_name) for class_ in public_classes]\n    return imports, modular_classes, public_classes"
    },
    {
      "function_id": "huggingface__transformers__benchmark_v2__framework__hardware_metrics.py__get_intel_xpu_stats",
      "function_name": "get_intel_xpu_stats",
      "source_file": "huggingface/transformers:benchmark_v2/framework/hardware_metrics.py",
      "baseline_emissions_kg": 8.9936288188037e-10,
      "optimized_emissions_kg": 3.1686821895709724e-10,
      "reduction_pct": 64.76747869618596,
      "original_source": "def get_intel_xpu_stats() -> tuple[int, float]:\n    \"\"\"Returns the utilization and memory used of an Intel XPU\"\"\"\n    # xpu-smi outputs CSV format: Timestamp, DeviceId, GPU Memory Utilization (%), GPU Memory Used (MiB)\n    xpu_smi_output = subprocess.check_output([\"xpu-smi\", \"dump\", \"-m\", \"5,18\", \"-n\", \"1\"])\n    lines = xpu_smi_output.decode(\"utf-8\").strip().split(\"\\n\")\n\n    # Parse all data lines (skip header) and collect stats from all cards\n    xpu_stats = []\n    for line in lines[1:]:\n        data_line = line.split(\",\")\n        if len(data_line) < 4:\n            continue\n        device_id = data_line[1].strip()\n        utilization_str = data_line[2].strip()\n        memory_used_str = data_line[3].strip()\n        if utilization_str != \"N/A\" and memory_used_str != \"N/A\":\n            utilization = int(float(utilization_str))\n            memory_used_mib = float(memory_used_str)\n            xpu_stats.append((device_id, utilization, memory_used_mib))\n\n    if not xpu_stats:\n        return 0, 0.0\n\n    # Sort by utilization (descending) and pick the highest\n    xpu_stats.sort(key=lambda x: x[1], reverse=True)\n    device_id, utilization, memory_used_mib = xpu_stats[0]\n    memory_used_gb = memory_used_mib / 1024\n    return utilization, memory_used_gb",
      "optimized_source": "def get_intel_xpu_stats() -> tuple[int, float]:\n    \"\"\"Returns the utilization and memory used of an Intel XPU\"\"\"\n    xpu_smi_output = subprocess.check_output([\"xpu-smi\", \"dump\", \"-m\", \"5,18\", \"-n\", \"1\"])\n\n    best_utilization = 0\n    best_memory = 0.0\n    found = False\n\n    # Skip the header line and process remaining lines\n    newline_pos = xpu_smi_output.find(b\"\\n\")\n    if newline_pos == -1:\n        return 0, 0.0\n\n    data = xpu_smi_output[newline_pos + 1:]\n\n    for row in data.split(b\"\\n\"):\n        if not row:\n            continue\n        parts = row.split(b\",\")\n        if len(parts) < 4:\n            continue\n        util_raw = parts[2].strip()\n        mem_raw = parts[3].strip()\n        if util_raw == b\"N/A\" or mem_raw == b\"N/A\":\n            continue\n        utilization = int(float(util_raw))\n        if not found or utilization > best_utilization:\n            best_utilization = utilization\n            best_memory = float(mem_raw)\n            found = True\n        elif utilization == best_utilization:\n            mem = float(mem_raw)\n            if mem > best_memory:\n                best_memory = mem\n\n    if not found:\n        return 0, 0.0\n\n    return best_utilization, best_memory / 1024"
    },
    {
      "function_id": "huggingface__transformers__src__transformers__image_utils.py__load_image",
      "function_name": "load_image",
      "source_file": "huggingface/transformers:src/transformers/image_utils.py",
      "baseline_emissions_kg": 7.483895436802414e-10,
      "optimized_emissions_kg": 2.9747703013708807e-10,
      "reduction_pct": 60.25104403861249,
      "original_source": "def load_image(image: Union[str, \"PIL.Image.Image\"], timeout: float | None = None) -> \"PIL.Image.Image\":\n    \"\"\"\n    Loads `image` to a PIL Image.\n\n    Args:\n        image (`str` or `PIL.Image.Image`):\n            The image to convert to the PIL Image format.\n        timeout (`float`, *optional*):\n            The timeout value in seconds for the URL request.\n\n    Returns:\n        `PIL.Image.Image`: A PIL Image.\n    \"\"\"\n    requires_backends(load_image, [\"vision\"])\n    if isinstance(image, str):\n        if image.startswith(\"http://\") or image.startswith(\"https://\"):\n            # We need to actually check for a real protocol, otherwise it's impossible to use a local file\n            # like http_huggingface_co.png\n            image = PIL.Image.open(BytesIO(httpx.get(image, timeout=timeout, follow_redirects=True).content))\n        elif os.path.isfile(image):\n            image = PIL.Image.open(image)\n        else:\n            if image.startswith(\"data:image/\"):\n                image = image.split(\",\")[1]\n\n            # Try to load as base64\n            try:\n                b64 = base64.decodebytes(image.encode())\n                image = PIL.Image.open(BytesIO(b64))\n            except Exception as e:\n                raise ValueError(\n                    f\"Incorrect image source. Must be a valid URL starting with `http://` or `https://`, a valid path to an image file, or a base64 encoded string. Got {image}. Failed with {e}\"\n                )\n    elif not isinstance(image, PIL.Image.Image):\n        raise TypeError(\n            \"Incorrect format used for image. Should be an url linking to an image, a base64 string, a local path, or a PIL image.\"\n        )\n    image = PIL.ImageOps.exif_transpose(image)\n    image = image.convert(\"RGB\")\n    return image",
      "optimized_source": "def load_image(image: Union[str, \"PIL.Image.Image\"], timeout: float | None = None) -> \"PIL.Image.Image\":\n    \"\"\"\n    Loads `image` to a PIL Image.\n\n    Args:\n        image (`str` or `PIL.Image.Image`):\n            The image to convert to the PIL Image format.\n        timeout (`float`, *optional*):\n            The timeout value in seconds for the URL request.\n\n    Returns:\n        `PIL.Image.Image`: A PIL Image.\n    \"\"\"\n    requires_backends(load_image, [\"vision\"])\n    if isinstance(image, str):\n        if image.startswith(\"http://\") or image.startswith(\"https://\"):\n            response = httpx.get(image, timeout=timeout, follow_redirects=True)\n            image = PIL.Image.open(BytesIO(response.content))\n        elif os.path.isfile(image):\n            image = PIL.Image.open(image)\n        else:\n            if image.startswith(\"data:image/\"):\n                # Find comma position directly instead of split which creates a list\n                comma_idx = image.index(\",\") + 1\n                image = image[comma_idx:]\n\n            try:\n                b64 = base64.b64decode(image, validate=True)\n                image = PIL.Image.open(BytesIO(b64))\n            except Exception as e:\n                raise ValueError(\n                    f\"Incorrect image source. Must be a valid URL starting with `http://` or `https://`, a valid path to an image file, or a base64 encoded string. Got {image}. Failed with {e}\"\n                )\n    elif not isinstance(image, PIL.Image.Image):\n        raise TypeError(\n            \"Incorrect format used for image. Should be an url linking to an image, a base64 string, a local path, or a PIL image.\"\n        )\n    image = PIL.ImageOps.exif_transpose(image)\n    if image.mode != \"RGB\":\n        image = image.convert(\"RGB\")\n    return image"
    },
    {
      "function_id": "huggingface__transformers__src__transformers__integrations__accelerate.py__get_module_size_with_ties",
      "function_name": "get_module_size_with_ties",
      "source_file": "huggingface/transformers:src/transformers/integrations/accelerate.py",
      "baseline_emissions_kg": 1.005195314763114e-09,
      "optimized_emissions_kg": 4.500239440065343e-10,
      "reduction_pct": 55.23019880841886,
      "original_source": "def get_module_size_with_ties(\n    tied_params,\n    module_size,\n    module_sizes,\n    modules_to_treat,\n) -> tuple[int, list[str], list[nn.Module]]:\n    \"\"\"\n    Calculate the total size of a module, including its tied parameters.\n\n    Args:\n        tied_params (`List[str]`): The list of tied parameters.\n        module_size (`int`): The size of the module without tied parameters.\n        module_sizes (`Dict[str, int]`): A dictionary mapping each layer name to its size.\n        modules_to_treat (`List[Tuple[str, nn.Module]]`): The list of named modules to treat.\n\n    Returns:\n        `Tuple[int, List[str], List[nn.Module]]`: The total size of the module, the names of the tied modules, and the\n        tied modules.\n    \"\"\"\n    if len(tied_params) < 1:\n        return module_size, [], []\n    tied_module_names = []\n    tied_modules = []\n\n    for tied_param in tied_params:\n        tied_module_index = [i for i, (n, _) in enumerate(modules_to_treat) if tied_param.startswith(n + \".\")][0]\n        tied_module_names.append(modules_to_treat[tied_module_index][0])\n        tied_modules.append(modules_to_treat[tied_module_index][1])\n\n    module_size_with_ties = module_size\n    for tied_param, tied_module_name in zip(tied_params, tied_module_names):\n        module_size_with_ties += module_sizes[tied_module_name] - module_sizes[tied_param]\n\n    return module_size_with_ties, tied_module_names, tied_modules",
      "optimized_source": "def get_module_size_with_ties(\n    tied_params,\n    module_size,\n    module_sizes,\n    modules_to_treat,\n) -> tuple[int, list[str], list[nn.Module]]:\n    \"\"\"\n    Calculate the total size of a module, including its tied parameters.\n\n    Args:\n        tied_params (`List[str]`): The list of tied parameters.\n        module_size (`int`): The size of the module without tied parameters.\n        module_sizes (`Dict[str, int]`): A dictionary mapping each layer name to its size.\n        modules_to_treat (`List[Tuple[str, nn.Module]]`): The list of named modules to treat.\n\n    Returns:\n        `Tuple[int, List[str], List[nn.Module]]`: The total size of the module, the names of the tied modules, and the\n        tied modules.\n    \"\"\"\n    if len(tied_params) < 1:\n        return module_size, [], []\n    tied_module_names = []\n    tied_modules = []\n\n    for tied_param in tied_params:\n        tied_module_index = [i for i, (n, _) in enumerate(modules_to_treat) if tied_param.startswith(n + \".\")][0]\n        tied_module_names.append(modules_to_treat[tied_module_index][0])\n        tied_modules.append(modules_to_treat[tied_module_index][1])\n\n    module_size_with_ties = module_size\n    for tied_param, tied_module_name in zip(tied_params, tied_module_names):\n        module_size_with_ties += module_sizes[tied_module_name] - module_sizes[tied_param]\n\n    return module_size_with_ties, tied_module_names, tied_modules"
    },
    {
      "function_id": "huggingface__transformers__src__transformers__image_transforms.py__resize",
      "function_name": "resize",
      "source_file": "huggingface/transformers:src/transformers/image_transforms.py",
      "baseline_emissions_kg": 6.53271773722372e-10,
      "optimized_emissions_kg": 3.0733710734639316e-10,
      "reduction_pct": 52.95417317739407,
      "original_source": "def resize(\n    image: np.ndarray,\n    size: tuple[int, int],\n    resample: Optional[\"PILImageResampling\"] = None,\n    reducing_gap: int | None = None,\n    data_format: ChannelDimension | None = None,\n    return_numpy: bool = True,\n    input_data_format: str | ChannelDimension | None = None,\n) -> np.ndarray:\n    \"\"\"\n    Resizes `image` to `(height, width)` specified by `size` using the PIL library.\n\n    Args:\n        image (`np.ndarray`):\n            The image to resize.\n        size (`tuple[int, int]`):\n            The size to use for resizing the image.\n        resample (`int`, *optional*, defaults to `PILImageResampling.BILINEAR`):\n            The filter to user for resampling.\n        reducing_gap (`int`, *optional*):\n            Apply optimization by resizing the image in two steps. The bigger `reducing_gap`, the closer the result to\n            the fair resampling. See corresponding Pillow documentation for more details.\n        data_format (`ChannelDimension`, *optional*):\n            The channel dimension format of the output image. If unset, will use the inferred format from the input.\n        return_numpy (`bool`, *optional*, defaults to `True`):\n            Whether or not to return the resized image as a numpy array. If False a `PIL.Image.Image` object is\n            returned.\n        input_data_format (`ChannelDimension`, *optional*):\n            The channel dimension format of the input image. If unset, will use the inferred format from the input.\n\n    Returns:\n        `np.ndarray`: The resized image.\n    \"\"\"\n    requires_backends(resize, [\"vision\"])\n\n    resample = resample if resample is not None else PILImageResampling.BILINEAR\n\n    if not len(size) == 2:\n        raise ValueError(\"size must have 2 elements\")\n\n    # For all transformations, we want to keep the same data format as the input image unless otherwise specified.\n    # The resized image from PIL will always have channels last, so find the input format first.\n    if input_data_format is None:\n        input_data_format = infer_channel_dimension_format(image)\n    data_format = input_data_format if data_format is None else data_format\n\n    # To maintain backwards compatibility with the resizing done in previous image feature extractors, we use\n    # the pillow library to resize the image and then convert back to numpy\n    do_rescale = False\n    if not isinstance(image, PIL.Image.Image):\n        do_rescale = _rescale_for_pil_conversion(image)\n        image = to_pil_image(image, do_rescale=do_rescale, input_data_format=input_data_format)\n    height, width = size\n    # PIL images are in the format (width, height)\n    resized_image = image.resize((width, height), resample=resample, reducing_gap=reducing_gap)\n\n    if return_numpy:\n        resized_image = np.array(resized_image)\n        # If the input image channel dimension was of size 1, then it is dropped when converting to a PIL image\n        # so we need to add it back if necessary.\n        resized_image = np.expand_dims(resized_image, axis=-1) if resized_image.ndim == 2 else resized_image\n        # The image is always in channels last format after converting from a PIL image\n        resized_image = to_channel_dimension_format(\n            resized_image, data_format, input_channel_dim=ChannelDimension.LAST\n        )\n        # If an image was rescaled to be in the range [0, 255] before converting to a PIL image, then we need to\n        # rescale it back to the original range.\n        resized_image = rescale(resized_image, 1 / 255) if do_rescale else resized_image\n    return resized_image",
      "optimized_source": "def resize(\n    image: np.ndarray,\n    size: tuple[int, int],\n    resample: Optional[\"PILImageResampling\"] = None,\n    reducing_gap: int | None = None,\n    data_format: ChannelDimension | None = None,\n    return_numpy: bool = True,\n    input_data_format: str | ChannelDimension | None = None,\n) -> np.ndarray:\n    requires_backends(resize, [\"vision\"])\n\n    if resample is None:\n        resample = PILImageResampling.BILINEAR\n\n    if len(size) != 2:\n        raise ValueError(\"size must have 2 elements\")\n\n    if input_data_format is None:\n        input_data_format = infer_channel_dimension_format(image)\n    if data_format is None:\n        data_format = input_data_format\n\n    do_rescale = False\n    if not isinstance(image, PIL.Image.Image):\n        do_rescale = _rescale_for_pil_conversion(image)\n        image = to_pil_image(image, do_rescale=do_rescale, input_data_format=input_data_format)\n\n    height, width = size\n    resized_image = image.resize((width, height), resample=resample, reducing_gap=reducing_gap)\n\n    if return_numpy:\n        resized_image = np.array(resized_image)\n        ndim = resized_image.ndim\n        if ndim == 2:\n            resized_image = resized_image[:, :, np.newaxis]\n\n        if data_format != ChannelDimension.LAST:\n            resized_image = to_channel_dimension_format(\n                resized_image, data_format, input_channel_dim=ChannelDimension.LAST\n            )\n\n        if do_rescale:\n            dtype = np.float64 if resized_image.dtype == np.float64 else np.float32\n            resized_image = resized_image.astype(dtype, copy=False)\n            resized_image *= np.float32(1.0 / 255.0) if dtype == np.float32 else (1.0 / 255.0)\n\n    return resized_image"
    },
    {
      "function_id": "huggingface__transformers__src__transformers__data__metrics__squad_metrics.py__find_best_thresh_v2",
      "function_name": "find_best_thresh_v2",
      "source_file": "huggingface/transformers:src/transformers/data/metrics/squad_metrics.py",
      "baseline_emissions_kg": 5.939197803672864e-10,
      "optimized_emissions_kg": 2.912201854633168e-10,
      "reduction_pct": 50.96641076961217,
      "original_source": "def find_best_thresh_v2(preds, scores, na_probs, qid_to_has_ans):\n    num_no_ans = sum(1 for k in qid_to_has_ans if not qid_to_has_ans[k])\n    cur_score = num_no_ans\n    best_score = cur_score\n    best_thresh = 0.0\n    qid_list = sorted(na_probs, key=lambda k: na_probs[k])\n    for qid in qid_list:\n        if qid not in scores:\n            continue\n        if qid_to_has_ans[qid]:\n            diff = scores[qid]\n        else:\n            if preds[qid]:\n                diff = -1\n            else:\n                diff = 0\n        cur_score += diff\n        if cur_score > best_score:\n            best_score = cur_score\n            best_thresh = na_probs[qid]\n\n    has_ans_score, has_ans_cnt = 0, 0\n    for qid in qid_list:\n        if not qid_to_has_ans[qid]:\n            continue\n        has_ans_cnt += 1\n\n        if qid not in scores:\n            continue\n        has_ans_score += scores[qid]\n\n    return 100.0 * best_score / len(scores), best_thresh, 1.0 * has_ans_score / has_ans_cnt",
      "optimized_source": "def find_best_thresh_v2(preds, scores, na_probs, qid_to_has_ans):\n    num_no_ans = sum(1 for k, v in qid_to_has_ans.items() if not v)\n    cur_score = num_no_ans\n    best_score = cur_score\n    best_thresh = 0.0\n    \n    qid_list = sorted(na_probs, key=na_probs.__getitem__)\n    \n    has_ans_score = 0\n    has_ans_cnt = 0\n    \n    scores_get = scores.get\n    qid_to_has_ans_get = qid_to_has_ans.__getitem__\n    preds_get = preds.__getitem__\n    na_probs_get = na_probs.__getitem__\n    \n    for qid in qid_list:\n        has_ans = qid_to_has_ans_get(qid)\n        \n        if has_ans:\n            has_ans_cnt += 1\n        \n        score = scores_get(qid)\n        if score is None:\n            continue\n        \n        if has_ans:\n            has_ans_score += score\n            diff = score\n        else:\n            diff = -1 if preds_get(qid) else 0\n        \n        cur_score += diff\n        if cur_score > best_score:\n            best_score = cur_score\n            best_thresh = na_probs_get(qid)\n    \n    return 100.0 * best_score / len(scores), best_thresh, 1.0 * has_ans_score / has_ans_cnt"
    },
    {
      "function_id": "huggingface__transformers__src__transformers__dynamic_module_utils.py__get_relative_imports",
      "function_name": "get_relative_imports",
      "source_file": "huggingface/transformers:src/transformers/dynamic_module_utils.py",
      "baseline_emissions_kg": 8.455335673847e-10,
      "optimized_emissions_kg": 4.5302181604201903e-10,
      "reduction_pct": 46.42178223115964,
      "original_source": "def get_relative_imports(module_file: str | os.PathLike) -> list[str]:\n    \"\"\"\n    Get the list of modules that are relatively imported in a module file.\n\n    Args:\n        module_file (`str` or `os.PathLike`): The module file to inspect.\n\n    Returns:\n        `list[str]`: The list of relative imports in the module.\n    \"\"\"\n    with open(module_file, encoding=\"utf-8\") as f:\n        content = f.read()\n\n    # Imports of the form `import .xxx`\n    relative_imports = re.findall(r\"^\\s*import\\s+\\.(\\S+)\\s*$\", content, flags=re.MULTILINE)\n    # Imports of the form `from .xxx import yyy`\n    relative_imports += re.findall(r\"^\\s*from\\s+\\.(\\S+)\\s+import\", content, flags=re.MULTILINE)\n    # Unique-ify\n    return list(set(relative_imports))",
      "optimized_source": "def get_relative_imports(module_file: str | os.PathLike) -> list[str]:\n    \"\"\"\n    Get the list of modules that are relatively imported in a module file.\n\n    Args:\n        module_file (`str` or `os.PathLike`): The module file to inspect.\n\n    Returns:\n        `list[str]`: The list of relative imports in the module.\n    \"\"\"\n    _re_import = re.compile(r\"^\\s*import\\s+\\.(\\S+)\\s*$\", flags=re.MULTILINE)\n    _re_from = re.compile(r\"^\\s*from\\s+\\.(\\S+)\\s+import\", flags=re.MULTILINE)\n\n    with open(module_file, encoding=\"utf-8\") as f:\n        content = f.read()\n\n    result = set(_re_import.findall(content))\n    result.update(_re_from.findall(content))\n    return list(result)"
    },
    {
      "function_id": "huggingface__transformers__src__transformers__image_utils.py__get_channel_dimension_axis",
      "function_name": "get_channel_dimension_axis",
      "source_file": "huggingface/transformers:src/transformers/image_utils.py",
      "baseline_emissions_kg": 6.52191674086498e-10,
      "optimized_emissions_kg": 3.5733770696970165e-10,
      "reduction_pct": 45.20971040143804,
      "original_source": "def get_channel_dimension_axis(image: np.ndarray, input_data_format: ChannelDimension | str | None = None) -> int:\n    \"\"\"\n    Returns the channel dimension axis of the image.\n\n    Args:\n        image (`np.ndarray`):\n            The image to get the channel dimension axis of.\n        input_data_format (`ChannelDimension` or `str`, *optional*):\n            The channel dimension format of the image. If `None`, will infer the channel dimension from the image.\n\n    Returns:\n        The channel dimension axis of the image.\n    \"\"\"\n    if input_data_format is None:\n        input_data_format = infer_channel_dimension_format(image)\n    if input_data_format == ChannelDimension.FIRST:\n        return image.ndim - 3\n    elif input_data_format == ChannelDimension.LAST:\n        return image.ndim - 1\n    raise ValueError(f\"Unsupported data format: {input_data_format}\")",
      "optimized_source": "def get_channel_dimension_axis(image: np.ndarray, input_data_format: ChannelDimension | str | None = None) -> int:\n    if input_data_format is None:\n        input_data_format = infer_channel_dimension_format(image)\n    if input_data_format == ChannelDimension.FIRST:\n        return image.ndim - 3\n    if input_data_format == ChannelDimension.LAST:\n        return image.ndim - 1\n    raise ValueError(f\"Unsupported data format: {input_data_format}\")"
    },
    {
      "function_id": "huggingface__transformers__src__transformers__initialization.py__no_init_weights",
      "function_name": "no_init_weights",
      "source_file": "huggingface/transformers:src/transformers/initialization.py",
      "baseline_emissions_kg": 9.65188753666007e-10,
      "optimized_emissions_kg": 5.68066580608138e-10,
      "reduction_pct": 41.144508941852926,
      "original_source": "def no_init_weights():\n    \"\"\"\n    Disable weight initialization both at the torch-level, and at the transformers-level (`init_weights`).\n    This is used to speed-up initializing an empty model with deepspeed, as we do not initialize the model on meta device\n    with deepspeed, but we still don't need to run expensive weight initializations as we are loading params afterwards.\n    \"\"\"\n    from .modeling_utils import PreTrainedModel\n\n    def empty_func(*args, **kwargs):\n        pass\n\n    originals = defaultdict(dict)\n    try:\n        # Replace all torch funcs by empty ones\n        for module_name in TORCH_MODULES_TO_PATCH:\n            if module_name in sys.modules:\n                module = sys.modules[module_name]\n                for func_name in TORCH_INIT_FUNCTIONS.keys():\n                    if hasattr(module, func_name):\n                        originals[module][func_name] = getattr(module, func_name)\n                        setattr(module, func_name, empty_func)\n\n        # Also patch our own `init_weights`\n        original_init_weights = PreTrainedModel.init_weights\n        PreTrainedModel.init_weights = empty_func\n\n        yield\n    finally:\n        # Set back the original torch functions on all modules\n        for module, functions in originals.items():\n            for func_name, func in functions.items():\n                setattr(module, func_name, func)\n        # Set back `init_weights`\n        PreTrainedModel.init_weights = original_init_weights",
      "optimized_source": "def no_init_weights():\n    \"\"\"\n    Disable weight initialization both at the torch-level, and at the transformers-level (`init_weights`).\n    This is used to speed-up initializing an empty model with deepspeed, as we do not initialize the model on meta device\n    with deepspeed, but we still don't need to run expensive weight initializations as we are loading params afterwards.\n    \"\"\"\n    from .modeling_utils import PreTrainedModel\n\n    def empty_func(*args, **kwargs):\n        pass\n\n    originals = {}\n    try:\n        # Replace all torch funcs by empty ones\n        for module_name in TORCH_MODULES_TO_PATCH:\n            module = sys.modules.get(module_name)\n            if module is not None:\n                mod_originals = {}\n                for func_name in TORCH_INIT_FUNCTIONS:\n                    orig = getattr(module, func_name, None)\n                    if orig is not None:\n                        mod_originals[func_name] = orig\n                        setattr(module, func_name, empty_func)\n                if mod_originals:\n                    originals[module] = mod_originals\n\n        # Also patch our own `init_weights`\n        original_init_weights = PreTrainedModel.init_weights\n        PreTrainedModel.init_weights = empty_func\n\n        yield\n    finally:\n        # Set back the original torch functions on all modules\n        for module, functions in originals.items():\n            for func_name, func in functions.items():\n                setattr(module, func_name, func)\n        # Set back `init_weights`\n        PreTrainedModel.init_weights = original_init_weights"
    },
    {
      "function_id": "huggingface__transformers__src__transformers__image_utils.py__validate_preprocess_arguments",
      "function_name": "validate_preprocess_arguments",
      "source_file": "huggingface/transformers:src/transformers/image_utils.py",
      "baseline_emissions_kg": 6.04586997014121e-10,
      "optimized_emissions_kg": 3.577616046037292e-10,
      "reduction_pct": 40.82545500141262,
      "original_source": "def validate_preprocess_arguments(\n    do_rescale: bool | None = None,\n    rescale_factor: float | None = None,\n    do_normalize: bool | None = None,\n    image_mean: float | list[float] | None = None,\n    image_std: float | list[float] | None = None,\n    do_pad: bool | None = None,\n    pad_size: dict[str, int] | int | None = None,\n    do_center_crop: bool | None = None,\n    crop_size: dict[str, int] | None = None,\n    do_resize: bool | None = None,\n    size: dict[str, int] | None = None,\n    resample: Optional[\"PILImageResampling\"] = None,\n    interpolation: Optional[\"InterpolationMode\"] = None,\n):\n    \"\"\"\n    Checks validity of typically used arguments in an `ImageProcessor` `preprocess` method.\n    Raises `ValueError` if arguments incompatibility is caught.\n    Many incompatibilities are model-specific. `do_pad` sometimes needs `size_divisor`,\n    sometimes `size_divisibility`, and sometimes `size`. New models and processors added should follow\n    existing arguments when possible.\n\n    \"\"\"\n    if do_rescale and rescale_factor is None:\n        raise ValueError(\"`rescale_factor` must be specified if `do_rescale` is `True`.\")\n\n    if do_pad and pad_size is None:\n        # Processors pad images using different args depending on the model, so the below check is pointless\n        # but we keep it for BC for now. TODO: remove in v5\n        # Usually padding can be called with:\n        #   - \"pad_size/size\" if we're padding to specific values\n        #   - \"size_divisor\" if we're padding to any value divisible by X\n        #   - \"None\" if we're padding to the maximum size image in batch\n        raise ValueError(\n            \"Depending on the model, `size_divisor` or `pad_size` or `size` must be specified if `do_pad` is `True`.\"\n        )\n\n    if do_normalize and (image_mean is None or image_std is None):\n        raise ValueError(\"`image_mean` and `image_std` must both be specified if `do_normalize` is `True`.\")\n\n    if do_center_crop and crop_size is None:\n        raise ValueError(\"`crop_size` must be specified if `do_center_crop` is `True`.\")\n\n    if interpolation is not None and resample is not None:\n        raise ValueError(\n            \"Only one of `interpolation` and `resample` should be specified, depending on image processor type.\"\n        )\n\n    if do_resize and not (size is not None and (resample is not None or interpolation is not None)):\n        raise ValueError(\"`size` and `resample/interpolation` must be specified if `do_resize` is `True`.\")",
      "optimized_source": "def validate_preprocess_arguments(\n    do_rescale: bool | None = None,\n    rescale_factor: float | None = None,\n    do_normalize: bool | None = None,\n    image_mean: float | list[float] | None = None,\n    image_std: float | list[float] | None = None,\n    do_pad: bool | None = None,\n    pad_size: dict[str, int] | int | None = None,\n    do_center_crop: bool | None = None,\n    crop_size: dict[str, int] | None = None,\n    do_resize: bool | None = None,\n    size: dict[str, int] | None = None,\n    resample: Optional[\"PILImageResampling\"] = None,\n    interpolation: Optional[\"InterpolationMode\"] = None,\n):\n    \"\"\"\n    Checks validity of typically used arguments in an `ImageProcessor` `preprocess` method.\n    Raises `ValueError` if arguments incompatibility is caught.\n    Many incompatibilities are model-specific. `do_pad` sometimes needs `size_divisor`,\n    sometimes `size_divisibility`, and sometimes `size`. New models and processors added should follow\n    existing arguments when possible.\n\n    \"\"\"\n    if do_rescale and rescale_factor is None:\n        raise ValueError(\"`rescale_factor` must be specified if `do_rescale` is `True`.\")\n\n    if do_pad and pad_size is None:\n        # Processors pad images using different args depending on the model, so the below check is pointless\n        # but we keep it for BC for now. TODO: remove in v5\n        # Usually padding can be called with:\n        #   - \"pad_size/size\" if we're padding to specific values\n        #   - \"size_divisor\" if we're padding to any value divisible by X\n        #   - \"None\" if we're padding to the maximum size image in batch\n        raise ValueError(\n            \"Depending on the model, `size_divisor` or `pad_size` or `size` must be specified if `do_pad` is `True`.\"\n        )\n\n    if do_normalize and (image_mean is None or image_std is None):\n        raise ValueError(\"`image_mean` and `image_std` must both be specified if `do_normalize` is `True`.\")\n\n    if do_center_crop and crop_size is None:\n        raise ValueError(\"`crop_size` must be specified if `do_center_crop` is `True`.\")\n\n    if interpolation is not None and resample is not None:\n        raise ValueError(\n            \"Only one of `interpolation` and `resample` should be specified, depending on image processor type.\"\n        )\n\n    if do_resize and not (size is not None and (resample is not None or interpolation is not None)):\n        raise ValueError(\"`size` and `resample/interpolation` must be specified if `do_resize` is `True`.\")"
    },
    {
      "function_id": "huggingface__transformers__src__transformers__cli__add_fast_image_processor.py__add_fast_image_processor_to_doc",
      "function_name": "add_fast_image_processor_to_doc",
      "source_file": "huggingface/transformers:src/transformers/cli/add_fast_image_processor.py",
      "baseline_emissions_kg": 5.468271741003262e-10,
      "optimized_emissions_kg": 3.2649598390271417e-10,
      "reduction_pct": 40.29265563843173,
      "original_source": "def add_fast_image_processor_to_doc(fast_image_processor_name: str, model_name: str):\n    \"\"\"\n    Add the fast image processor to the model's doc file.\n    \"\"\"\n    doc_source = REPO_PATH / \"docs\" / \"source\"\n    # find the doc files\n    doc_files = list(doc_source.glob(f\"*/model_doc/{model_name}.md\"))\n    if not doc_files:\n        # try again with \"-\"\n        doc_files = list(doc_source.glob(f\"*/model_doc/{model_name.replace('_', '-')}.md\"))\n    if not doc_files:\n        raise ValueError(f\"No doc files found for {model_name}\")\n\n    base_doc_string = (\n        f\"## {fast_image_processor_name[:-4]}\\n\\n[[autodoc]] {fast_image_processor_name[:-4]}\\n    - preprocess\"\n    )\n    fast_doc_string = f\"## {fast_image_processor_name}\\n\\n[[autodoc]] {fast_image_processor_name}\\n    - preprocess\"\n\n    for doc_file in doc_files:\n        with open(doc_file, \"r\", encoding=\"utf-8\") as f:\n            content = f.read()\n\n        if fast_doc_string not in content:\n            # add the fast image processor to the doc\n            updated_content = content.replace(\n                base_doc_string,\n                base_doc_string + \"\\n\\n\" + fast_doc_string,\n            )\n\n            # write the updated content\n            with open(doc_file, \"w\", encoding=\"utf-8\") as f:\n                f.write(updated_content)",
      "optimized_source": "def add_fast_image_processor_to_doc(fast_image_processor_name: str, model_name: str):\n    \"\"\"\n    Add the fast image processor to the model's doc file.\n    \"\"\"\n    doc_source = REPO_PATH / \"docs\" / \"source\"\n    # find the doc files\n    doc_files = list(doc_source.glob(f\"*/model_doc/{model_name}.md\"))\n    if not doc_files:\n        doc_files = list(doc_source.glob(f\"*/model_doc/{model_name.replace('_', '-')}.md\"))\n    if not doc_files:\n        raise ValueError(f\"No doc files found for {model_name}\")\n\n    base_name = fast_image_processor_name[:-4]\n    base_doc_string = f\"## {base_name}\\n\\n[[autodoc]] {base_name}\\n    - preprocess\"\n    fast_doc_string = f\"## {fast_image_processor_name}\\n\\n[[autodoc]] {fast_image_processor_name}\\n    - preprocess\"\n    combined = base_doc_string + \"\\n\\n\" + fast_doc_string\n\n    for doc_file in doc_files:\n        with open(doc_file, \"r\", encoding=\"utf-8\") as f:\n            content = f.read()\n\n        if fast_doc_string not in content:\n            updated_content = content.replace(base_doc_string, combined)\n            with open(doc_file, \"w\", encoding=\"utf-8\") as f:\n                f.write(updated_content)"
    },
    {
      "function_id": "huggingface__transformers__src__transformers__image_transforms.py__corners_to_center_format",
      "function_name": "corners_to_center_format",
      "source_file": "huggingface/transformers:src/transformers/image_transforms.py",
      "baseline_emissions_kg": 5.91089774511056e-10,
      "optimized_emissions_kg": 3.7036181688599756e-10,
      "reduction_pct": 37.34254374602277,
      "original_source": "def corners_to_center_format(bboxes_corners: TensorType) -> TensorType:\n    \"\"\"\n    Converts bounding boxes from corners format to center format.\n\n    corners format: contains the coordinates for the top-left and bottom-right corners of the box\n        (top_left_x, top_left_y, bottom_right_x, bottom_right_y)\n    center format: contains the coordinate for the center of the box and its the width, height dimensions\n        (center_x, center_y, width, height)\n    \"\"\"\n    # Inverse function accepts different input types so implemented here too\n    if is_torch_tensor(bboxes_corners):\n        return _corners_to_center_format_torch(bboxes_corners)\n    elif isinstance(bboxes_corners, np.ndarray):\n        return _corners_to_center_format_numpy(bboxes_corners)\n\n    raise ValueError(f\"Unsupported input type {type(bboxes_corners)}\")",
      "optimized_source": "def corners_to_center_format(bboxes_corners: TensorType) -> TensorType:\n    if isinstance(bboxes_corners, np.ndarray):\n        return _corners_to_center_format_numpy(bboxes_corners)\n    elif is_torch_tensor(bboxes_corners):\n        return _corners_to_center_format_torch(bboxes_corners)\n    raise ValueError(f\"Unsupported input type {type(bboxes_corners)}\")"
    },
    {
      "function_id": "huggingface__transformers__src__transformers__integrations__accelerate.py__accelerate_dispatch",
      "function_name": "accelerate_dispatch",
      "source_file": "huggingface/transformers:src/transformers/integrations/accelerate.py",
      "baseline_emissions_kg": 4.5871915942407206e-10,
      "optimized_emissions_kg": 2.89455032319529e-10,
      "reduction_pct": 36.89929309189012,
      "original_source": "def accelerate_dispatch(model, hf_quantizer, device_map, offload_folder, offload_index, offload_buffers):\n    device_map_kwargs = {\n        \"device_map\": device_map,\n        \"offload_dir\": offload_folder,\n        \"offload_index\": offload_index,\n        \"offload_buffers\": offload_buffers,\n    }\n    if \"skip_keys\" in inspect.signature(dispatch_model).parameters:\n        device_map_kwargs[\"skip_keys\"] = model._skip_keys_device_placement\n    # For HQQ method we force-set the hooks for single GPU envs\n    if (\n        \"force_hooks\" in inspect.signature(dispatch_model).parameters\n        and hf_quantizer is not None\n        and hf_quantizer.quantization_config.quant_method == QuantizationMethod.HQQ\n    ):\n        device_map_kwargs[\"force_hooks\"] = True\n    if (\n        hf_quantizer is not None\n        and hf_quantizer.quantization_config.quant_method == QuantizationMethod.FBGEMM_FP8\n        and isinstance(device_map, dict)\n        and (\"cpu\" in device_map.values() or \"disk\" in device_map.values())\n    ):\n        device_map_kwargs[\"offload_buffers\"] = True\n\n    if not is_fsdp_enabled() and not is_deepspeed_zero3_enabled():\n        dispatch_model(model, **device_map_kwargs)",
      "optimized_source": "def accelerate_dispatch(model, hf_quantizer, device_map, offload_folder, offload_index, offload_buffers):\n    device_map_kwargs = {\n        \"device_map\": device_map,\n        \"offload_dir\": offload_folder,\n        \"offload_index\": offload_index,\n        \"offload_buffers\": offload_buffers,\n    }\n    \n    params = inspect.signature(dispatch_model).parameters\n    \n    if \"skip_keys\" in params:\n        device_map_kwargs[\"skip_keys\"] = model._skip_keys_device_placement\n    \n    if hf_quantizer is not None:\n        quant_method = hf_quantizer.quantization_config.quant_method\n        \n        if quant_method == QuantizationMethod.HQQ:\n            if \"force_hooks\" in params:\n                device_map_kwargs[\"force_hooks\"] = True\n        elif (\n            quant_method == QuantizationMethod.FBGEMM_FP8\n            and isinstance(device_map, dict)\n            and (\"cpu\" in device_map.values() or \"disk\" in device_map.values())\n        ):\n            device_map_kwargs[\"offload_buffers\"] = True\n\n    if not is_fsdp_enabled() and not is_deepspeed_zero3_enabled():\n        dispatch_model(model, **device_map_kwargs)"
    },
    {
      "function_id": "huggingface__transformers__src__transformers__debug_utils.py__detect_overflow",
      "function_name": "detect_overflow",
      "source_file": "huggingface/transformers:src/transformers/debug_utils.py",
      "baseline_emissions_kg": 4.610862146771977e-10,
      "optimized_emissions_kg": 2.9861991772486356e-10,
      "reduction_pct": 35.23555720833581,
      "original_source": "def detect_overflow(var, ctx):\n    \"\"\"\n    Report whether the tensor contains any `nan` or `inf` entries.\n\n    This is useful for detecting overflows/underflows and best to call right after the function that did some math that\n    modified the tensor in question.\n\n    This function contains a few other helper features that you can enable and tweak directly if you want to track\n    various other things.\n\n    Args:\n        var: the tensor variable to check\n        ctx: the message to print as a context\n\n    Return:\n        `True` if `inf` or `nan` was detected, `False` otherwise\n    \"\"\"\n    detected = False\n    if torch.isnan(var).any().item():\n        detected = True\n        print(f\"{ctx} has nans\")\n    if torch.isinf(var).any().item():\n        detected = True\n        print(f\"{ctx} has infs\")\n\n    # if needed to monitor large elements can enable the following\n    if 0:  # and detected:\n        n100 = var[torch.ge(var.abs(), 100)]\n        if n100.numel() > 0:\n            print(f\"{ctx}:  n100={n100.numel()}\")\n        n1000 = var[torch.ge(var.abs(), 1000)]\n        if n1000.numel() > 0:\n            print(f\"{ctx}: n1000={n1000.numel()}\")\n        n10000 = var[torch.ge(var.abs(), 10000)]\n        if n10000.numel() > 0:\n            print(f\"{ctx}: n10000={n10000.numel()}\")\n\n    if 0:\n        print(f\"min={var.min():9.2e} max={var.max():9.2e}\")\n\n    if 0:\n        print(f\"min={var.min():9.2e} max={var.max():9.2e} var={var.var():9.2e} mean={var.mean():9.2e} ({ctx})\")\n\n    return detected",
      "optimized_source": "def detect_overflow(var, ctx):\n    \"\"\"\n    Report whether the tensor contains any `nan` or `inf` entries.\n\n    This is useful for detecting overflows/underflows and best to call right after the function that did some math that\n    modified the tensor in question.\n\n    This function contains a few other helper features that you can enable and tweak directly if you want to track\n    various other things.\n\n    Args:\n        var: the tensor variable to check\n        ctx: the message to print as a context\n\n    Return:\n        `True` if `inf` or `nan` was detected, `False` otherwise\n    \"\"\"\n    # Use a single pass: check if all values are finite (not nan and not inf)\n    # torch.isfinite returns False for both nan and inf\n    if torch.isfinite(var).all().item():\n        return False\n\n    # Only do the more expensive per-type check if we already know something is wrong\n    detected = False\n    if torch.isnan(var).any().item():\n        detected = True\n        print(f\"{ctx} has nans\")\n    if torch.isinf(var).any().item():\n        detected = True\n        print(f\"{ctx} has infs\")\n\n    return detected"
    },
    {
      "function_id": "huggingface__transformers__src__transformers__dynamic_module_utils.py__resolve_trust_remote_code",
      "function_name": "resolve_trust_remote_code",
      "source_file": "huggingface/transformers:src/transformers/dynamic_module_utils.py",
      "baseline_emissions_kg": 1.037057811852719e-09,
      "optimized_emissions_kg": 7.24053929981892e-10,
      "reduction_pct": 30.18191255043351,
      "original_source": "def resolve_trust_remote_code(\n    trust_remote_code, model_name, has_local_code, has_remote_code, error_message=None, upstream_repo=None\n):\n    \"\"\"\n    Resolves the `trust_remote_code` argument. If there is remote code to be loaded, the user must opt-in to loading\n    it.\n\n    Args:\n        trust_remote_code (`bool` or `None`):\n            User-defined `trust_remote_code` value.\n        model_name (`str`):\n            The name of the model repository in huggingface.co.\n        has_local_code (`bool`):\n            Whether the model has local code.\n        has_remote_code (`bool`):\n            Whether the model has remote code.\n        error_message (`str`, *optional*):\n            Custom error message to display if there is remote code to load and the user didn't opt-in. If unset, the error\n            message will be regarding loading a model with custom code.\n\n    Returns:\n        The resolved `trust_remote_code` value.\n    \"\"\"\n    if error_message is None:\n        if upstream_repo is not None:\n            error_message = (\n                f\"The repository {model_name} references custom code contained in {upstream_repo} which \"\n                f\"must be executed to correctly load the model. You can inspect the repository \"\n                f\"content at https://hf.co/{upstream_repo} .\\n\"\n            )\n        elif os.path.isdir(model_name):\n            error_message = (\n                f\"The repository {model_name} contains custom code which must be executed \"\n                f\"to correctly load the model. You can inspect the repository \"\n                f\"content at {os.path.abspath(model_name)} .\\n\"\n            )\n        else:\n            error_message = (\n                f\"The repository {model_name} contains custom code which must be executed \"\n                f\"to correctly load the model. You can inspect the repository \"\n                f\"content at https://hf.co/{model_name} .\\n\"\n            )\n\n    if trust_remote_code is None:\n        if has_local_code:\n            trust_remote_code = False\n        elif has_remote_code and TIME_OUT_REMOTE_CODE > 0:\n            prev_sig_handler = None\n            try:\n                prev_sig_handler = signal.signal(signal.SIGALRM, _raise_timeout_error)\n                signal.alarm(TIME_OUT_REMOTE_CODE)\n                while trust_remote_code is None:\n                    answer = input(\n                        f\"{error_message} You can inspect the repository content at https://hf.co/{model_name}.\\n\"\n                        f\"You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\\n\\n\"\n                        f\"Do you wish to run the custom code? [y/N] \"\n                    )\n                    if answer.lower() in [\"yes\", \"y\", \"1\"]:\n                        trust_remote_code = True\n                    elif answer.lower() in [\"no\", \"n\", \"0\", \"\"]:\n                        trust_remote_code = False\n                signal.alarm(0)\n            except Exception:\n                # OS which does not support signal.SIGALRM\n                raise ValueError(\n                    f\"{error_message} You can inspect the repository content at https://hf.co/{model_name}.\\n\"\n                    f\"Please pass the argument `trust_remote_code=True` to allow custom code to be run.\"\n                )\n            finally:\n                if prev_sig_handler is not None:\n                    signal.signal(signal.SIGALRM, prev_sig_handler)\n                    signal.alarm(0)\n        elif has_remote_code:\n            # For the CI which puts the timeout at 0\n            _raise_timeout_error(None, None)\n\n    if has_remote_code and not has_local_code and not trust_remote_code:\n        raise ValueError(\n            f\"{error_message} You can inspect the repository content at https://hf.co/{model_name}.\\n\"\n            f\"Please pass the argument `trust_remote_code=True` to allow custom code to be run.\"\n        )\n\n    return trust_remote_code",
      "optimized_source": "def resolve_trust_remote_code(\n    trust_remote_code, model_name, has_local_code, has_remote_code, error_message=None, upstream_repo=None\n):\n    if trust_remote_code is not None:\n        if has_remote_code and not has_local_code and not trust_remote_code:\n            if error_message is None:\n                error_message = _build_error_message(model_name, upstream_repo)\n            raise ValueError(\n                f\"{error_message} You can inspect the repository content at https://hf.co/{model_name}.\\n\"\n                f\"Please pass the argument `trust_remote_code=True` to allow custom code to be run.\"\n            )\n        return trust_remote_code\n\n    if has_local_code:\n        return False\n\n    if has_remote_code:\n        if error_message is None:\n            error_message = _build_error_message(model_name, upstream_repo)\n\n        deny_msg = (\n            f\"{error_message} You can inspect the repository content at https://hf.co/{model_name}.\\n\"\n            f\"Please pass the argument `trust_remote_code=True` to allow custom code to be run.\"\n        )\n\n        if TIME_OUT_REMOTE_CODE > 0:\n            prev_sig_handler = None\n            try:\n                prev_sig_handler = signal.signal(signal.SIGALRM, _raise_timeout_error)\n                signal.alarm(TIME_OUT_REMOTE_CODE)\n                prompt = (\n                    f\"{error_message} You can inspect the repository content at https://hf.co/{model_name}.\\n\"\n                    f\"You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\\n\\n\"\n                    f\"Do you wish to run the custom code? [y/N] \"\n                )\n                while True:\n                    answer = input(prompt).lower()\n                    if answer in (\"yes\", \"y\", \"1\"):\n                        return True\n                    elif answer in (\"no\", \"n\", \"0\", \"\"):\n                        raise ValueError(deny_msg)\n                signal.alarm(0)\n            except ValueError:\n                raise\n            except Exception:\n                raise ValueError(deny_msg)\n            finally:\n                if prev_sig_handler is not None:\n                    signal.signal(signal.SIGALRM, prev_sig_handler)\n                    signal.alarm(0)\n        else:\n            _raise_timeout_error(None, None)\n\n    return trust_remote_code\n\n\ndef _build_error_message(model_name, upstream_repo):\n    if upstream_repo is not None:\n        return (\n            f\"The repository {model_name} references custom code contained in {upstream_repo} which \"\n            f\"must be executed to correctly load the model. You can inspect the repository \"\n            f\"content at https://hf.co/{upstream_repo} .\\n\"\n        )\n    elif os.path.isdir(model_name):\n        return (\n            f\"The repository {model_name} contains custom code which must be executed \"\n            f\"to correctly load the model. You can inspect the repository \"\n            f\"content at {os.path.abspath(model_name)} .\\n\"\n        )\n    else:\n        return (\n            f\"The repository {model_name} contains custom code which must be executed \"\n            f\"to correctly load the model. You can inspect the repository \"\n            f\"content at https://hf.co/{model_name} .\\n\"\n        )"
    },
    {
      "function_id": "huggingface__transformers__.circleci__parse_test_outputs.py__parse_pytest_output",
      "function_name": "parse_pytest_output",
      "source_file": "huggingface/transformers:.circleci/parse_test_outputs.py",
      "baseline_emissions_kg": 4.5287259990582245e-10,
      "optimized_emissions_kg": 3.220348256989382e-10,
      "reduction_pct": 28.89063596121573,
      "original_source": "def parse_pytest_output(file_path):\n    skipped_tests = {}\n    skipped_count = 0\n    with open(file_path, 'r', encoding='utf-8') as file:\n        for line in file:\n            match = re.match(r'^SKIPPED \\[(\\d+)\\] (tests/.*): (.*)$', line)\n            if match:\n                skipped_count += 1\n                test_file, test_line, reason = match.groups()\n                skipped_tests[reason] = skipped_tests.get(reason, []) + [(test_file, test_line)]\n    for k,v in sorted(skipped_tests.items(), key=lambda x:len(x[1])):\n        print(f\"{len(v):4} skipped because: {k}\")\n    print(\"Number of skipped tests:\", skipped_count)",
      "optimized_source": "def parse_pytest_output(file_path):\n    skipped_tests = {}\n    skipped_count = 0\n    pattern = re.compile(r'^SKIPPED \\[(\\d+)\\] (tests/.*): (.*)$')\n    with open(file_path, 'r', encoding='utf-8') as file:\n        for line in file:\n            if not line.startswith('SKIPPED ['):\n                continue\n            match = pattern.match(line)\n            if match:\n                skipped_count += 1\n                test_file, test_line, reason = match.groups()\n                if reason in skipped_tests:\n                    skipped_tests[reason].append((test_file, test_line))\n                else:\n                    skipped_tests[reason] = [(test_file, test_line)]\n    for k, v in sorted(skipped_tests.items(), key=lambda x: len(x[1])):\n        print(f\"{len(v):4} skipped because: {k}\")\n    print(\"Number of skipped tests:\", skipped_count)"
    },
    {
      "function_id": "huggingface__transformers__src__transformers__image_processing_utils.py__convert_to_size_dict",
      "function_name": "convert_to_size_dict",
      "source_file": "huggingface/transformers:src/transformers/image_processing_utils.py",
      "baseline_emissions_kg": 6.052940365698305e-10,
      "optimized_emissions_kg": 4.397179712716024e-10,
      "reduction_pct": 27.354650020432864,
      "original_source": "def convert_to_size_dict(\n    size, max_size: int | None = None, default_to_square: bool = True, height_width_order: bool = True\n):\n    # By default, if size is an int we assume it represents a tuple of (size, size).\n    if isinstance(size, int) and default_to_square:\n        if max_size is not None:\n            raise ValueError(\"Cannot specify both size as an int, with default_to_square=True and max_size\")\n        return {\"height\": size, \"width\": size}\n    # In other configs, if size is an int and default_to_square is False, size represents the length of\n    # the shortest edge after resizing.\n    elif isinstance(size, int) and not default_to_square:\n        size_dict = {\"shortest_edge\": size}\n        if max_size is not None:\n            size_dict[\"longest_edge\"] = max_size\n        return size_dict\n    # Otherwise, if size is a tuple it's either (height, width) or (width, height)\n    elif isinstance(size, (tuple, list)) and height_width_order:\n        return {\"height\": size[0], \"width\": size[1]}\n    elif isinstance(size, (tuple, list)) and not height_width_order:\n        return {\"height\": size[1], \"width\": size[0]}\n    elif size is None and max_size is not None:\n        if default_to_square:\n            raise ValueError(\"Cannot specify both default_to_square=True and max_size\")\n        return {\"longest_edge\": max_size}\n\n    raise ValueError(f\"Could not convert size input to size dict: {size}\")",
      "optimized_source": "def convert_to_size_dict(\n    size, max_size: int | None = None, default_to_square: bool = True, height_width_order: bool = True\n):\n    if isinstance(size, int):\n        if default_to_square:\n            if max_size is not None:\n                raise ValueError(\"Cannot specify both size as an int, with default_to_square=True and max_size\")\n            return {\"height\": size, \"width\": size}\n        else:\n            if max_size is not None:\n                return {\"shortest_edge\": size, \"longest_edge\": max_size}\n            return {\"shortest_edge\": size}\n    elif isinstance(size, (tuple, list)):\n        if height_width_order:\n            return {\"height\": size[0], \"width\": size[1]}\n        else:\n            return {\"height\": size[1], \"width\": size[0]}\n    elif size is None and max_size is not None:\n        if default_to_square:\n            raise ValueError(\"Cannot specify both default_to_square=True and max_size\")\n        return {\"longest_edge\": max_size}\n\n    raise ValueError(f\"Could not convert size input to size dict: {size}\")"
    },
    {
      "function_id": "huggingface__transformers__src__transformers__integrations__bitsandbytes.py__dequantize_bnb_weight",
      "function_name": "dequantize_bnb_weight",
      "source_file": "huggingface/transformers:src/transformers/integrations/bitsandbytes.py",
      "baseline_emissions_kg": 4.3043268965266176e-10,
      "optimized_emissions_kg": 3.168122530361871e-10,
      "reduction_pct": 26.39679544510452,
      "original_source": "def dequantize_bnb_weight(weight: \"torch.nn.Parameter\", state=None):\n    \"\"\"\n    Helper function to dequantize 4bit or 8bit bnb weights.\n\n    If the weight is not a bnb quantized weight, it will be returned as is.\n    \"\"\"\n    if not isinstance(weight, torch.nn.Parameter):\n        raise TypeError(f\"Input weight should be of type nn.Parameter, got {type(weight)} instead\")\n\n    cls_name = weight.__class__.__name__\n    if cls_name not in (\"Params4bit\", \"Int8Params\"):\n        return weight\n\n    if cls_name == \"Params4bit\":\n        output_tensor = bnb.functional.dequantize_4bit(weight.data, weight.quant_state)\n        return output_tensor\n\n    if state.SCB is None:\n        state.SCB = weight.SCB\n\n    if hasattr(bnb.functional, \"int8_vectorwise_dequant\"):\n        # Use bitsandbytes API if available (requires v0.45.0+)\n        dequantized = bnb.functional.int8_vectorwise_dequant(weight.data, state.SCB)\n    else:\n        # Multiply by (scale/127) to dequantize.\n        dequantized = weight.data * state.SCB.view(-1, 1) * 7.874015718698502e-3\n\n    return dequantized",
      "optimized_source": "def dequantize_bnb_weight(weight: \"torch.nn.Parameter\", state=None):\n    \"\"\"\n    Helper function to dequantize 4bit or 8bit bnb weights.\n\n    If the weight is not a bnb quantized weight, it will be returned as is.\n    \"\"\"\n    if not isinstance(weight, torch.nn.Parameter):\n        raise TypeError(f\"Input weight should be of type nn.Parameter, got {type(weight)} instead\")\n\n    cls_name = weight.__class__.__name__\n    if cls_name not in (\"Params4bit\", \"Int8Params\"):\n        return weight\n\n    if cls_name == \"Params4bit\":\n        output_tensor = bnb.functional.dequantize_4bit(weight.data, weight.quant_state)\n        return output_tensor\n\n    if state.SCB is None:\n        state.SCB = weight.SCB\n\n    if hasattr(bnb.functional, \"int8_vectorwise_dequant\"):\n        # Use bitsandbytes API if available (requires v0.45.0+)\n        dequantized = bnb.functional.int8_vectorwise_dequant(weight.data, state.SCB)\n    else:\n        # Multiply by (scale/127) to dequantize.\n        dequantized = weight.data * state.SCB.view(-1, 1) * 7.874015718698502e-3\n\n    return dequantized"
    },
    {
      "function_id": "huggingface__transformers__src__transformers__image_utils.py__validate_annotations",
      "function_name": "validate_annotations",
      "source_file": "huggingface/transformers:src/transformers/image_utils.py",
      "baseline_emissions_kg": 4.251202777901199e-10,
      "optimized_emissions_kg": 3.1969311618595385e-10,
      "reduction_pct": 24.79937258043828,
      "original_source": "def validate_annotations(\n    annotation_format: AnnotationFormat,\n    supported_annotation_formats: tuple[AnnotationFormat, ...],\n    annotations: list[dict],\n) -> None:\n    if annotation_format not in supported_annotation_formats:\n        raise ValueError(f\"Unsupported annotation format: {format} must be one of {supported_annotation_formats}\")\n\n    if annotation_format is AnnotationFormat.COCO_DETECTION:\n        if not valid_coco_detection_annotations(annotations):\n            raise ValueError(\n                \"Invalid COCO detection annotations. Annotations must a dict (single image) or list of dicts \"\n                \"(batch of images) with the following keys: `image_id` and `annotations`, with the latter \"\n                \"being a list of annotations in the COCO format.\"\n            )\n\n    if annotation_format is AnnotationFormat.COCO_PANOPTIC:\n        if not valid_coco_panoptic_annotations(annotations):\n            raise ValueError(\n                \"Invalid COCO panoptic annotations. Annotations must a dict (single image) or list of dicts \"\n                \"(batch of images) with the following keys: `image_id`, `file_name` and `segments_info`, with \"\n                \"the latter being a list of annotations in the COCO format.\"\n            )",
      "optimized_source": "def validate_annotations(\n    annotation_format: AnnotationFormat,\n    supported_annotation_formats: tuple[AnnotationFormat, ...],\n    annotations: list[dict],\n) -> None:\n    if annotation_format not in supported_annotation_formats:\n        raise ValueError(f\"Unsupported annotation format: {format} must be one of {supported_annotation_formats}\")\n\n    if annotation_format is AnnotationFormat.COCO_DETECTION:\n        if not valid_coco_detection_annotations(annotations):\n            raise ValueError(\n                \"Invalid COCO detection annotations. Annotations must a dict (single image) or list of dicts \"\n                \"(batch of images) with the following keys: `image_id` and `annotations`, with the latter \"\n                \"being a list of annotations in the COCO format.\"\n            )\n\n    if annotation_format is AnnotationFormat.COCO_PANOPTIC:\n        if not valid_coco_panoptic_annotations(annotations):\n            raise ValueError(\n                \"Invalid COCO panoptic annotations. Annotations must a dict (single image) or list of dicts \"\n                \"(batch of images) with the following keys: `image_id`, `file_name` and `segments_info`, with \"\n                \"the latter being a list of annotations in the COCO format.\"\n            )"
    },
    {
      "function_id": "huggingface__transformers__benchmark_v2__framework__benchmark_config.py__is_fa2_or_kernel_available",
      "function_name": "is_fa2_or_kernel_available",
      "source_file": "huggingface/transformers:benchmark_v2/framework/benchmark_config.py",
      "baseline_emissions_kg": 3.435149614670081e-10,
      "optimized_emissions_kg": 5.774213695636717e-10,
      "reduction_pct": 21.53385506075622,
      "original_source": "def is_fa2_or_kernel_available() -> bool:\n    \"\"\"Returns True if the flash_attn_2 or a fallback kernel is available\"\"\"\n    # Early return if flash_attn_2 is available\n    if is_flash_attn_2_available():\n        return True\n    # Early return if kernels is not available\n    if not is_kernels_available():\n        logger.warning(\n            \"flash_attention_2 is not available. kernels is not installed. Benchmarking flash_attention_2 will not \"\n            \"be possible.\"\n        )\n        return False\n    # If kernels is available, try to get the flash_attn_2 kernel\n    try:\n        from kernels import get_kernel\n\n        # TODO: Pass the 'version' kwarg to specify the binary version once kernels >= 0.12.0 is supported.\n        get_kernel(\"kernels-community/flash-attn2\")\n    except Exception as _:\n        logger.warning(\n            \"flash_attention_2 is not available. kernels is installed, but the flash_attn kernel is not available.\"\n            \"Benchmarking flash_attention_2 will not be possible.\"\n        )\n        return False\n    return True",
      "optimized_source": "def is_fa2_or_kernel_available() -> bool:\n    \"\"\"Returns True if the flash_attn_2 or a fallback kernel is available\"\"\"\n    # Early return if flash_attn_2 is available\n    if is_flash_attn_2_available():\n        return True\n    # Early return if kernels is not available\n    if not is_kernels_available():\n        logger.warning(\n            \"flash_attention_2 is not available. kernels is not installed. Benchmarking flash_attention_2 will not \"\n            \"be possible.\"\n        )\n        return False\n    # If kernels is available, try to get the flash_attn_2 kernel\n    try:\n        from kernels import get_kernel\n\n        # TODO: Pass the 'version' kwarg to specify the binary version once kernels >= 0.12.0 is supported.\n        get_kernel(\"kernels-community/flash-attn2\")\n    except Exception as _:\n        logger.warning(\n            \"flash_attention_2 is not available. kernels is installed, but the flash_attn kernel is not available.\"\n            \"Benchmarking flash_attention_2 will not be possible.\"\n        )\n        return False\n    return True"
    },
    {
      "function_id": "huggingface__transformers__benchmark_v2__framework__benchmark_config.py__is_fa2_or_kernel_available",
      "function_name": "is_fa2_or_kernel_available",
      "source_file": "huggingface/transformers:benchmark_v2/framework/benchmark_config.py",
      "baseline_emissions_kg": 7.35885992628755e-10,
      "optimized_emissions_kg": 5.774213695636717e-10,
      "reduction_pct": 21.53385506075622,
      "original_source": "def is_fa2_or_kernel_available() -> bool:\n    \"\"\"Returns True if the flash_attn_2 or a fallback kernel is available\"\"\"\n    # Early return if flash_attn_2 is available\n    if is_flash_attn_2_available():\n        return True\n    # Early return if kernels is not available\n    if not is_kernels_available():\n        logger.warning(\n            \"flash_attention_2 is not available. kernels is not installed. Benchmarking flash_attention_2 will not \"\n            \"be possible.\"\n        )\n        return False\n    # If kernels is available, try to get the flash_attn_2 kernel\n    try:\n        from kernels import get_kernel\n\n        # TODO: Pass the 'version' kwarg to specify the binary version once kernels >= 0.12.0 is supported.\n        get_kernel(\"kernels-community/flash-attn2\")\n    except Exception as _:\n        logger.warning(\n            \"flash_attention_2 is not available. kernels is installed, but the flash_attn kernel is not available.\"\n            \"Benchmarking flash_attention_2 will not be possible.\"\n        )\n        return False\n    return True",
      "optimized_source": "def is_fa2_or_kernel_available() -> bool:\n    \"\"\"Returns True if the flash_attn_2 or a fallback kernel is available\"\"\"\n    # Early return if flash_attn_2 is available\n    if is_flash_attn_2_available():\n        return True\n    # Early return if kernels is not available\n    if not is_kernels_available():\n        logger.warning(\n            \"flash_attention_2 is not available. kernels is not installed. Benchmarking flash_attention_2 will not \"\n            \"be possible.\"\n        )\n        return False\n    # If kernels is available, try to get the flash_attn_2 kernel\n    try:\n        from kernels import get_kernel\n\n        # TODO: Pass the 'version' kwarg to specify the binary version once kernels >= 0.12.0 is supported.\n        get_kernel(\"kernels-community/flash-attn2\")\n    except Exception as _:\n        logger.warning(\n            \"flash_attention_2 is not available. kernels is installed, but the flash_attn kernel is not available.\"\n            \"Benchmarking flash_attention_2 will not be possible.\"\n        )\n        return False\n    return True"
    },
    {
      "function_id": "huggingface__transformers__.github__scripts__assign_reviewers.py__main",
      "function_name": "main",
      "source_file": "huggingface/transformers:.github/scripts/assign_reviewers.py",
      "baseline_emissions_kg": 4.949274915998103e-10,
      "optimized_emissions_kg": 4.1111615627596617e-10,
      "reduction_pct": 16.9340634226907,
      "original_source": "def main():\n    script_dir = Path(__file__).parent.absolute()\n    with open(script_dir / \"codeowners_for_review_action\") as f:\n        codeowners_lines = f.readlines()\n\n    g = Github(os.environ['GITHUB_TOKEN'])\n    repo = g.get_repo(\"huggingface/transformers\")\n    with open(os.environ['GITHUB_EVENT_PATH']) as f:\n        event = json.load(f)\n\n    # The PR number is available in the event payload\n    pr_number = event['pull_request']['number']\n    pr = repo.get_pull(pr_number)\n    pr_author = pr.user.login\n    if pr_author_is_in_hf(pr_author, codeowners_lines):\n        print(f\"PR author {pr_author} is in codeowners, skipping review request.\")\n        return\n\n    existing_reviews = list(pr.get_reviews())\n    if existing_reviews:\n        print(f\"Already has reviews: {[r.user.login for r in existing_reviews]}\")\n        return\n\n    users_requested, teams_requested = pr.get_review_requests()\n    users_requested = list(users_requested)\n    if users_requested:\n        print(f\"Reviewers already requested: {users_requested}\")\n        return\n\n    locs_per_owner = Counter()\n    for file in pr.get_files():\n        owners = get_file_owners(file.filename, codeowners_lines)\n        for owner in owners:\n            locs_per_owner[owner] += file.changes\n\n    # Assign the top 2 based on locs changed as reviewers, but skip the owner if present\n    locs_per_owner.pop(pr_author, None)\n    top_owners = locs_per_owner.most_common(2)\n    print(\"Top owners\", top_owners)\n    top_owners = [owner[0] for owner in top_owners]\n    try:\n        pr.create_review_request(top_owners)\n    except github.GithubException as e:\n        print(f\"Failed to request review for {top_owners}: {e}\")",
      "optimized_source": "def main():\n    script_dir = Path(__file__).parent.absolute()\n    with open(script_dir / \"codeowners_for_review_action\") as f:\n        codeowners_lines = f.readlines()\n\n    g = Github(os.environ['GITHUB_TOKEN'])\n    repo = g.get_repo(\"huggingface/transformers\")\n    with open(os.environ['GITHUB_EVENT_PATH']) as f:\n        event = json.load(f)\n\n    pr_number = event['pull_request']['number']\n    pr = repo.get_pull(pr_number)\n    pr_author = pr.user.login\n    if pr_author_is_in_hf(pr_author, codeowners_lines):\n        print(f\"PR author {pr_author} is in codeowners, skipping review request.\")\n        return\n\n    reviews_iter = pr.get_reviews()\n    first_review = next(iter(reviews_iter), None)\n    if first_review is not None:\n        logins = [first_review.user.login] + [r.user.login for r in reviews_iter]\n        print(f\"Already has reviews: {logins}\")\n        return\n\n    users_requested, teams_requested = pr.get_review_requests()\n    first_user = next(iter(users_requested), None)\n    if first_user is not None:\n        print(f\"Reviewers already requested: {[first_user] + list(users_requested)}\")\n        return\n\n    locs_per_owner = Counter()\n    for file in pr.get_files():\n        owners = get_file_owners(file.filename, codeowners_lines)\n        if owners:\n            changes = file.changes\n            for owner in owners:\n                locs_per_owner[owner] += changes\n\n    locs_per_owner.pop(pr_author, None)\n    top_owners = locs_per_owner.most_common(2)\n    print(\"Top owners\", top_owners)\n    top_owners = [owner for owner, _ in top_owners]\n    if top_owners:\n        try:\n            pr.create_review_request(top_owners)\n        except github.GithubException as e:\n            print(f\"Failed to request review for {top_owners}: {e}\")"
    },
    {
      "function_id": "huggingface__transformers__src__transformers__image_processing_utils.py__select_best_resolution",
      "function_name": "select_best_resolution",
      "source_file": "huggingface/transformers:src/transformers/image_processing_utils.py",
      "baseline_emissions_kg": 3.5273747092578557e-10,
      "optimized_emissions_kg": 3.0392382573627403e-10,
      "reduction_pct": 13.838519922875369,
      "original_source": "def select_best_resolution(original_size: tuple, possible_resolutions: list) -> tuple:\n    \"\"\"\n    Selects the best resolution from a list of possible resolutions based on the original size.\n\n    This is done by calculating the effective and wasted resolution for each possible resolution.\n\n    The best fit resolution is the one that maximizes the effective resolution and minimizes the wasted resolution.\n\n    Args:\n        original_size (tuple):\n            The original size of the image in the format (height, width).\n        possible_resolutions (list):\n            A list of possible resolutions in the format [(height1, width1), (height2, width2), ...].\n\n    Returns:\n        tuple: The best fit resolution in the format (height, width).\n    \"\"\"\n    original_height, original_width = original_size\n    best_fit = None\n    max_effective_resolution = 0\n    min_wasted_resolution = float(\"inf\")\n\n    for height, width in possible_resolutions:\n        scale = min(width / original_width, height / original_height)\n        downscaled_width, downscaled_height = int(original_width * scale), int(original_height * scale)\n        effective_resolution = min(downscaled_width * downscaled_height, original_width * original_height)\n        wasted_resolution = (width * height) - effective_resolution\n\n        if effective_resolution > max_effective_resolution or (\n            effective_resolution == max_effective_resolution and wasted_resolution < min_wasted_resolution\n        ):\n            max_effective_resolution = effective_resolution\n            min_wasted_resolution = wasted_resolution\n            best_fit = (height, width)\n\n    return best_fit",
      "optimized_source": "def select_best_resolution(original_size: tuple, possible_resolutions: list) -> tuple:\n    original_height, original_width = original_size\n    original_area = original_width * original_height\n    best_fit = None\n    max_effective_resolution = 0\n    min_wasted_resolution = float(\"inf\")\n\n    for height, width in possible_resolutions:\n        scale_w = width / original_width\n        scale_h = height / original_height\n        scale = scale_w if scale_w < scale_h else scale_h\n        dw = int(original_width * scale)\n        dh = int(original_height * scale)\n        effective_resolution = dw * dh\n        if effective_resolution > original_area:\n            effective_resolution = original_area\n        wasted_resolution = width * height - effective_resolution\n\n        if effective_resolution > max_effective_resolution or (\n            effective_resolution == max_effective_resolution and wasted_resolution < min_wasted_resolution\n        ):\n            max_effective_resolution = effective_resolution\n            min_wasted_resolution = wasted_resolution\n            best_fit = (height, width)\n\n    return best_fit"
    },
    {
      "function_id": "huggingface__transformers__src__transformers__integrations__awq.py__replace_quantization_scales",
      "function_name": "replace_quantization_scales",
      "source_file": "huggingface/transformers:src/transformers/integrations/awq.py",
      "baseline_emissions_kg": 4.6410781379963727e-10,
      "optimized_emissions_kg": 4.0041164743888657e-10,
      "reduction_pct": 13.724433087922389,
      "original_source": "def replace_quantization_scales(model, model_type):\n    from gptqmodel.quantization.awq.modules.act import ScaledActivation\n\n    if model_type not in AWQ_SCALES_MAPPINGS:\n        return model\n    for name, module in model.named_children():\n        act_name = AWQ_SCALES_MAPPINGS[model_type][\"act\"]\n        layer_before_act_name = AWQ_SCALES_MAPPINGS[model_type][\"layer_before_act\"]\n        if name == act_name and hasattr(model, layer_before_act_name):\n            layer_before_act = getattr(model, AWQ_SCALES_MAPPINGS[model_type][\"layer_before_act\"])\n            size = layer_before_act.out_features\n            scale_like = torch.ones(size)\n            model._modules[name] = ScaledActivation(module, scale_like)\n        _ = replace_quantization_scales(module, model_type)\n    return model",
      "optimized_source": "def replace_quantization_scales(model, model_type):\n    if model_type not in AWQ_SCALES_MAPPINGS:\n        return model\n\n    from gptqmodel.quantization.awq.modules.act import ScaledActivation\n\n    mapping = AWQ_SCALES_MAPPINGS[model_type]\n    act_name = mapping[\"act\"]\n    layer_before_act_name = mapping[\"layer_before_act\"]\n\n    stack = [model]\n    while stack:\n        current = stack.pop()\n        modules = current._modules\n        for name, module in modules.items():\n            if name == act_name:\n                layer_before_act = getattr(current, layer_before_act_name, None)\n                if layer_before_act is not None:\n                    size = layer_before_act.out_features\n                    scale_like = torch.ones(size)\n                    modules[name] = ScaledActivation(module, scale_like)\n                    # Don't recurse into the replaced module\n                    continue\n            stack.append(module)\n    return model"
    },
    {
      "function_id": "huggingface__transformers__src__transformers__integrations__awq.py__replace_with_awq_linear",
      "function_name": "replace_with_awq_linear",
      "source_file": "huggingface/transformers:src/transformers/integrations/awq.py",
      "baseline_emissions_kg": 4.677585241966881e-10,
      "optimized_emissions_kg": 4.4040466712031056e-10,
      "reduction_pct": 5.847858598270138,
      "original_source": "def replace_with_awq_linear(\n    model,\n    modules_to_not_convert=None,\n    quantization_config=None,\n    device_map: str | dict | None = None,\n) -> bool:\n    \"\"\"\n    Public method that replaces the linear layers of the given model with awq quantized layers.\n\n    Args:\n        model (`torch.nn.Module`):\n            The model to convert, can be any `torch.nn.Module` instance.\n        quantization_config (`AwqConfig`):\n            The quantization config object that contains the quantization parameters.\n        modules_to_not_convert (`list[str]`, *optional*, defaults to `None`):\n            A list of nn.Linear weights to not convert. If a parameter path is in the list (e.g. `lm_head.weight`), the corresponding module will not be\n            converted.\n        device_map (`Union[str, dict]`, *optional*, defaults to `None`):\n            The device map that maps the parameters to the device\n    \"\"\"\n    from gptqmodel.quantization import METHOD\n    from gptqmodel.utils.importer import hf_select_quant_linear_v2\n\n    target_cls = hf_select_quant_linear_v2(\n        bits=quantization_config.bits,\n        group_size=quantization_config.group_size,\n        desc_act=False,\n        sym=False,\n        format=quantization_config.format,\n        backend=quantization_config.backend,\n        device_map=device_map,\n        quant_method=METHOD.AWQ,\n        zero_point=quantization_config.zero_point,\n        pack=False,\n    )\n\n    for module_name, module in model.named_modules():\n        if not should_convert_module(module_name, modules_to_not_convert):\n            continue\n        with torch.device(\"meta\"):\n            if isinstance(module, nn.Linear):\n                new_module = target_cls(\n                    bits=quantization_config.bits,\n                    sym=quantization_config.sym,\n                    desc_act=quantization_config.desc_act,\n                    group_size=quantization_config.group_size,\n                    in_features=module.in_features,\n                    out_features=module.out_features,\n                    bias=module.bias is not None,\n                    dev=module.weight.device,\n                    register_buffers=True,\n                )\n                new_module.requires_grad_(False)\n                model.set_submodule(module_name, new_module)\n                has_been_replaced = True\n\n    if not has_been_replaced:\n        logger.warning(\n            \"You are loading your model using eetq but no linear modules were found in your model.\"\n            \" Please double check your model architecture, or submit an issue on github if you think this is\"\n            \" a bug.\"\n        )\n\n    return model",
      "optimized_source": "def replace_with_awq_linear(\n    model,\n    modules_to_not_convert=None,\n    quantization_config=None,\n    device_map: str | dict | None = None,\n) -> bool:\n    \"\"\"\n    Public method that replaces the linear layers of the given model with awq quantized layers.\n\n    Args:\n        model (`torch.nn.Module`):\n            The model to convert, can be any `torch.nn.Module` instance.\n        quantization_config (`AwqConfig`):\n            The quantization config object that contains the quantization parameters.\n        modules_to_not_convert (`list[str]`, *optional*, defaults to `None`):\n            A list of nn.Linear weights to not convert. If a parameter path is in the list (e.g. `lm_head.weight`), the corresponding module will not be\n            converted.\n        device_map (`Union[str, dict]`, *optional*, defaults to `None`):\n            The device map that maps the parameters to the device\n    \"\"\"\n    from gptqmodel.quantization import METHOD\n    from gptqmodel.utils.importer import hf_select_quant_linear_v2\n\n    target_cls = hf_select_quant_linear_v2(\n        bits=quantization_config.bits,\n        group_size=quantization_config.group_size,\n        desc_act=False,\n        sym=False,\n        format=quantization_config.format,\n        backend=quantization_config.backend,\n        device_map=device_map,\n        quant_method=METHOD.AWQ,\n        zero_point=quantization_config.zero_point,\n        pack=False,\n    )\n\n    for module_name, module in model.named_modules():\n        if not should_convert_module(module_name, modules_to_not_convert):\n            continue\n        with torch.device(\"meta\"):\n            if isinstance(module, nn.Linear):\n                new_module = target_cls(\n                    bits=quantization_config.bits,\n                    sym=quantization_config.sym,\n                    desc_act=quantization_config.desc_act,\n                    group_size=quantization_config.group_size,\n                    in_features=module.in_features,\n                    out_features=module.out_features,\n                    bias=module.bias is not None,\n                    dev=module.weight.device,\n                    register_buffers=True,\n                )\n                new_module.requires_grad_(False)\n                model.set_submodule(module_name, new_module)\n                has_been_replaced = True\n\n    if not has_been_replaced:\n        logger.warning(\n            \"You are loading your model using eetq but no linear modules were found in your model.\"\n            \" Please double check your model architecture, or submit an issue on github if you think this is\"\n            \" a bug.\"\n        )\n\n    return model"
    },
    {
      "function_id": "huggingface__transformers__src__transformers__image_processing_utils.py__get_size_dict",
      "function_name": "get_size_dict",
      "source_file": "huggingface/transformers:src/transformers/image_processing_utils.py",
      "baseline_emissions_kg": 3.558184548805002e-10,
      "optimized_emissions_kg": 3.3690712171955967e-10,
      "reduction_pct": 5.314882604189762,
      "original_source": "def get_size_dict(\n    size: int | Iterable[int] | dict[str, int] | None = None,\n    max_size: int | None = None,\n    height_width_order: bool = True,\n    default_to_square: bool = True,\n    param_name=\"size\",\n) -> dict:\n    \"\"\"\n    Converts the old size parameter in the config into the new dict expected in the config. This is to ensure backwards\n    compatibility with the old image processor configs and removes ambiguity over whether the tuple is in (height,\n    width) or (width, height) format.\n\n    - If `size` is tuple, it is converted to `{\"height\": size[0], \"width\": size[1]}` or `{\"height\": size[1], \"width\":\n    size[0]}` if `height_width_order` is `False`.\n    - If `size` is an int, and `default_to_square` is `True`, it is converted to `{\"height\": size, \"width\": size}`.\n    - If `size` is an int and `default_to_square` is False, it is converted to `{\"shortest_edge\": size}`. If `max_size`\n      is set, it is added to the dict as `{\"longest_edge\": max_size}`.\n\n    Args:\n        size (`Union[int, Iterable[int], dict[str, int]]`, *optional*):\n            The `size` parameter to be cast into a size dictionary.\n        max_size (`Optional[int]`, *optional*):\n            The `max_size` parameter to be cast into a size dictionary.\n        height_width_order (`bool`, *optional*, defaults to `True`):\n            If `size` is a tuple, whether it's in (height, width) or (width, height) order.\n        default_to_square (`bool`, *optional*, defaults to `True`):\n            If `size` is an int, whether to default to a square image or not.\n    \"\"\"\n    if not isinstance(size, dict):\n        size_dict = convert_to_size_dict(size, max_size, default_to_square, height_width_order)\n        logger.info(\n            f\"{param_name} should be a dictionary on of the following set of keys: {VALID_SIZE_DICT_KEYS}, got {size}.\"\n            f\" Converted to {size_dict}.\",\n        )\n    else:\n        size_dict = size\n\n    if not is_valid_size_dict(size_dict):\n        raise ValueError(\n            f\"{param_name} must have one of the following set of keys: {VALID_SIZE_DICT_KEYS}, got {size_dict.keys()}\"\n        )\n    return size_dict",
      "optimized_source": "from collections.abc import Iterable\n\ndef get_size_dict(\n    size: int | Iterable[int] | dict[str, int] | None = None,\n    max_size: int | None = None,\n    height_width_order: bool = True,\n    default_to_square: bool = True,\n    param_name=\"size\",\n) -> dict:\n    if not isinstance(size, dict):\n        size_dict = convert_to_size_dict(size, max_size, default_to_square, height_width_order)\n        logger.info(\n            f\"{param_name} should be a dictionary on of the following set of keys: {VALID_SIZE_DICT_KEYS}, got {size}.\"\n            f\" Converted to {size_dict}.\",\n        )\n    else:\n        size_dict = size\n\n    if not is_valid_size_dict(size_dict):\n        raise ValueError(\n            f\"{param_name} must have one of the following set of keys: {VALID_SIZE_DICT_KEYS}, got {size_dict.keys()}\"\n        )\n    return size_dict"
    },
    {
      "function_id": "huggingface__transformers__src__transformers__cli__system.py__env",
      "function_name": "env",
      "source_file": "huggingface/transformers:src/transformers/cli/system.py",
      "baseline_emissions_kg": 8.499659530411e-10,
      "optimized_emissions_kg": 9.32517447162536e-10,
      "reduction_pct": -9.712329514620478,
      "original_source": "def env(\n    accelerate_config_file: Annotated[\n        str | None,\n        typer.Argument(help=\"The accelerate config file to use for the default values in the launching script.\"),\n    ] = None,\n) -> None:\n    \"\"\"Print information about the environment.\"\"\"\n    import safetensors\n\n    safetensors_version = safetensors.__version__\n\n    accelerate_version = \"not installed\"\n    accelerate_config = accelerate_config_str = \"not found\"\n\n    if is_accelerate_available():\n        import accelerate\n        from accelerate.commands.config import default_config_file, load_config_from_file\n\n        accelerate_version = accelerate.__version__\n        # Get the default from the config file.\n        if accelerate_config_file is not None or os.path.isfile(default_config_file):\n            accelerate_config = load_config_from_file(accelerate_config_file).to_dict()\n\n        accelerate_config_str = (\n            \"\\n\".join([f\"\\t- {prop}: {val}\" for prop, val in accelerate_config.items()])\n            if isinstance(accelerate_config, dict)\n            else f\"\\t{accelerate_config}\"\n        )\n\n    pt_version = \"not installed\"\n    pt_cuda_available = \"NA\"\n    pt_accelerator = \"NA\"\n    if is_torch_available():\n        import torch\n\n        pt_version = torch.__version__\n        pt_cuda_available = torch.cuda.is_available()\n        pt_xpu_available = is_torch_xpu_available()\n        pt_npu_available = is_torch_npu_available()\n        pt_hpu_available = is_torch_hpu_available()\n\n        if pt_cuda_available:\n            pt_accelerator = \"CUDA\"\n        elif pt_xpu_available:\n            pt_accelerator = \"XPU\"\n        elif pt_npu_available:\n            pt_accelerator = \"NPU\"\n        elif pt_hpu_available:\n            pt_accelerator = \"HPU\"\n\n    deepspeed_version = \"not installed\"\n    if is_deepspeed_available():\n        # Redirect command line output to silence deepspeed import output.\n        with contextlib.redirect_stdout(io.StringIO()):\n            import deepspeed\n        deepspeed_version = deepspeed.__version__\n\n    info = {\n        \"`transformers` version\": __version__,\n        \"Platform\": platform.platform(),\n        \"Python version\": platform.python_version(),\n        \"Huggingface_hub version\": huggingface_hub.__version__,\n        \"Safetensors version\": f\"{safetensors_version}\",\n        \"Accelerate version\": f\"{accelerate_version}\",\n        \"Accelerate config\": f\"{accelerate_config_str}\",\n        \"DeepSpeed version\": f\"{deepspeed_version}\",\n        \"PyTorch version (accelerator?)\": f\"{pt_version} ({pt_accelerator})\",\n        \"Using distributed or parallel set-up in script?\": \"<fill in>\",\n    }\n    if is_torch_available():\n        if pt_cuda_available:\n            info[\"Using GPU in script?\"] = \"<fill in>\"\n            info[\"GPU type\"] = torch.cuda.get_device_name()\n        elif pt_xpu_available:\n            info[\"Using XPU in script?\"] = \"<fill in>\"\n            info[\"XPU type\"] = torch.xpu.get_device_name()\n        elif pt_hpu_available:\n            info[\"Using HPU in script?\"] = \"<fill in>\"\n            info[\"HPU type\"] = torch.hpu.get_device_name()\n        elif pt_npu_available:\n            info[\"Using NPU in script?\"] = \"<fill in>\"\n            info[\"NPU type\"] = torch.npu.get_device_name()\n            info[\"CANN version\"] = torch.version.cann\n\n    print(\"\\nCopy-and-paste the text below in your GitHub issue and FILL OUT the two last points.\\n\")\n    print(_format_dict(info))\n\n    return info",
      "optimized_source": "def env(\n    accelerate_config_file: Annotated[\n        str | None,\n        typer.Argument(help=\"The accelerate config file to use for the default values in the launching script.\"),\n    ] = None,\n) -> None:\n    \"\"\"Print information about the environment.\"\"\"\n    import safetensors\n\n    safetensors_version = safetensors.__version__\n\n    accelerate_version = \"not installed\"\n    accelerate_config = accelerate_config_str = \"not found\"\n\n    if is_accelerate_available():\n        import accelerate\n        from accelerate.commands.config import default_config_file, load_config_from_file\n\n        accelerate_version = accelerate.__version__\n        # Get the default from the config file.\n        if accelerate_config_file is not None or os.path.isfile(default_config_file):\n            accelerate_config = load_config_from_file(accelerate_config_file).to_dict()\n\n        accelerate_config_str = (\n            \"\\n\".join([f\"\\t- {prop}: {val}\" for prop, val in accelerate_config.items()])\n            if isinstance(accelerate_config, dict)\n            else f\"\\t{accelerate_config}\"\n        )\n\n    pt_version = \"not installed\"\n    pt_cuda_available = \"NA\"\n    pt_accelerator = \"NA\"\n    if is_torch_available():\n        import torch\n\n        pt_version = torch.__version__\n        pt_cuda_available = torch.cuda.is_available()\n        pt_xpu_available = is_torch_xpu_available()\n        pt_npu_available = is_torch_npu_available()\n        pt_hpu_available = is_torch_hpu_available()\n\n        if pt_cuda_available:\n            pt_accelerator = \"CUDA\"\n        elif pt_xpu_available:\n            pt_accelerator = \"XPU\"\n        elif pt_npu_available:\n            pt_accelerator = \"NPU\"\n        elif pt_hpu_available:\n            pt_accelerator = \"HPU\"\n\n    deepspeed_version = \"not installed\"\n    if is_deepspeed_available():\n        # Redirect command line output to silence deepspeed import output.\n        with contextlib.redirect_stdout(io.StringIO()):\n            import deepspeed\n        deepspeed_version = deepspeed.__version__\n\n    info = {\n        \"`transformers` version\": __version__,\n        \"Platform\": platform.platform(),\n        \"Python version\": platform.python_version(),\n        \"Huggingface_hub version\": huggingface_hub.__version__,\n        \"Safetensors version\": f\"{safetensors_version}\",\n        \"Accelerate version\": f\"{accelerate_version}\",\n        \"Accelerate config\": f\"{accelerate_config_str}\",\n        \"DeepSpeed version\": f\"{deepspeed_version}\",\n        \"PyTorch version (accelerator?)\": f\"{pt_version} ({pt_accelerator})\",\n        \"Using distributed or parallel set-up in script?\": \"<fill in>\",\n    }\n    if is_torch_available():\n        if pt_cuda_available:\n            info[\"Using GPU in script?\"] = \"<fill in>\"\n            info[\"GPU type\"] = torch.cuda.get_device_name()\n        elif pt_xpu_available:\n            info[\"Using XPU in script?\"] = \"<fill in>\"\n            info[\"XPU type\"] = torch.xpu.get_device_name()\n        elif pt_hpu_available:\n            info[\"Using HPU in script?\"] = \"<fill in>\"\n            info[\"HPU type\"] = torch.hpu.get_device_name()\n        elif pt_npu_available:\n            info[\"Using NPU in script?\"] = \"<fill in>\"\n            info[\"NPU type\"] = torch.npu.get_device_name()\n            info[\"CANN version\"] = torch.version.cann\n\n    print(\"\\nCopy-and-paste the text below in your GitHub issue and FILL OUT the two last points.\\n\")\n    print(_format_dict(info))\n\n    return info"
    },
    {
      "function_id": "huggingface__transformers__src__transformers__image_utils.py__is_valid_annotation_coco_detection",
      "function_name": "is_valid_annotation_coco_detection",
      "source_file": "huggingface/transformers:src/transformers/image_utils.py",
      "baseline_emissions_kg": 5.909202186157927e-10,
      "optimized_emissions_kg": 6.526291052403395e-10,
      "reduction_pct": -10.442845697359529,
      "original_source": "def is_valid_annotation_coco_detection(annotation: dict[str, list | tuple]) -> bool:\n    if (\n        isinstance(annotation, dict)\n        and \"image_id\" in annotation\n        and \"annotations\" in annotation\n        and isinstance(annotation[\"annotations\"], (list, tuple))\n        and (\n            # an image can have no annotations\n            len(annotation[\"annotations\"]) == 0 or isinstance(annotation[\"annotations\"][0], dict)\n        )\n    ):\n        return True\n    return False",
      "optimized_source": "def is_valid_annotation_coco_detection(annotation: dict[str, list | tuple]) -> bool:\n    if not isinstance(annotation, dict):\n        return False\n    anns = annotation.get(\"annotations\")\n    if anns is None or \"image_id\" not in annotation or not isinstance(anns, (list, tuple)):\n        return False\n    return len(anns) == 0 or isinstance(anns[0], dict)"
    },
    {
      "function_id": "huggingface__transformers__src__transformers__image_utils.py__is_valid_annotation_coco_panoptic",
      "function_name": "is_valid_annotation_coco_panoptic",
      "source_file": "huggingface/transformers:src/transformers/image_utils.py",
      "baseline_emissions_kg": 4.790852827980411e-10,
      "optimized_emissions_kg": 5.350476742547471e-10,
      "reduction_pct": -11.681091752571533,
      "original_source": "def is_valid_annotation_coco_panoptic(annotation: dict[str, list | tuple]) -> bool:\n    if (\n        isinstance(annotation, dict)\n        and \"image_id\" in annotation\n        and \"segments_info\" in annotation\n        and \"file_name\" in annotation\n        and isinstance(annotation[\"segments_info\"], (list, tuple))\n        and (\n            # an image can have no segments\n            len(annotation[\"segments_info\"]) == 0 or isinstance(annotation[\"segments_info\"][0], dict)\n        )\n    ):\n        return True\n    return False",
      "optimized_source": "def is_valid_annotation_coco_panoptic(annotation: dict[str, list | tuple]) -> bool:\n    if not isinstance(annotation, dict):\n        return False\n    if \"image_id\" not in annotation or \"segments_info\" not in annotation or \"file_name\" not in annotation:\n        return False\n    segments = annotation[\"segments_info\"]\n    if not isinstance(segments, (list, tuple)):\n        return False\n    if len(segments) == 0:\n        return True\n    return isinstance(segments[0], dict)"
    },
    {
      "function_id": "huggingface__transformers__src__transformers__integrations__aqlm.py__replace_with_aqlm_linear",
      "function_name": "replace_with_aqlm_linear",
      "source_file": "huggingface/transformers:src/transformers/integrations/aqlm.py",
      "baseline_emissions_kg": 4.714532935438911e-10,
      "optimized_emissions_kg": 5.451078564772615e-10,
      "reduction_pct": -15.622875890782886,
      "original_source": "def replace_with_aqlm_linear(model, modules_to_not_convert: list[str] | None = None, quantization_config=None):\n    \"\"\"\n    Public method that recursively replaces the Linear layers of the given model with AQLM quantized layers.\n\n    Args:\n        model (`torch.nn.Module`):\n            The model to convert, can be any `torch.nn.Module` instance.\n        modules_to_not_convert (`list[str]`, *optional*, defaults to `None`):\n            A list of nn.Linear weights to not convert. If a parameter path is in the list (e.g. `lm_head.weight`), the corresponding module will not be\n            converted.\n        quantization_config (`AqlmConfig`):\n            The quantization config object that contains the quantization parameters.\n    \"\"\"\n    from aqlm import QuantizedLinear\n\n    has_been_replaced = False\n    # we need this to correctly materialize the weights during quantization\n    for module_name, module in model.named_modules():\n        if not should_convert_module(module_name, modules_to_not_convert):\n            continue\n        with torch.device(\"meta\"):\n            if isinstance(module, nn.Linear):\n                new_module = QuantizedLinear(\n                    module.in_features,\n                    module.out_features,\n                    bias=module.bias is not None,\n                    in_group_size=quantization_config.in_group_size,\n                    out_group_size=quantization_config.out_group_size,\n                    num_codebooks=quantization_config.num_codebooks,\n                    nbits_per_codebook=quantization_config.nbits_per_codebook,\n                )\n                new_module.source_cls = type(module)\n                new_module.requires_grad_(False)\n                model.set_submodule(module_name, new_module)\n                has_been_replaced = True\n\n    if not has_been_replaced:\n        logger.warning(\n            \"You are loading your model using eetq but no linear modules were found in your model.\"\n            \" Please double check your model architecture, or submit an issue on github if you think this is\"\n            \" a bug.\"\n        )\n\n    return model",
      "optimized_source": "def replace_with_aqlm_linear(model, modules_to_not_convert: list[str] | None = None, quantization_config=None):\n    \"\"\"\n    Public method that recursively replaces the Linear layers of the given model with AQLM quantized layers.\n\n    Args:\n        model (`torch.nn.Module`):\n            The model to convert, can be any `torch.nn.Module` instance.\n        modules_to_not_convert (`list[str]`, *optional*, defaults to `None`):\n            A list of nn.Linear weights to not convert. If a parameter path is in the list (e.g. `lm_head.weight`), the corresponding module will not be\n            converted.\n        quantization_config (`AqlmConfig`):\n            The quantization config object that contains the quantization parameters.\n    \"\"\"\n    from aqlm import QuantizedLinear\n\n    has_been_replaced = False\n    # we need this to correctly materialize the weights during quantization\n    for module_name, module in model.named_modules():\n        if not should_convert_module(module_name, modules_to_not_convert):\n            continue\n        with torch.device(\"meta\"):\n            if isinstance(module, nn.Linear):\n                new_module = QuantizedLinear(\n                    module.in_features,\n                    module.out_features,\n                    bias=module.bias is not None,\n                    in_group_size=quantization_config.in_group_size,\n                    out_group_size=quantization_config.out_group_size,\n                    num_codebooks=quantization_config.num_codebooks,\n                    nbits_per_codebook=quantization_config.nbits_per_codebook,\n                )\n                new_module.source_cls = type(module)\n                new_module.requires_grad_(False)\n                model.set_submodule(module_name, new_module)\n                has_been_replaced = True\n\n    if not has_been_replaced:\n        logger.warning(\n            \"You are loading your model using eetq but no linear modules were found in your model.\"\n            \" Please double check your model architecture, or submit an issue on github if you think this is\"\n            \" a bug.\"\n        )\n\n    return model"
    },
    {
      "function_id": "huggingface__transformers__src__transformers__initialization.py__guard_torch_init_functions",
      "function_name": "guard_torch_init_functions",
      "source_file": "huggingface/transformers:src/transformers/initialization.py",
      "baseline_emissions_kg": 4.992886446721968e-10,
      "optimized_emissions_kg": 6.330429437221025e-10,
      "reduction_pct": -26.78897276698951,
      "original_source": "def guard_torch_init_functions():\n    \"\"\"\n    Guard the `torch.nn.init` primitive functions to behave exactly like the functions in this file, i.e. be\n    protected against the `_is_hf_initialized` flag to avoid re-init if the param was already loaded.\n\n    Usually, all models are using the init from `transformers` which are already guarded, but just to make extra sure\n    and for remote code, we also use this context manager.\n    \"\"\"\n    originals = defaultdict(dict)\n    try:\n        # Replace all torch funcs by the ones in this file\n        for module_name in TORCH_MODULES_TO_PATCH:\n            if module_name in sys.modules:\n                module = sys.modules[module_name]\n                for func_name in TORCH_INIT_FUNCTIONS.keys():\n                    if hasattr(module, func_name):\n                        originals[module][func_name] = getattr(module, func_name)\n                        setattr(module, func_name, globals()[func_name])\n        yield\n    finally:\n        # Set back the original functions on all modules\n        for module, functions in originals.items():\n            for func_name, func in functions.items():\n                setattr(module, func_name, func)",
      "optimized_source": "def guard_torch_init_functions():\n    originals = defaultdict(dict)\n    try:\n        global_vars = globals()\n        modules_to_patch = []\n        for module_name in TORCH_MODULES_TO_PATCH:\n            mod = sys.modules.get(module_name)\n            if mod is not None:\n                modules_to_patch.append(mod)\n        \n        if modules_to_patch:\n            for func_name in TORCH_INIT_FUNCTIONS:\n                replacement = global_vars.get(func_name)\n                if replacement is None:\n                    continue\n                for module in modules_to_patch:\n                    original = getattr(module, func_name, None)\n                    if original is not None:\n                        originals[module][func_name] = original\n                        setattr(module, func_name, replacement)\n        yield\n    finally:\n        for module, functions in originals.items():\n            for func_name, func in functions.items():\n                setattr(module, func_name, func)"
    },
    {
      "function_id": "huggingface__transformers__src__transformers__hf_argparser.py__HfArg",
      "function_name": "HfArg",
      "source_file": "huggingface/transformers:src/transformers/hf_argparser.py",
      "baseline_emissions_kg": 2.266608071498921e-09,
      "optimized_emissions_kg": 2.969280161388451e-09,
      "reduction_pct": -31.001040661822444,
      "original_source": "def HfArg(\n    *,\n    aliases: str | list[str] | None = None,\n    help: str | None = None,\n    default: Any = dataclasses.MISSING,\n    default_factory: Callable[[], Any] = dataclasses.MISSING,\n    metadata: dict | None = None,\n    **kwargs,\n) -> dataclasses.Field:\n    \"\"\"Argument helper enabling a concise syntax to create dataclass fields for parsing with `HfArgumentParser`.\n\n    Example comparing the use of `HfArg` and `dataclasses.field`:\n    ```\n    @dataclass\n    class Args:\n        regular_arg: str = dataclasses.field(default=\"Huggingface\", metadata={\"aliases\": [\"--example\", \"-e\"], \"help\": \"This syntax could be better!\"})\n        hf_arg: str = HfArg(default=\"Huggingface\", aliases=[\"--example\", \"-e\"], help=\"What a nice syntax!\")\n    ```\n\n    Args:\n        aliases (Union[str, list[str]], optional):\n            Single string or list of strings of aliases to pass on to argparse, e.g. `aliases=[\"--example\", \"-e\"]`.\n            Defaults to None.\n        help (str, optional): Help string to pass on to argparse that can be displayed with --help. Defaults to None.\n        default (Any, optional):\n            Default value for the argument. If not default or default_factory is specified, the argument is required.\n            Defaults to dataclasses.MISSING.\n        default_factory (Callable[[], Any], optional):\n            The default_factory is a 0-argument function called to initialize a field's value. It is useful to provide\n            default values for mutable types, e.g. lists: `default_factory=list`. Mutually exclusive with `default=`.\n            Defaults to dataclasses.MISSING.\n        metadata (dict, optional): Further metadata to pass on to `dataclasses.field`. Defaults to None.\n\n    Returns:\n        Field: A `dataclasses.Field` with the desired properties.\n    \"\"\"\n    if metadata is None:\n        # Important, don't use as default param in function signature because dict is mutable and shared across function calls\n        metadata = {}\n    if aliases is not None:\n        metadata[\"aliases\"] = aliases\n    if help is not None:\n        metadata[\"help\"] = help\n\n    return dataclasses.field(metadata=metadata, default=default, default_factory=default_factory, **kwargs)",
      "optimized_source": "def HfArg(\n    *,\n    aliases: str | list[str] | None = None,\n    help: str | None = None,\n    default: Any = dataclasses.MISSING,\n    default_factory: Callable[[], Any] = dataclasses.MISSING,\n    metadata: dict | None = None,\n    **kwargs,\n) -> dataclasses.Field:\n    \"\"\"Argument helper enabling a concise syntax to create dataclass fields for parsing with `HfArgumentParser`.\n\n    Example comparing the use of `HfArg` and `dataclasses.field`:\n    ```\n    @dataclass\n    class Args:\n        regular_arg: str = dataclasses.field(default=\"Huggingface\", metadata={\"aliases\": [\"--example\", \"-e\"], \"help\": \"This syntax could be better!\"})\n        hf_arg: str = HfArg(default=\"Huggingface\", aliases=[\"--example\", \"-e\"], help=\"What a nice syntax!\")\n    ```\n\n    Args:\n        aliases (Union[str, list[str]], optional):\n            Single string or list of strings of aliases to pass on to argparse, e.g. `aliases=[\"--example\", \"-e\"]`.\n            Defaults to None.\n        help (str, optional): Help string to pass on to argparse that can be displayed with --help. Defaults to None.\n        default (Any, optional):\n            Default value for the argument. If not default or default_factory is specified, the argument is required.\n            Defaults to dataclasses.MISSING.\n        default_factory (Callable[[], Any], optional):\n            The default_factory is a 0-argument function called to initialize a field's value. It is useful to provide\n            default values for mutable types, e.g. lists: `default_factory=list`. Mutually exclusive with `default=`.\n            Defaults to dataclasses.MISSING.\n        metadata (dict, optional): Further metadata to pass on to `dataclasses.field`. Defaults to None.\n\n    Returns:\n        Field: A `dataclasses.Field` with the desired properties.\n    \"\"\"\n    if metadata is None:\n        # Important, don't use as default param in function signature because dict is mutable and shared across function calls\n        metadata = {}\n    if aliases is not None:\n        metadata[\"aliases\"] = aliases\n    if help is not None:\n        metadata[\"help\"] = help\n\n    return dataclasses.field(metadata=metadata, default=default, default_factory=default_factory, **kwargs)"
    },
    {
      "function_id": "huggingface__transformers__src__transformers__image_processing_utils.py__get_patch_output_size",
      "function_name": "get_patch_output_size",
      "source_file": "huggingface/transformers:src/transformers/image_processing_utils.py",
      "baseline_emissions_kg": 3.2095980310696176e-10,
      "optimized_emissions_kg": 5.76565038861008e-10,
      "reduction_pct": -79.63777185794967,
      "original_source": "def get_patch_output_size(image, target_resolution, input_data_format):\n    \"\"\"\n    Given an image and a target resolution, calculate the output size of the image after cropping to the target\n    \"\"\"\n    original_height, original_width = get_image_size(image, channel_dim=input_data_format)\n    target_height, target_width = target_resolution\n\n    scale_w = target_width / original_width\n    scale_h = target_height / original_height\n\n    if scale_w < scale_h:\n        new_width = target_width\n        new_height = min(math.ceil(original_height * scale_w), target_height)\n    else:\n        new_height = target_height\n        new_width = min(math.ceil(original_width * scale_h), target_width)\n\n    return new_height, new_width",
      "optimized_source": "def get_patch_output_size(image, target_resolution, input_data_format):\n    original_height, original_width = get_image_size(image, channel_dim=input_data_format)\n    target_height, target_width = target_resolution\n\n    scale_w = target_width / original_width\n    scale_h = target_height / original_height\n\n    if scale_w < scale_h:\n        new_width = target_width\n        new_height = min(-(-original_height * target_width // original_width), target_height)\n    else:\n        new_height = target_height\n        new_width = min(-(-original_width * target_height // original_height), target_width)\n\n    return new_height, new_width"
    },
    {
      "function_id": "huggingface__transformers__src__transformers__backbone_utils.py__load_backbone",
      "function_name": "load_backbone",
      "source_file": "huggingface/transformers:src/transformers/backbone_utils.py",
      "baseline_emissions_kg": 0.0004700472377881,
      "optimized_emissions_kg": 0.0015188715297154,
      "reduction_pct": -223.13167860805788,
      "original_source": "def load_backbone(config):\n    \"\"\"\n    Loads the backbone model from a config object.\n\n    If the config is from the backbone model itself, then we return a backbone model with randomly initialized\n    weights.\n\n    If the config is from the parent model of the backbone model itself, then we load the pretrained backbone weights\n    if specified.\n    \"\"\"\n    from transformers import AutoBackbone\n\n    backbone_config = getattr(config, \"backbone_config\", None)\n\n    if backbone_config is None:\n        backbone = AutoBackbone.from_config(config=config)\n    else:\n        backbone = AutoBackbone.from_config(config=backbone_config)\n    return backbone",
      "optimized_source": "def load_backbone(config):\n    \"\"\"\n    Loads the backbone model from a config object.\n\n    If the config is from the backbone model itself, then we return a backbone model with randomly initialized\n    weights.\n\n    If the config is from the parent model of the backbone model itself, then we load the pretrained backbone weights\n    if specified.\n    \"\"\"\n    from transformers import AutoBackbone\n\n    backbone_config = getattr(config, \"backbone_config\", None)\n\n    if backbone_config is None:\n        backbone = AutoBackbone.from_config(config=config)\n    else:\n        backbone = AutoBackbone.from_config(config=backbone_config)\n    return backbone"
    },
    {
      "function_id": "huggingface__transformers__src__transformers__image_utils.py__make_list_of_images",
      "function_name": "make_list_of_images",
      "source_file": "huggingface/transformers:src/transformers/image_utils.py",
      "baseline_emissions_kg": 5.80210648526554e-10,
      "optimized_emissions_kg": 8.375058158472531e-09,
      "reduction_pct": -1343.451301650703,
      "original_source": "def make_list_of_images(images, expected_ndims: int = 3) -> list[ImageInput]:\n    \"\"\"\n    Ensure that the output is a list of images. If the input is a single image, it is converted to a list of length 1.\n    If the input is a batch of images, it is converted to a list of images.\n\n    Args:\n        images (`ImageInput`):\n            Image of images to turn into a list of images.\n        expected_ndims (`int`, *optional*, defaults to 3):\n            Expected number of dimensions for a single input image. If the input image has a different number of\n            dimensions, an error is raised.\n    \"\"\"\n    if is_batched(images):\n        return images\n\n    # Either the input is a single image, in which case we create a list of length 1\n    if is_pil_image(images):\n        # PIL images are never batched\n        return [images]\n\n    if is_valid_image(images):\n        if images.ndim == expected_ndims + 1:\n            # Batch of images\n            images = list(images)\n        elif images.ndim == expected_ndims:\n            # Single image\n            images = [images]\n        else:\n            raise ValueError(\n                f\"Invalid image shape. Expected either {expected_ndims + 1} or {expected_ndims} dimensions, but got\"\n                f\" {images.ndim} dimensions.\"\n            )\n        return images\n    raise ValueError(\n        f\"Invalid image type. Expected either PIL.Image.Image, numpy.ndarray, or torch.Tensor, but got {type(images)}.\"\n    )",
      "optimized_source": "def make_list_of_images(images, expected_ndims: int = 3) -> list[ImageInput]:\n    \"\"\"\n    Ensure that the output is a list of images. If the input is a single image, it is converted to a list of length 1.\n    If the input is a batch of images, it is converted to a list of images.\n\n    Args:\n        images (`ImageInput`):\n            Image of images to turn into a list of images.\n        expected_ndims (`int`, *optional*, defaults to 3):\n            Expected number of dimensions for a single input image. If the input image has a different number of\n            dimensions, an error is raised.\n    \"\"\"\n    if is_batched(images):\n        return images\n\n    # Either the input is a single image, in which case we create a list of length 1\n    if is_pil_image(images):\n        # PIL images are never batched\n        return [images]\n\n    if is_valid_image(images):\n        if images.ndim == expected_ndims + 1:\n            # Batch of images\n            images = list(images)\n        elif images.ndim == expected_ndims:\n            # Single image\n            images = [images]\n        else:\n            raise ValueError(\n                f\"Invalid image shape. Expected either {expected_ndims + 1} or {expected_ndims} dimensions, but got\"\n                f\" {images.ndim} dimensions.\"\n            )\n        return images\n    raise ValueError(\n        f\"Invalid image type. Expected either PIL.Image.Image, numpy.ndarray, or torch.Tensor, but got {type(images)}.\"\n    )"
    }
  ]
}